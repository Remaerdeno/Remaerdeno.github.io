<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://www.reamder.com">
  <title>梦想家的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="梦想家的博客">
<meta property="og:url" content="http://www.reamder.com/page/3/index.html">
<meta property="og:site_name" content="梦想家的博客">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="梦想家的博客">
  
    <link rel="alternative" href="/atom.xml" title="梦想家的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/blogImg/timg.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Remaerd</a></h1>
		</hgroup>
		
		<p class="header-subtitle">以梦为马 不负韶华</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories/机器学习">机器学习</a></li>
	        
				<li><a href="/categories/深度学习">深度学习</a></li>
	        
				<li><a href="/categories/数学之美">数学之美</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/Remaerdeno" title="github"><i class="icon-github"></i></a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/Remaerdeno" title="zhihu"><i class="icon-zhihu"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:Remaerdeno@Gmail.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/blogImg/timg.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Remaerd</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>以梦为马 不负韶华<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/Remaerdeno" title="github"><i class="icon-github"></i></a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/Remaerdeno" title="zhihu"><i class="icon-zhihu"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:Remaerdeno@Gmail.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 25%"><a href="/">主页</a></li>
		        
					<li style="width: 25%"><a href="/categories/机器学习">机器学习</a></li>
		        
					<li style="width: 25%"><a href="/categories/深度学习">深度学习</a></li>
		        
					<li style="width: 25%"><a href="/categories/数学之美">数学之美</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-机器学习之线性代数" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/06/机器学习之线性代数/">机器学习之线性代数</a>
    </h1>
  

        
        <a href="/2018/02/06/机器学习之线性代数/" class="archive-article-date">
  	<time datetime="2018-02-06T11:46:53.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-06</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="标量、向量、矩阵和张量"><a href="#标量、向量、矩阵和张量" class="headerlink" title="标量、向量、矩阵和张量"></a>标量、向量、矩阵和张量</h2><ul>
<li><strong>标量：</strong>一个标量就是一个单独的数，一般用<strong>小写</strong>的变量名称表示。当然，当我们介绍标量时，要<strong>明确它们是哪种类型的数值</strong>。这个在写论文时要注意，比如：在定义自然数标量时，我们可能会说”令n ∈ N表示元素的数目”。</li>
<li><strong>向量：</strong>在物理学和工程学中，几何向量更常被称为矢量，这个学过高中数学和物理的就知道，但在线性代数中，经过进一步的抽象，大小和方向的概念亦不一定适用，但我们可以<strong>简单的理解为一列数，通过这列数中的索引，我们可以确定每个单独的数</strong>。通常会赋予向量<strong>粗体的小写</strong>名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱（如下图）：</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-824f0739982920b1502ca01588d35e21_hd.jpg" alt="img"></p>
<ul>
<li><strong>矩阵：</strong>矩阵是<strong>二维数组</strong>，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如<strong>A</strong>。 如果一个实数矩阵高度为m，宽度为n，那么我们说<strong><img src="https://www.zhihu.com/equation?tex=A%5Cepsilon+R%5E%7Bm%5Ctimes+n%7D+" alt="A\epsilon R^{m\times n} "></strong>，当我们到明确表达矩阵的时候，我们将它们写在用方括号包围起来的数组中，如下图：</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-dd3017aa861f53973da40d860ec93732_hd.jpg" alt="img"></p>
<ul>
<li><strong>张量：</strong>线性代数或几何代数中定义的<strong>张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，向量（矢量）视为一阶张量，那么矩阵就是二阶张量</strong>。例如，可以将任意一张彩色图片表示成一个三阶张量（就像C语言中的三维数组），三个维度分别是图片的高度、宽度和色彩数据。 使用字体<strong> A </strong>来表示张量 “A’’。张量 <strong>A</strong> 中坐标为 (i, j, k) 的元素记作 <img src="https://www.zhihu.com/equation?tex=A_%7Bi%2Cj%2Ck%7D" alt="A_{i,j,k}"> 。</li>
</ul>
<blockquote>
<p>上面的知识重要性不言而喻，这些都不知道就别说学过机器学习了…几乎一切运算都是基于向量矩阵来进行的，而在tensorflow中，用张量来表示一切数据，并用来运算。</p>
</blockquote>
<h2 id="矩阵向量的运算"><a href="#矩阵向量的运算" class="headerlink" title="矩阵向量的运算"></a>矩阵向量的运算</h2><p><strong>矩阵乘法：</strong>是矩阵运算中最重要的操作之一。两个矩阵<strong> A</strong> 和 <strong>B</strong> 的 矩阵乘积(matrix product)是第三个矩阵 <strong>C</strong>。为了使乘法定义良好,矩阵 <strong>A</strong> 的列数必须和矩阵 <strong>B</strong> 的行数相等。如果矩阵 <strong>A</strong>的形状是 m × n,矩阵 <strong>B</strong> 的形状是 n × p,那么矩阵C 的形状是 m × p。我们可以通过将两个或多个矩阵并列放置以书写矩阵乘法,例如<strong>C </strong>=<strong> AB.</strong></p>
<p>具体地,该乘法操作定义为 ：<img src="https://www.zhihu.com/equation?tex=C_%7Bi%2Cj%7D%3D%5Csum_%7Bk%7D%5E%7B%7D%7BA_%7Bi%2Ck%7DB_%7Bk%2Cj%7D%7D" alt="C_{i,j}=\sum_{k}^{}{A_{i,k}B_{k,j}}"></p>
<p>举个例子,如下所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a30d59ec32fa6cdd8ea63b160d4f584c_hd.jpg" alt="img"></p>
<p>需要注意的是，两个矩阵的标准乘积不是指两个矩阵中对应元素的乘积。 不过，那样的矩阵操作确实是存在的，被称为元素对应乘积或者Hadamard乘积，记为<strong>A <img src="https://www.zhihu.com/equation?tex=%5Codot" alt="\odot"> B</strong></p>
<blockquote>
<p>特别地，两个相同维数的向量 <img src="https://www.zhihu.com/equation?tex=x+" alt="x "> 和 <img src="https://www.zhihu.com/equation?tex=y" alt="y"> 的 点积(dot product)可看作是矩阵乘积 <img src="https://www.zhihu.com/equation?tex=x%5E%7BT%7Dy" alt="x^{T}y"> 。我们可以把矩阵乘积 <strong>C </strong>=<strong>AB</strong> 中计算 <img src="https://www.zhihu.com/equation?tex=C_%7Bi%2Cj%7D" alt="C_{i,j}"> 的步骤看作是 <strong>A</strong> 的第 i 行和<strong> B</strong> 的第 j 列之间的点积。注意，我们有时候也加两个向量的乘积为<strong>内积</strong></p>
</blockquote>
<p>矩阵乘积服从分配律:<strong>A(B + C)</strong> =<strong> AB + AC</strong></p>
<p>矩阵乘积也服从结合律：<strong>A(BC)</strong> =<strong> (AB)C</strong></p>
<p>但不同于标量乘积,矩阵乘积并不满足交换律(<strong>AB</strong> =<strong> BA </strong>的情况并非总是满足)。</p>
<p>然而,两个向量的 点积(dot product)满足交换律 ：<img src="https://www.zhihu.com/equation?tex=x%5E%7BT%7Dy%3Dy%5E%7BT%7Dx" alt="x^{T}y=y^{T}x"></p>
<p><strong>矩阵转置：</strong></p>
<ul>
<li>矩阵转置的结果为 <img src="https://www.zhihu.com/equation?tex=a_%7Bi%2Cj%7D%3D%28a_%7Bi%2Cj%7D%29%5E%7BT%7D" alt="a_{i,j}=(a_{i,j})^{T}"></li>
<li><img src="https://www.zhihu.com/equation?tex=R%5Cbullet+R%5E%7BT%7D" alt="R\bullet R^{T}"> 结果为对称矩阵，由 <img src="https://www.zhihu.com/equation?tex=R%5Cbullet+R%5E%7BT%7D%3D%28R%5Cbullet+R%5E%7BT%7D%29%5E%7BT%7D" alt="R\bullet R^{T}=(R\bullet R^{T})^{T}"> ,得证 <img src="https://www.zhihu.com/equation?tex=R%5Cbullet+R%5E%7BT%7D" alt="R\bullet R^{T}"> 结果为对称矩阵</li>
</ul>
<blockquote>
<p>矩阵的乘法和其他运算有必要深究，比如矩阵乘法的意义。在机器学习中，很多运算就是矩阵和向量的运算，而Hadamard乘积在反向传播推导中也有应用。</p>
</blockquote>
<h2 id="单位矩阵和逆矩阵"><a href="#单位矩阵和逆矩阵" class="headerlink" title="单位矩阵和逆矩阵"></a>单位矩阵和逆矩阵</h2><p>线性代数提供了被称为矩阵逆的强大工具。 对于大多数矩阵<strong>A</strong>，我们都能通过矩阵逆解析地求解。</p>
<p>为了描述矩阵逆，我们首先需要定义单位矩阵的概念。 任意向量和单位矩阵相乘，都不会改变。 我们将保持 <img src="https://www.zhihu.com/equation?tex=n" alt="n"> 维向量不变的单位矩阵记作 <img src="https://www.zhihu.com/equation?tex=I_%7Bn%7D" alt="I_{n}"> 。 形式上 <img src="https://www.zhihu.com/equation?tex=I_%7Bn%7D%5Cin+R%5E%7Bn%7D" alt="I_{n}\in R^{n}"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cforall+x%5Cin+R%5E%7Bn%7D%2CI_%7Bn%7Dx%3Dx" alt="\forall x\in R^{n},I_{n}x=x"> 。</p>
<p>单位矩阵的结构很简单：所有沿主对角线的元素都是1，而所有其他位置的元素都是0。 如</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-76f5c3a9d7d10fea474c58cd308bdae5_hd.jpg" alt="img"></p>
<p>矩阵<strong>A</strong>的矩阵逆记作 <img src="https://www.zhihu.com/equation?tex=A%5E%7B-1%7D" alt="A^{-1}"> ，其定义的矩阵满足如下条件</p>
<p><img src="https://www.zhihu.com/equation?tex=A%5E%7B-1%7DA%3DI_%7Bn%7D" alt="A^{-1}A=I_{n}"></p>
<p>现在我们可以通过以下步骤求解：</p>
<p>由<img src="https://www.zhihu.com/equation?tex=Ax%3Db" alt="Ax=b"> 得 <img src="https://www.zhihu.com/equation?tex=A%5E%7B-1%7DAx%3DA%5E%7B-1%7Db" alt="A^{-1}Ax=A^{-1}b"></p>
<p>由 <img src="https://www.zhihu.com/equation?tex=A%5E%7B-1%7DA%3DI_%7Bn%7D" alt="A^{-1}A=I_{n}">得<img src="https://www.zhihu.com/equation?tex=I_%7Bn%7Dx%3DA%5E%7B-1%7Db" alt="I_{n}x=A^{-1}b"></p>
<p>最终： <img src="https://www.zhihu.com/equation?tex=x%3DA%5E%7B-1%7Db+" alt="x=A^{-1}b "></p>
<p>求一个矩阵的逆矩阵比较简单，但是更加重要还有更加有用的是判断一个矩阵是否存在逆矩阵，这是一个重点难点，由于判别方式也非常的多种，这里就简述一些简单方法：</p>
<ul>
<li>一切不是方阵（行数不等于列数）的矩阵都没有逆矩阵</li>
<li>可逆矩阵就是非奇异矩阵，非奇异矩阵也是可逆矩阵（奇异矩阵涉及到秩的运算，不是很必要学啊，但推荐去了解吧，如果不想学，那知道这句就好）</li>
<li>行列式等于0的方阵是奇异矩阵，也就是说行列式不等于0等价于是可逆矩阵</li>
</ul>
<blockquote>
<p>矩阵的求逆运算在机器学习中也有非常广泛的应用，比如逻辑回归，比如SVM等等，也是非常重要的，各类的论文中也会涉及到很多这样的运算，所以真的必不可少！</p>
</blockquote>
<h2 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h2><p><strong>行列式,记作 det(A)：</strong>是一个将方阵 A 映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是 0,那么空间至少沿着某一维完全收缩了,使其失去了所有的体积。如果行列式是 1,那么这个转换保持空间体积不变。</p>
<blockquote>
<p>行列式也是一个很大的概念，深究起来非常方，如果不想了解很多，那只需要知道概念就好吧。</p>
</blockquote>
<h2 id="方差，标准差，协方差"><a href="#方差，标准差，协方差" class="headerlink" title="方差，标准差，协方差"></a>方差，标准差，协方差</h2><p><strong>方差：</strong>是衡量随机变量或一组数据时离散程度的度量，方差计算公式：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-1abb9c1049d755992a77c15f8acb96db_hd.jpg" alt="img"></p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="\sigma^{2}"> 为总体方差， <img src="https://www.zhihu.com/equation?tex=X" alt="X"> 为变量， <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu"> 为总体均值， <img src="https://www.zhihu.com/equation?tex=N" alt="N"> 为总体例数。下面的标准差公式中亦相同。</p>
<p><strong>标准差：</strong>也被称为标准偏差，或者实验标准差，公式为</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e716c62e213b641b836667438a7de4c3_hd.jpg" alt="img"></p>
<p>标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。</p>
<p><strong>为什么需要协方差？</strong></p>
<p>我们知道，标准差和方差一般是用来描述一维数据的，但现实生活我们常常遇到含有多维数据的数据集，最简单的 大家上学时免不了要统计多个学科的考试成绩。面对这样的数据集，我们当然可以按照每一维独立的计算其方差，但是通常我们还想了解更多，比如，一个男孩子的 猥琐程度跟他受女孩子欢迎程度是否存在一些联系。<strong>协方差就是这样一种用来度量两个随机变量关系的统计量。</strong></p>
<p><strong>协方差矩阵</strong></p>
<p>理解协方差矩阵的关键就在于牢记<strong>它计算的是不同维度之间的协方差，而不是不同样本之间，</strong>拿到一个样本矩阵，我们最先要明确的就是一行是一个样本还是一个维度，心中明确这个整个计算过程就会顺流而下，这么一来就不会迷茫了</p>
<p>举个例子（例子来自<a href="https://link.zhihu.com/?target=http%3A//www.zipperary.com/2014/01/12/covariance/" target="_blank" rel="noopener">这篇文章</a>）：</p>
<p>问题：</p>
<p>有一组数据（如下），分别为二维向量，这四个数据对应的协方差矩阵是多少？</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ac80e8101583c1b7d70b8caf4d6ea514_hd.jpg" alt="img"></p>
<p>解答：</p>
<p>由于数据是二维的，所以协方差矩阵是一个2*2的矩阵，矩阵的每个元素为：</p>
<p>元素(i,j) = (第 i 维所有元素 - 第 i 维的均值) * (第 j 维所有元素 - 第 j 维的均值) 。</p>
<p>其中「*」代表向量内积符号，即两个向量求内积，对应元素相乘之后再累加。</p>
<p>我们首先列出第一维：</p>
<p>D1: (1,3,4,5) 均值：3.25<br>D2: (2,6,2,2) 均值：3</p>
<p>下面计算协方差矩阵第(1,2)个元素：</p>
<p>元素(1,2)=(1-3.25,3-3.25,4-3.25,5-3.25)*(2-3,6-3,2-3,2-3)=-1</p>
<p>类似的，我们可以把2*2个元素都计算出来：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-eb00efd47533b0702b8d75d82e3f36eb_hd.jpg" alt="img"></p>
<p>这个题目的最终结果就是：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2e9fe85ba4b3a5e17191c15a60d1aa71_hd.jpg" alt="img"></p>
<p>我们来分析一下上面的例子。首先看一下元素(1,1)的计算过程：</p>
<p>把所有数据的第一个维度拿出来，求出均值，之后的求解过程完全是我们熟悉的「方差」的求法。也就是说，这完完全全就是在求所有数据第一维元素（共4个）的方差（8.75）嘛。类似地，元素(2,2)求的是第二维(共4个)元素的方差（12）。</p>
<p>再来看元素(1,2)，这分明就是我们高数里面学的求 x 和 y 的协方差，不再单独计算某一维度的分散程度，而是把两个维度的分散值结合起来，这里才真正体现了「协方差矩阵」中「协方差」的意味。从计算过程和计算结果都能看出，元素(2,1)与元素(1,2)是一样的。也就是说，所有协方差矩阵都是一个对称阵。</p>
<p>总结一下协方差矩阵的特点：</p>
<ul>
<li>对角线元素(i,i)为数据第 i 维的方差。</li>
<li>非对角线元素(i,j)为第 i 维和第 j 维的协方差。</li>
<li>协方差矩阵是对称阵。</li>
</ul>
<p>现在只需要了解这些就够了。</p>
<blockquote>
<p>这些知识也是非常基础的，在各个算法中都有涉及，像偏方差权衡，RL中的方差问题和解决，还有协方差矩阵在二元高斯分布（在下面一片概率论中会讲述）中决定了它的形状，<a href="https://link.zhihu.com/?target=http%3A//www.zipperary.com/2014/01/12/covariance/" target="_blank" rel="noopener">详细演示</a>。</p>
</blockquote>
<h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p>什么是范数，听得那么术语..其实就是衡量一个向量大小的单位。在机器学习中，我们也经常使用被称为范数(norm) 的函数衡量矩阵大小</p>
<p><img src="https://www.zhihu.com/equation?tex=L%5E%7BP%7D" alt="L^{P}"> 范数如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+_%7Bp%7D%5E%7B%7D+%3D%5Cleft%28+%5Csum_%7Bi%7D%5E%7B%7D%7B%5Cleft%7C+x_%7Bi%7D+%5Cright%7C+%5E%7Bp%7D+%7D+%5Cright%29+_%7B%7D%5E%7B%5Cfrac%7B1%7D%7Bp%7D+%7D+" alt="\left| \left| x \right| \right| _{p}^{} =\left( \sum_{i}^{}{\left| x_{i} \right| ^{p} } \right) _{}^{\frac{1}{p} } "></p>
<p>（为什么是这样的，不要管了，要扯就扯偏了，记得是衡量向量或者矩阵大小的就行了）</p>
<p>常见的：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%5E%7B1%7D" alt="L^{1}"> 范数<img src="https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+" alt="\left| \left| x \right| \right| ">：为x向量各个元素绝对值之和；</p>
<p><img src="https://www.zhihu.com/equation?tex=L%5E%7B2%7D" alt="L^{2}"> 范数<img src="https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+_%7B2%7D+" alt="\left| \left| x \right| \right| _{2} ">：为x向量各个元素平方和的开方，这个也就是两点直线距离嘛，回忆初高中的知识！</p>
<p>注意：当 p = 2 时, <img src="https://www.zhihu.com/equation?tex=L%5E%7B2%7D" alt="L^{2}"> 范数被称为<strong> 欧几里得范数(Euclidean norm)</strong>。它表示从原点</p>
<p>出发到向量 x 确定的点的欧几里得距离。 <img src="https://www.zhihu.com/equation?tex=L%5E%7B2%7D+" alt="L^{2} "> 范数在机器学习中出现地十分频繁</p>
<p>经常简化表示为 ∥x∥,略去了下标 2。平方 <img src="https://www.zhihu.com/equation?tex=L%5E%7B2%7D" alt="L^{2}"> 范数也经常用来衡量向量的大小,可以</p>
<p>简单地通过点积 <img src="https://www.zhihu.com/equation?tex=x%5E%7BT%7Dx" alt="x^{T}x"> 计算。</p>
<blockquote>
<p>这些知识在各大算法（如SVM）中亦有涉及，而且在距离量度中的欧式距离，华盛顿距离都有密切关系。</p>
</blockquote>
<h2 id="特殊类型的矩阵和向量"><a href="#特殊类型的矩阵和向量" class="headerlink" title="特殊类型的矩阵和向量"></a>特殊类型的矩阵和向量</h2><p>有些特殊类型的矩阵和向量是特别有用的,也相当于一些术语，比如一些文章直接说是XX矩阵或者XX向量，这个时候我们应该要明白这些矩阵或者向量是什么样子的，还有什么样的性质！</p>
<p><strong>对角矩阵(diagonal matrix)：</strong>只在主对角线上含有非零元素,其他位置都是零。形式上,矩阵 是对角矩阵,当且仅当对于所有的 <img src="https://www.zhihu.com/equation?tex=i%3Dj%EF%BC%8CD_%7Bi%2Cj%7D%E4%B8%8D%E7%AD%89%E4%BA%8E0" alt="i=j，D_{i,j}不等于0"></p>
<p>特殊的：<strong>单位矩阵</strong>是对角元素全部是 1的对角矩阵。</p>
<p><strong>单位向量：</strong>指模等于1（具有 单位范数）的向量。由于是非<a href="https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E9%259B%25B6%25E5%2590%2591%25E9%2587%258F" target="_blank" rel="noopener">零向量</a>，单位向量具有确定的方向。单位向量有无数个。</p>
<p>也就是说：对于单位向量，有 <img src="https://www.zhihu.com/equation?tex=%7C%7Cx%7C%7C_%7B2%7D" alt="||x||_{2}"> = 1.</p>
<p><strong>对称矩阵:</strong>是转置和自己相等的矩阵： <img src="https://www.zhihu.com/equation?tex=A%3DA%5E%7BT%7D" alt="A=A^{T}"></p>
<p>当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常会出现， 例如，如果<strong>A</strong>是一个距离度量矩阵， <img src="https://www.zhihu.com/equation?tex=A_%7Bi%2Cj%7D" alt="A_{i,j}"> 表示点 <img src="https://www.zhihu.com/equation?tex=i+" alt="i "> 到点 <img src="https://www.zhihu.com/equation?tex=j+" alt="j "> 的距离，那么 <img src="https://www.zhihu.com/equation?tex=A_%7Bi%2Cj%7D%3DA_%7Bj%2Ci%7D" alt="A_{i,j}=A_{j,i}"> ，因为距离函数是对称的。</p>
<p><strong>正交矩阵:</strong>是指行向量和列向量是分别标准正交的方阵： <img src="https://www.zhihu.com/equation?tex=A%5E%7BT%7DA%3DAA%5E%7BT%7D%3DI" alt="A^{T}A=AA^{T}=I"></p>
<p>这意味着 <img src="https://www.zhihu.com/equation?tex=A%5E%7B-1%7D%3DA%5E%7BT%7D" alt="A^{-1}=A^{T}"></p>
<p>所以正交矩阵受到关注是因为求逆计算代价小。 我们需要注意正交矩阵的定义。 违反直觉的是，正交矩阵的行向量不仅是正交的，还是标准正交的。 对于行向量或列向量互相正交但不是标准正交的矩阵，没有对应的专有术语。</p>
<h2 id="特征分解以及其意义"><a href="#特征分解以及其意义" class="headerlink" title="特征分解以及其意义"></a>特征分解以及其意义</h2><p>许多数学对象可以通过将它们分解成多个组成部分，或者找到它们的一些属性而更好地理解，这些属性是通用的，而不是由我们选择表示它们的方式引起的。</p>
<p>例如:整数可以分解为质数。 我们可以用十进制或二进制等不同方式表示整数12，但质因数分解永远是对的12=2×3×3。 从这个表示中我们可以获得一些有用的信息，比如12不能被5整除，或者12的倍数可以被3整除。</p>
<p>正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以通过<strong>分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。</strong></p>
<ul>
<li>特征分解是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。</li>
<li>一个变换（或者说矩阵）的特征向量就是这样一种向量，它经过这种特定的变换后保持方向不变，只是进行长度上的伸缩而已。</li>
</ul>
<p>特征向量的原始定义： <img src="https://www.zhihu.com/equation?tex=AX%3DCX" alt="AX=CX"></p>
<p>可以很容易看出， <img src="https://www.zhihu.com/equation?tex=CX" alt="CX"> 是<strong>方阵 </strong><img src="https://www.zhihu.com/equation?tex=A" alt="A"> 对向量 <img src="https://www.zhihu.com/equation?tex=X+" alt="X "> 进行变换后的结果，显然 <img src="https://www.zhihu.com/equation?tex=CX" alt="CX"> 和 <img src="https://www.zhihu.com/equation?tex=X" alt="X"> 的方向相同<strong>。</strong> <img src="https://www.zhihu.com/equation?tex=X" alt="X"> 是特征向量的话， <img src="https://www.zhihu.com/equation?tex=C" alt="C">表示的就是特征值。</p>
<p><strong>求解：</strong>令 <strong>A</strong> 是一个 <em>N</em>×<em>N</em> 的方阵，且有 <em>N</em> 个线性无关的特征向量 <img src="https://www.zhihu.com/equation?tex=q_%7Bi%7D%28i%3D1....N%29" alt="q_{i}(i=1....N)"></p>
<p>这样， <strong>A</strong> 可以被分解</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-98ac381ac0b5792669c994bf2f7c3146_hd.jpg" alt="img"></p>
<p>其中 <strong>Q</strong> 是<em>N</em>×<em>N</em>方阵，且其第 <em>i</em>列为 <strong>A</strong> 的特征向量 。 <strong>Λ</strong> 是对角矩阵，其对角线上的元素为对应的特征值，也即 <img src="https://www.zhihu.com/equation?tex=%5Cwedge+_%7Bii%7D%3DC_%7Bi%7D" alt="\wedge _{ii}=C_{i}"></p>
<p>这里需要注意只有可对角化矩阵才可以作特征分解。比如</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-bd712ecbcd7d67dde271764a136627d1_hd.jpg" alt="img"></p>
<p>不能被对角化，也就不能特征分解。</p>
<p><strong>特征值及特征向量的几何意义和物理意义：</strong></p>
<p>在空间中，对一个变换而言，特征向量指明的方向才是很重要的，特征值不那么重要。虽然我们求这两个量时先求出特征值，但特征向量才是更本质的东西！特征向量是指经过指定变换（与特定矩阵相乘）后不发生方向改变的那些向量，特征值是指在经过这些变换后特征向量的伸缩的倍数,也就是说<strong>矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果</strong>，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。</p>
<p>物理的含义就是图像的运动：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。</p>
<blockquote>
<p>注意：常有教科书说特征向量是在矩阵变换下不改变方向的向量，实际上当特征值小于零时，矩阵就会把特征向量完全反方向改变，当然特征向量还是特征向量。我也赞同特征向量不改变方向的说法：特征向量永远不改变方向，改变的只是特征值（方向反转特征值为负值了）。特征向量也是线性不变量。</p>
</blockquote>
<p><strong>特征分解的重要应用–PCA（主成分分析）：</strong></p>
<p>举个栗子：机器学习中的分类问题，给出178个葡萄酒样本，每个样本含有13个参数，比如酒精度、酸度、镁含量等，这些样本属于3个不同种类的葡萄酒。任务是提取3种葡萄酒的特征，以便下一次给出一个新的葡萄酒样本的时候，能根据已有数据判断出新样本是哪一种葡萄酒。</p>
<p>原数据有13维，但这之中含有冗余，减少数据量最直接的方法就是<strong>降维</strong>。做法：把数据集赋给一个178行13列的矩阵<strong>R</strong>，减掉均值并归一化，它的协方差矩阵<strong>C</strong>是13行13列的矩阵，对<strong>C</strong>进行特征分解，对角化，其中<strong>U</strong>是特征向量组成的矩阵，<strong>D</strong>是特征值组成的对角矩阵，并按由大到小排列。然后，另<strong>R’ =RU</strong>，就实现了数据集在特征向量这组正交基上的投影。嗯，重点来了，<strong>R’</strong>中的数据列是按照对应特征值的大小排列的，后面的列对应小特征值，去掉以后对整个数据集的影响比较小。比如，现在我们直接去掉后面的7列，只保留前6列，就完成了降维。</p>
<p>这个降维方法就叫<strong>PCA（Principal Component Analysis）</strong>。降维以后分类错误率与不降维的方法相差无几，但需要处理的数据量减小了一半（不降维需要处理13维，降维后只需要处理6维）。在深度学习之前，图像处理是很常用到PCA的，PCA是一个非常不错的降维方法！</p>
<h2 id="奇异值分解及其意义"><a href="#奇异值分解及其意义" class="headerlink" title="奇异值分解及其意义"></a>奇异值分解及其意义</h2><p>奇异值分解就是将矩阵 A 分解成三个矩阵的乘积: <img src="https://www.zhihu.com/equation?tex=A%3DUDV%5E%7BT%7D" alt="A=UDV^{T}"></p>
<p>假设 <strong>A</strong> 是一个 m × n 的矩阵,那么 U 是一个 m × m 的矩阵,D 是一个 m × n的矩阵,V 是一个 n × n 矩阵。这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵 U 和 V 都被定义为正交矩阵,而矩阵 D 被定义为对角矩阵。<strong>注意:</strong>矩阵 D 不一定是方阵。</p>
<blockquote>
<p>求解比较复杂，详细推荐查看这篇<a href="https://link.zhihu.com/?target=http%3A//nbviewer.jupyter.org/github/zlotus/notes-LSJU-machine-learning/blob/master/chapter15.ipynb" target="_blank" rel="noopener">奇异值分解</a></p>
</blockquote>
<p><strong>奇异值分解的意义：</strong></p>
<p>奇异值分解的含义是，把一个矩阵A看成线性变换（当然也可以看成是数据矩阵或者样本矩阵），那么这个线性变换的作用效果是这样的，<strong>我们可以在原空间找到一组标准正交基V，同时可以在对应空间找到一组标准正交基U，我们知道，看一个矩阵的作用效果只要看它在一组基上的作用效果即可，</strong>在内积空间上，我们更希望看到它在一组标准正交基上的作用效果。而矩阵A在标准正交基V上的作用效果恰好可以表示为在U的对应方向上只进行纯粹的伸缩！这就大大简化了我们对矩阵作用的认识，因为我们知道，我们面前不管是多么复杂的矩阵，它在某组 标准正交基上的作用就是在另外一组标准正交基上进行伸缩而已。</p>
<blockquote>
<p>更加详细的讲述请看：<a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/redline2005/article/details/24100293" target="_blank" rel="noopener">奇异值的意义</a></p>
</blockquote>
<p>特征分解也是这样的，也可以简化我们对矩阵的认识。对于可对角化的矩阵，该线性变换的作用就是将某些方向（特征向量方向）在该方向上做伸缩。</p>
<p>有了上述认识，当我们要看该矩阵对任一向量x的作用效果的时候，在特征分解的视角下，我们可以把x往特征向量方向上分解，然后每个方向上做伸缩，最后再把结果加起来即可；在奇异值分解的视角下，我们可以把x往V方向上分解，然后将各个分量分别对应到U方向上做伸缩，最后把各个分量上的结果加起来即可。</p>
<p>奇异值分解和上面所讲的特征分解有很大的关系，而我的理解是：</p>
<ul>
<li>不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。那么多类型的矩阵，我们居然总是可以从一个统一且简单的视角去看它，我们就会感叹奇异值分解是多么奇妙了！</li>
<li>协方差矩阵（或 <img src="https://www.zhihu.com/equation?tex=X%5E%7BT%7DX" alt="X^{T}X"> ）的奇异值分解结果和特征值分解结果一致。所以在PCA中，SVD是一种实现方式</li>
</ul>
<blockquote>
<p>上面的知识可能需要其他的一些前置知识，但我认为也不必要非学，用的不多，可以遇到再学吧，我们知道其主要公式，意义和应用就好，重要性也一目了然，对于矩阵的变换运算，比如<strong>降维（PCA）</strong>或<strong>推荐系统</strong>中都有其重要的作用。</p>
</blockquote>
<h2 id="Moore-Penrose-伪逆"><a href="#Moore-Penrose-伪逆" class="headerlink" title="Moore-Penrose 伪逆"></a>Moore-Penrose 伪逆</h2><p>对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：</p>
<p><img src="https://www.zhihu.com/equation?tex=Ax%3Dy" alt="Ax=y"></p>
<p>等式两边同时左乘左逆B后，得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=x%3DBy" alt="x=By"></p>
<p>是否存在唯一的映射将A映射到B取决于问题的形式。</p>
<p>如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。</p>
<p>Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-1581c66947da5c30172f4ef80dd0b70f_hd%20%281%29.jpg" alt="img"></p>
<p>但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2845b623dc537e3bae0db22c4938e9c1_hd.jpg" alt="img"></p>
<p>其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。</p>
<blockquote>
<p>注意，这里的伪逆也是应用奇异值分解来求得的，这就很好体现知识是联系的啦，伪逆的应用在机器学习中也是大量存在的，比如最简单的线性回归中求广义逆矩阵，也就是伪逆。</p>
</blockquote>
<h2 id="迹运算"><a href="#迹运算" class="headerlink" title="迹运算"></a>迹运算</h2><p>迹运算返回的是矩阵对角元素的和：</p>
<p><img src="https://www.zhihu.com/equation?tex=Tr%28A%29%3D%5Csum_%7Bi%7D%5E%7B%7D%7BA_%7Bi%2Cj%7D%7D" alt="Tr(A)=\sum_{i}^{}{A_{i,j}}"></p>
<p>迹运算因为很多原因而有用。 若不使用求和符号，有些矩阵运算很难描述，而通过矩阵乘法和迹运算符号可以清楚地表示。 例如，迹运算提供了另一种描述矩阵Frobenius 范数的方式：<img src="https://www.zhihu.com/equation?tex=%7C%7CA%7C%7C_%7BF%7D%3D%5Csqrt%7BTr%28AA%5E%7BT%7D%29%7D" alt="||A||_{F}=\sqrt{Tr(AA^{T})}"></p>
<p>(不必知道是什么，只要知道有这样的运算就好，如果有兴趣，当然可以去了解)</p>
<p>用迹运算表示表达式，我们可以使用很多有用的等式巧妙地处理表达式。 例如，迹运算在转置运算下是不变的： <img src="https://www.zhihu.com/equation?tex=Tr%28A%29%3DTr%28A%5E%7BT%7D%29" alt="Tr(A)=Tr(A^{T})"></p>
<p>多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相乘的迹是相同的。 当然，我们需要考虑挪动之后矩阵乘积依然定义良好：<strong>Tr</strong>(<strong>ABC</strong>) =<strong> Tr</strong>(<strong>CAB</strong>) =<strong> Tr</strong>(<strong>BCA</strong>).</p>
<blockquote>
<p>迹运算也是常用的数学知识，比如这些知识在<strong>正规方程组</strong>计算中就有着重要的作用。</p>
</blockquote>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">线性代数</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习//" class="article-tag-list-link color5">机器学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/06/机器学习之线性代数/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-L0-L1和L2范数" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/06/L0-L1和L2范数/">L0,L1和L2范数</a>
    </h1>
  

        
        <a href="/2018/02/06/L0-L1和L2范数/" class="archive-article-date">
  	<time datetime="2018-02-06T11:00:57.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-06</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>​    监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。多么简约的哲学啊！因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。要知道，有时候人的先验是非常重要的。前人的经验会让你少走很多弯路，这就是为什么我们平时学习最好找个大牛带带的原因。一句点拨可以为我们拨开眼前乌云，还我们一片晴空万里，醍醐灌顶。对机器学习也是一样，如果被我们人稍微点拨一下，它肯定能更快的学习相应的任务。只是由于人和机器的交流目前还没有那么直接的方法，目前这个媒介只能由规则项来担当了。</p>
<p>​       还有几种角度来看待规则化的。规则化符合奥卡姆剃刀(Occam’s razor)原理。这名字好霸气，razor！不过它的思想很平易近人：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。民间还有个说法就是，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。</p>
<p>​       一般来说，监督学习可以看做最小化下面的目标函数：</p>
<p><img src="http://img.blog.csdn.net/20140504122253546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。</p>
<p>​        OK，到这里，如果你在机器学习浴血奋战多年，你会发现，哎哟哟，机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是牛逼的 Boosting了；如果是log-Loss，那就是Logistic Regression了；还有等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。但这里，我们先不究loss函数的问题，我们把目光转向“规则项Ω(w)”。</p>
<p>​       规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：零范数、一范数、二范数、迹范数、Frobenius范数和核范数等等。这么多范数，到底它们表达啥意思？具有啥能力？什么时候才能用？什么时候需要用呢？不急不急，下面我们挑几个常见的娓娓道来。</p>
<p><strong>一、L0范数与L1范数</strong></p>
<p>​       L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。这太直观了，太露骨了吧，换句话说，让参数W是稀疏的。OK，看到了“稀疏”二字，大家都应该从当下风风火火的“压缩感知”和“稀疏编码”中醒悟过来，原来用的漫山遍野的“稀疏”就是通过这玩意来实现的。但你又开始怀疑了，是这样吗？看到的papers世界中，稀疏不是都通过L1范数来实现吗？脑海里是不是到处都是||W||1影子呀！几乎是抬头不见低头见。没错，这就是这节的题目把L0和L1放在一起的原因，因为他们有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？</p>
<p>​       L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。现在我们来分析下这个价值一个亿的问题：为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。所以关于L1范数的直观理解，请待会看看第二节。</p>
<p>​       对了，上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。</p>
<p><img src="http://img.blog.csdn.net/20140504122328921?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       OK，来个一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。</p>
<p>​       好，到这里，我们大概知道了L1可以实现稀疏，但我们会想呀，为什么要稀疏？让我们的参数稀疏有什么好处呢？这里扯两点：</p>
<p><strong>1）特征选择(Feature Selection)：</strong></p>
<p>​       大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p>
<p><strong>2）可解释性(Interpretability)：</strong></p>
<p>​       另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1<em>x1+w2</em>x2+…+w1000<em>x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w</em>就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。</p>
<p><strong>二、L2范数</strong></p>
<p>​       除了L1范数，还有一种更受宠幸的规则化范数是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，糟糕的一塌糊涂。通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。例如下图所示（来自Ng的course）：</p>
<p><img src="http://img.blog.csdn.net/20140504122353812?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       上面的图是线性回归，下面的图是Logistic回归，也可以说是分类的情况。从左到右分别是欠拟合（underfitting，也称High-bias）、合适的拟合和过拟合（overfitting，也称High variance）三种情况。可以看到，如果模型复杂（可以拟合任意的复杂函数），它可以让我们的模型拟合所有的数据点，也就是基本上没有误差。对于回归来说，就是我们的函数曲线通过了所有的数据点，如上图右。对分类来说，就是我们的函数曲线要把所有的数据点都分类正确，如下图右。这两种情况很明显过拟合了。</p>
<p><img src="http://img.blog.csdn.net/20140504122410234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       OK，那现在到我们非常关键的问题了，为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数是个什么东西。</p>
<p>​       L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。其实我也不太懂，希望大家可以指点下。</p>
<p>​       这里也一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。</p>
<p>​       L2范数的好处是什么呢？这里也扯上两点：</p>
<p><strong>1）学习理论的角度：</strong></p>
<p>​       从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。</p>
<p><strong>2）优化计算的角度：</strong></p>
<p>​       从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。哎，等等，这condition number是啥？我先google一下哈。</p>
<p>​       这里我们也故作高雅的来聊聊优化问题。优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。前者俺就不说了，大家都懂吧，我们要找的是全局最小值，如果局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔，这很明显不是观众愿意看到的剧情。那下面我们来聊聊ill-condition。ill-condition对应的是well-condition。那他们分别代表什么？假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。我们具体举个例子吧：</p>
<p><img src="http://img.blog.csdn.net/20140504122435031?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       咱们先看左边的那个。第一行假设是我们的AX=b，第二行我们稍微改变下b，得到的x和没改变前的差别很大，看到吧。第三行我们稍微改变下系数矩阵A，可以看到结果的变化也很大。换句话来说，这个系统的解对系数矩阵A或者b太敏感了。又因为一般我们的系数矩阵A和b是从实验数据里面估计得到的，所以它是存在误差的，如果我们的系统对这个误差是可以容忍的就还好，但系统对这个误差太敏感了，以至于我们的解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是ill-conditioned病态的，不正常的，不稳定的，有问题的，哈哈。这清楚了吧。右边那个就叫well-condition的系统了。</p>
<p>​       还是再啰嗦一下吧，对于一个ill-condition的系统，我的输入稍微改变下，输出就发生很大的改变，这不好啊，这表明我们的系统不能实用啊。你想想看，例如对于一个回归问题y=f(x)，我们是用训练样本x去训练模型f，使得y尽量输出我们期待的值，例如0。那假如我们遇到一个样本x’，这个样本和训练样本x差别很小，面对他，系统本应该输出和上面的y差不多的值的，例如0.00001，最后却给我输出了一个0.9999，这很明显不对呀。就好像，你很熟悉的一个人脸上长了个青春痘，你就不认识他了，那你大脑就太差劲了，哈哈。所以如果一个系统是ill-conditioned病态的，我们就会对它的结果产生怀疑。那到底要相信它多少呢？我们得找个标准来衡量吧，因为有些系统的病没那么重，它的结果还是可以相信的，不能一刀切吧。终于回来了，上面的condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number值小的就是well-conditioned的，大的就是ill-conditioned的。</p>
<p>​       如果方阵A是非奇异的，那么A的conditionnumber定义为：</p>
<p><img src="http://img.blog.csdn.net/20140504122518843?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       也就是矩阵A的norm乘以它的逆的norm。所以具体的值是多少，就要看你选择的norm是什么了。如果方阵A是奇异的，那么A的condition number就是正无穷大了。实际上，每一个可逆方阵都存在一个condition number。但如果要计算它，我们需要先知道这个方阵的norm（范数）和Machine Epsilon（机器的精度）。为什么要范数？范数就相当于衡量一个矩阵的大小，我们知道矩阵是没有大小的，当上面不是要衡量一个矩阵A或者向量b变化的时候，我们的解x变化的大小吗？所以肯定得要有一个东西来度量矩阵和向量的大小吧？对了，他就是范数，表示矩阵大小或者向量长度。OK，经过比较简单的证明，对于AX=b，我们可以得到以下的结论：</p>
<p><img src="http://img.blog.csdn.net/20140504122535671?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       也就是我们的解x的相对变化和A或者b的相对变化是有像上面那样的关系的，其中k(A)的值就相当于倍率，看到了吗？相当于x变化的界。</p>
<p>​       对condition number来个一句话总结：conditionnumber是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的condition number在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是ill-conditioned的，如果一个系统是ill-conditioned的，它的输出结果就不要太相信了。</p>
<p>​       好了，对这么一个东西，已经说了好多了。对了，我们为什么聊到这个的了？回到第一句话：从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：</p>
<p><img src="http://img.blog.csdn.net/20140504122601437?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       然而，如果当我们的样本X的数目比每个样本的维度还要小的时候，矩阵XTX将会不是满秩的，也就是XTX会变得不可逆，所以w<em>就没办法直接计算出来了。或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数）。也就是说，我们的数据不足以确定一个解，如果我们从所有可行解里随机选一个的话，很可能并不是真正好的解，总而言之，我们过拟合了。</em></p>
<p>​       但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：</p>
<p><img src="http://img.blog.csdn.net/20140504122620968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       这里面，专业点的描述是：要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是λ=0的情况，如果矩阵XTX的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善condition number。</p>
<p>​       另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成λ-strongly convex（λ强凸）的了。哎哟哟，这里又出现个λ强凸，啥叫λ强凸呢？</p>
<p>​       当f满足：</p>
<p><img src="http://img.blog.csdn.net/20140504122800609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       时，我们称f为λ-stronglyconvex函数，其中参数λ&gt;0。当λ=0时退回到普通convex 函数的定义。</p>
<p>​       在直观的说明强凸之前，我们先看看普通的凸是怎样的。假设我们让f在x的地方做一阶泰勒近似（一阶泰勒展开忘了吗？f(x)=f(a)+f’(a)(x-a)+o(||x-a||).）：</p>
<p><img src="http://img.blog.csdn.net/20140504122816453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       直观来讲，convex 性质是指函数曲线位于该点处的切线，也就是线性近似之上，而 strongly convex 则进一步要求位于该处的一个二次函数上方，也就是说要求函数不要太“平坦”而是可以保证有一定的“向上弯曲”的趋势。专业点说，就是convex 可以保证函数在任意一点都处于它的一阶泰勒函数之上，而strongly convex可以保证函数在任意一点都存在一个非常漂亮的二次下界quadratic lower bound。当然这是一个很强的假设，但是同时也是非常重要的假设。可能还不好理解，那我们画个图来形象的理解下。</p>
<p><img src="http://img.blog.csdn.net/20140504122856125?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       大家一看到上面这个图就全明白了吧。不用我啰嗦了吧。还是啰嗦一下吧。我们取我们的最优解w的地方。如果我们的函数f(w)，见左图，也就是红色那个函数，都会位于蓝色虚线的那根二次函数之上，这样就算wt和w<em>离的比较近的时候，f(wt)和f(w</em>)的值差别还是挺大的，也就是会保证在我们的最优解w<em>附近的时候，还存在较大的梯度值，这样我们才可以在比较少的迭代次数内达到w</em>。但对于右图，红色的函数f(w)只约束在一个线性的蓝色虚线之上，假设是如右图的很不幸的情况（非常平坦），那在wt还离我们的最优点w<em>很远的时候，我们的近似梯度(f(wt)-f(w</em>))/(wt-w<em>)就已经非常小了，在wt处的近似梯度∂f/∂w就更小了，这样通过梯度下降wt+1=wt-α</em>(∂f/∂w)，我们得到的结果就是w的变化非常缓慢，像蜗牛一样，非常缓慢的向我们的最优点w<em>爬动，那在有限的迭代时间内，它离我们的最优点还是很远。</em></p>
<p>​       所以仅仅靠convex 性质并不能保证在梯度下降和有限的迭代次数的情况下得到的点w会是一个比较好的全局最小点w的近似点（插个话，有地方说，实际上让迭代在接近最优的地方停止，也是一种规则化或者提高泛化性能的方法）。正如上面分析的那样，如果f(w)在全局最小点w<em>周围是非常平坦的情况的话，我们有可能会找到一个很远的点。但如果我们有“强凸”的话，就能对情况做一些控制，我们就可以得到一个更好的近似解。至于有多好嘛，这里面有一个bound，这个 bound 的好坏也要取决于strongly convex性质中的常数α的大小。看到这里，不知道大家学聪明了没有。如果要获得strongly convex怎么做？最简单的就是往里面加入一项(α/2)</em>||w||2。</p>
<p>​       呃，讲个strongly convex花了那么多的篇幅。实际上，在梯度下降中，目标函数收敛速率的上界实际上是和矩阵XTX的 condition number有关，XTX的 condition number 越小，上界就越小，也就是收敛速度会越快。</p>
<p>这一个优化说了那么多的东西。还是来个一句话总结吧：L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。</p>
<p>​       好了，这里兑现上面的承诺，来直观的聊聊L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？我看到的有两种几何上直观的解析：</p>
<p><strong>1）下降速度：</strong></p>
<p>​       我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。不过我觉得这里解释的不太中肯，当然了也不知道是不是自己理解的问题。</p>
<p><img src="http://img.blog.csdn.net/20140504122926296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       L1在江湖上人称Lasso，L2人称Ridge。不过这两个名字还挺让人迷糊的，看上面的图片，Lasso的图看起来就像ridge，而ridge的图看起来就像lasso。</p>
<p><strong>2）模型空间的限制：</strong></p>
<p>​       实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式：</p>
<p><img src="http://img.blog.csdn.net/20140504122943984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解：</p>
<p><img src="http://img.blog.csdn.net/20140504123020546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>​       可以看到，L1-ball 与L2-ball 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。</p>
<p>​       相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。</p>
<p>​       因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">范数</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/机器学习//" class="article-tag-list-link color5">机器学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/06/L0-L1和L2范数/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-卷积神经网络4" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/卷积神经网络4/">特殊应用和神经网络转换</a>
    </h1>
  

        
        <a href="/2018/02/05/卷积神经网络4/" class="archive-article-date">
  	<time datetime="2018-02-05T08:48:49.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Part-1：人脸识别"><a href="#Part-1：人脸识别" class="headerlink" title="Part 1：人脸识别"></a><strong>Part 1：人脸识别</strong></h2><h2 id="1-人脸验证和人脸识别"><a href="#1-人脸验证和人脸识别" class="headerlink" title="1. 人脸验证和人脸识别"></a><strong>1. 人脸验证和人脸识别</strong></h2><p><strong>人脸验证（Verification）：</strong></p>
<ul>
<li>Input：图片、名字/ID；</li>
<li>Output：输入的图片是否是对应的人；</li>
<li>1 to 1 问题。</li>
</ul>
<p><strong>人脸识别（Recognition）：</strong></p>
<ul>
<li>拥有一个具有K个人的数据库；</li>
<li>输入一副人脸图片；</li>
<li>如果图片是任意这K个人中的一位，则输出对应人的ID。</li>
</ul>
<p>人脸识别问题对于人脸验证问题来说，具有更高的难度。如对于一个验证系统来说，如果我们拥有 <img src="http://www.zhihu.com/equation?tex=99%5C%25" alt="99\%"> 的精确度，那么这个验证系统已经具有了很高的精度；但是假设在另外一个识别系统中，如果我们把这个验证系统应用在具有K个人的识别系统中，那么系统犯错误的机会就变成了K倍。所以如果我们想在识别系统中得到更高的精度，那么就需要得到一个具有更高精度的验证系统。</p>
<h2 id="2-one-shot-learning"><a href="#2-one-shot-learning" class="headerlink" title="2. one shot learning"></a><strong>2. one shot learning</strong></h2><p>对于大多数的人脸识别系统都存在的一个问题就是one shot learning。</p>
<p><strong>什么是 one shot learning：</strong></p>
<p>对于一个人脸识别系统，我们需要仅仅通过先前的一张人脸的图片或者说一个人脸的样例，就能够实现该人的识别，那么这样的问题就是 one shot 问题。对于存在于数据库中的人脸图片，系统能够识别到对应的人；而不在数据库中的人脸图片，则系统给出无法通过识别的结果。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-f99ac74d722d0dad67ddfe4ddb0d4173_hd.jpg" alt="img"></p>
<p>对于one shot learning 问题，因为只有单个样本，是不足以训练一个稳健的卷积神经网络来进行不同人的识别过程。而且，在有新的样本成员加入的时候，往往还需要对网络进行重新训练。所以我们不能以传统的方法来实现识别系统。</p>
<p><strong>Similarity 函数：</strong></p>
<p>为了能够让人脸识别系统实现一次学习，需要让神经网络学习 <strong>Similarity </strong>函数：</p>
<ul>
<li>d(img1, img2)：两幅图片之间的差异度</li>
<li>输入：两幅图片</li>
<li>输出：两者之间的差异度</li>
<li>如果 <img src="http://www.zhihu.com/equation?tex=d%28img1%2C+img2%29+%5Cleqslant+%5Ctau" alt="d(img1, img2) \leqslant \tau"> ，则输出“same”;</li>
</ul>
<p>如果 <img src="http://www.zhihu.com/equation?tex=d%28img1%2C+img2%29+%3E+%5Ctau" alt="d(img1, img2) &gt; \tau"> ，则输出“different”.</p>
<p>对于人脸识别系统，通过将输入的人脸图片与数据库中所拥有的图片成对输入Similarity函数，两两对比，则可解决one shot problem。如果有新的人加入团队，则只需将其图片添加至数据库即可。</p>
<h2 id="3-Siamese-网络"><a href="#3-Siamese-网络" class="headerlink" title="3. Siamese 网络"></a><strong>3. Siamese 网络</strong></h2><p>利用Siamese 网络来实现 Similarity 函数。</p>
<p><strong>构建网络：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-f6939ce709933cee586636484584267e_hd.jpg" alt="img"></p>
<p>对于一个卷积神经网络结构，我们去掉最后的softmax层，将图片样本1输入网络，最后由网络输出一个N维的向量（图中实例以128表示），这N维向量则代表输入图片样本1的编码。将不同人的图片样本输入相同参数的网络结构，得到各自相应的图片编码。</p>
<p><strong>Similarity 函数实现：</strong></p>
<p>将Similarity 函数表示成两幅图片编码之差的范数：</p>
<p><img src="http://www.zhihu.com/equation?tex=d%28x1%2C+x2%29+%3D+%7C%7Cf%28x1%29-f%28x2%29%7C%7C_%7B2%7D%5E%7B2%7D" alt="d(x1, x2) = ||f(x1)-f(x2)||_{2}^{2}"></p>
<p>那么也就是说：</p>
<ul>
<li>我们的神经网络的参数定义了图片的编码；</li>
<li>学习网络的参数，使我们得到好的Similarity 函数：</li>
</ul>
<p>- 如果 <img src="http://www.zhihu.com/equation?tex=x_%7B1%7D%EF%BC%8Cx_%7B2%7D" alt="x_{1}，x_{2}"> 是同一个人的图片，那么得到的 <img src="http://www.zhihu.com/equation?tex=%7C%7Cf%28x1%29-f%28x2%29%7C%7C%5E%7B2%7D" alt="||f(x1)-f(x2)||^{2}"> 很小；</p>
<p>- 如果 <img src="http://www.zhihu.com/equation?tex=x_%7B1%7D%EF%BC%8Cx_%7B2%7D" alt="x_{1}，x_{2}"> 不是同一个人的图片，那么得到的 <img src="http://www.zhihu.com/equation?tex=%7C%7Cf%28x1%29-f%28x2%29%7C%7C%5E%7B2%7D" alt="||f(x1)-f(x2)||^{2}"> 很大。</p>
<h2 id="4-Triplet-损失"><a href="#4-Triplet-损失" class="headerlink" title="4. Triplet 损失"></a><strong>4. Triplet 损失</strong></h2><p>如何通过学习神经网络的参数，得到优质的人脸图片的编码？方法之一就是定义 Triplet 损失函数，并在其之上运用梯度下降。</p>
<p><strong>学习目标：</strong></p>
<p>为了使用Triplet 损失函数，我们需要比较成对的图像（三元组术语）：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a7ff8e78c52501e38564b545bcc5dce7_hd.jpg" alt="img"></p>
<ul>
<li>Anchor （A）： 目标图片；</li>
<li>Positive（P）：与Anchor 属于同一个人的图片；</li>
<li>Negative（N）：与Anchor不属于同一个人的图片。</li>
</ul>
<p>对于Anchor 和 Positive，我们希望二者编码的差异小一些；对于Anchor 和Negative，我们希望他们编码的差异大一些。所以我们的目标以编码差的范数来表示为：</p>
<p><img src="http://www.zhihu.com/equation?tex=d%28A%2CP%29%3D%7C%7Cf%28A%29+-+f%28P%29%7C%7C%5E%7B2%7D+%5Cleqslant+%7C%7Cf%28A%29+-+f%28N%29%7C%7C%5E%7B2%7D+%3D+d%28A%2CN%29" alt="d(A,P)=||f(A) - f(P)||^{2} \leqslant ||f(A) - f(N)||^{2} = d(A,N)"></p>
<p>也就是：</p>
<p><img src="http://www.zhihu.com/equation?tex=%7C%7Cf%28A%29+-+f%28P%29%7C%7C%5E%7B2%7D+-+%7C%7Cf%28A%29+-+f%28N%29%7C%7C%5E%7B2%7D+%5Cleqslant+0" alt="||f(A) - f(P)||^{2} - ||f(A) - f(N)||^{2} \leqslant 0"></p>
<p>上面的公式存在一个问题就是，当 <img src="http://www.zhihu.com/equation?tex=f%28A%29%3Df%28P%29%3Df%28N%29%3D0" alt="f(A)=f(P)=f(N)=0"> 时，也就是神经网络学习到的函数总是输出0时，或者 <img src="http://www.zhihu.com/equation?tex=f%28A%29%3Df%28P%29%3Df%28N%29" alt="f(A)=f(P)=f(N)"> 时，也满足上面的公式，但却不是我们想要的目标结果。所以为了防止出现这种情况，我们对上式进行修改，使得两者差要小于一个较小的负数：</p>
<p><img src="http://www.zhihu.com/equation?tex=%7C%7Cf%28A%29+-+f%28P%29%7C%7C%5E%7B2%7D+-+%7C%7Cf%28A%29+-+f%28N%29%7C%7C%5E%7B2%7D+%5Cleqslant+-+%5Calpha" alt="||f(A) - f(P)||^{2} - ||f(A) - f(N)||^{2} \leqslant - \alpha"></p>
<p>一般将 <img src="http://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> 写成 <img src="http://www.zhihu.com/equation?tex=%2B%5Calpha" alt="+\alpha"> ，称为“margin”，即：</p>
<p><img src="http://www.zhihu.com/equation?tex=%7C%7Cf%28A%29+-+f%28P%29%7C%7C%5E%7B2%7D+-+%7C%7Cf%28A%29+-+f%28N%29%7C%7C%5E%7B2%7D+%2B+%5Calpha+%5Cleqslant+0" alt="||f(A) - f(P)||^{2} - ||f(A) - f(N)||^{2} + \alpha \leqslant 0"></p>
<p>不同 <strong>margin </strong>值的设置对模型学习具有不同的效果，margin 的作用就是拉大了 Anchor与Positive 图片对 和 Anchor与Negative 图片对之间的差距。</p>
<p><strong>Triplet 损失函数：</strong></p>
<p>Triplet 损失函数的定义基于三张图片：Anchor、Positive、Negative。</p>
<p><img src="http://www.zhihu.com/equation?tex=L%28A%2CP%2CN%29+%3D+%5Cmax+%28%7C%7Cf%28A%29+-+f%28P%29%7C%7C%5E%7B2%7D+-+%7C%7Cf%28A%29+-+f%28N%29%7C%7C%5E%7B2%7D+%2B+%5Calpha%2C+%5C+0%29" alt="L(A,P,N) = \max (||f(A) - f(P)||^{2} - ||f(A) - f(N)||^{2} + \alpha, \ 0)"></p>
<p><strong>整个网络的代价函数：</strong></p>
<p><img src="http://www.zhihu.com/equation?tex=J+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7DL%28A%5E%7B%28i%29%7D%2CP%5E%7B%28i%29%7D%2CN%5E%7B%28i%29%7D%29" alt="J = \sum\limits_{i=1}^{m}L(A^{(i)},P^{(i)},N^{(i)})"></p>
<p>假设我们有一个10000张片的训练集，里面是1000个不同的人的照片样本。我们需要做的就是从这10000张训练集中抽取图片生成（A,P,N）的三元组，来训练我们的学习算法，并在Triplet 损失函数上进行梯度下降。</p>
<p>注意：为了训练我们的网络，我们必须拥有Anchor和Positive对，所以这里我们必须有每个人的多张照片，而不能仅仅是一张照片，否则无法训练网络。</p>
<p><strong>三元组（A,P,N）的选择：</strong></p>
<p>在训练的过程中，如果我们随机地选择图片构成三元组（A,P,N），那么对于下面的条件是很容易满足的：</p>
<p><img src="http://www.zhihu.com/equation?tex=d%28A%2CP%29+%2B+%5Calpha+%5Cleqslant+d%28A%2CN%29" alt="d(A,P) + \alpha \leqslant d(A,N)"></p>
<p>所以，为了更好地训练网络，我们需要选择那些训练有“难度”的三元组，也就是选择的三元组满足：</p>
<p><img src="http://www.zhihu.com/equation?tex=d%28A%2CP%29+%5Capprox+d%28A%2CN%29" alt="d(A,P) \approx d(A,N)"></p>
<ul>
<li>算法将会努力使得 <img src="http://www.zhihu.com/equation?tex=d%28A%2CN%29" alt="d(A,N)"> 变大，或者使得 <img src="http://www.zhihu.com/equation?tex=d%28A%2CN%29+%2B+%5Calpha" alt="d(A,N) + \alpha"> 变小，从而使两者之间至少有一个 <img src="http://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> 的间隔；</li>
<li>增加学习算法的计算效率，避免那些太简单的三元组。</li>
</ul>
<p>最终通过训练，我们学习到的参数，会使得对于同一个人的图片，编码的距离很小；对不同人的图片，编码的距离就很大。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-66e71bb3504c077cc01c2b0842a73a56_hd.jpg" alt="img"></p>
<p>对于大型的人脸识别系统，常常具有上百万甚至上亿的训练数据集，我们并我容易得到。所以对于该领域，我们常常是下载别人在网上上传的预训练模型，而不是从头开始。</p>
<h2 id="5-脸部验证和二分类"><a href="#5-脸部验证和二分类" class="headerlink" title="5. 脸部验证和二分类"></a><strong>5. 脸部验证和二分类</strong></h2><p>除了利用 Triplet 损失函数来学习人脸识别卷积网络参数的方法外，还有其他的方式。我们可以将人脸识别问题利用Siamese网络当成一个二分类问题，同样可以实现参数的学习。</p>
<p><strong>Siamese 二分类改进：</strong></p>
<p>对两张图片应用Siamese 网络，计算得到两张图片的N维编码，然后将两个编码输入到一个logistic regression 单元中，然后进行预测。如果是相同的人，那么输出是1；如果是不同的人，输出是0。那么这里我们就将人脸识别的问题，转化为一个二分类问题。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d57edc4f3b7a44456304a1409250f9a2_hd.jpg" alt="img"></p>
<p>对于最后的sigmoid函数，我们可以进行如下计算：</p>
<p><img src="http://www.zhihu.com/equation?tex=%5Chat+y+%3D+%5Csigma%28%5Csum%5Climits_%7Bk%3D1%7D%5E%7BN%7D+w_%7Bi%7D%7Cf%28x%5E%7B%28i%29%7D%29_%7Bk%7D+-+f%28x%5E%7B%28j%29%7D%29_%7Bk%7D+%7C+%2B+b%29" alt="\hat y = \sigma(\sum\limits_{k=1}^{N} w_{i}|f(x^{(i)})_{k} - f(x^{(j)})_{k} | + b)"></p>
<p>其中， <img src="http://www.zhihu.com/equation?tex=f%28x%5E%7B%28i%29%7D%29" alt="f(x^{(i)})"> 代表图片 <img src="http://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D" alt="x^{(i)}"> 的编码，下标 <img src="http://www.zhihu.com/equation?tex=k" alt="k"> 代表选择N维编码向量中的第 <img src="http://www.zhihu.com/equation?tex=k" alt="k"> 个元素。</p>
<p>我们以两个图片编码向量对应元素之间的差值作为特征输入到logistic regression 的单元中，增加参数 <img src="http://www.zhihu.com/equation?tex=w_%7Bi%7D" alt="w_{i}"> 和 <img src="http://www.zhihu.com/equation?tex=b" alt="b"> ，通过训练得到合适的参数权重和偏置，进而判断两张图片是否为同一个人。</p>
<p>同时输入逻辑回归单元的特征可以进行更改，如还可以是：</p>
<p><img src="http://www.zhihu.com/equation?tex=%5Cdfrac%7B%28f%28x%5E%7B%28i%29%7D%29_%7Bk%7D+-+f%28x%5E%7B%28j%29%7D%29_%7Bk%7D%29%5E%7B2%7D%7D%7Bf%28x%5E%7B%28i%29%7D%29_%7Bk%7D+%2B+f%28x%5E%7B%28j%29%7D%29_%7Bk%7D%7D" alt="\dfrac{(f(x^{(i)})_{k} - f(x^{(j)})_{k})^{2}}{f(x^{(i)})_{k} + f(x^{(j)})_{k}}"></p>
<p>上式也被称为 <img src="http://www.zhihu.com/equation?tex=%5Cchi" alt="\chi"> 方公式，有时也称为 <img src="http://www.zhihu.com/equation?tex=%5Cchi" alt="\chi"> 方相似度。</p>
<p>在实际的人脸验证系统中，我们可以对数据库的人脸图片进行预计算，存储卷积网络得到的编码。当有图片进行识别时，运用卷积网络计算新图片的编码，与预计算保存好的编码输入到逻辑回归单元中进行预测。这样可以提高我们系统预测的效率，节省计算时间。</p>
<p><strong>总结：</strong></p>
<p>利用Siamese 网络，我们可以将人脸验证当作一个监督学习，创建成对的训练集和是否同一个人的输出标签。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-27a30af7b011078e36550c4d90822619_hd.jpg" alt="img"></p>
<p>我们利用不同的图片对，使用反向传播的算法对Siamese网络进行训练，进而得到人脸验证系统。</p>
<hr>
<h2 id="Part-2：神经风格迁移"><a href="#Part-2：神经风格迁移" class="headerlink" title="Part 2：神经风格迁移"></a><strong>Part 2：神经风格迁移</strong></h2><h2 id="6-深度网络学习内容可视化"><a href="#6-深度网络学习内容可视化" class="headerlink" title="6. 深度网络学习内容可视化"></a><strong>6. 深度网络学习内容可视化</strong></h2><p><strong>如何可视化：</strong></p>
<p>假设我们训练了一个卷积神经网络如下所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-7d88dd1be307864f0d913624906ad685_hd.jpg" alt="img"></p>
<p>我们希望看到不同层的隐藏单元的计算结果。依次对各个层进行如下操作：</p>
<ul>
<li>在当前层挑选一个隐藏单元；</li>
<li>遍历训练集，找到最大化地激活了该运算单元的图片或者图片块；</li>
<li>对该层的其他运算单元执行操作。</li>
</ul>
<p>对于在第一层的隐藏单元中，其只能看到卷积网络的小部分内容，也就是最后我们找到的那些最大化激活第一层隐层单元的是一些小的图片块。我们可以理解为第一层的神经单元通常会寻找一些简单的特征，如边缘或者颜色阴影等。</p>
<p><strong>各层网络可视化：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-5ffb8a100d2d392a5e55204452292727_hd.jpg" alt="img"></p>
<p>对于卷积网络的各层单元，随着网络深度的增加，隐藏层计算单元随着层数的增加，从简单的事物逐渐到更加复杂的事物。</p>
<h2 id="7-神经风格迁移代价函数"><a href="#7-神经风格迁移代价函数" class="headerlink" title="7. 神经风格迁移代价函数"></a><strong>7. 神经风格迁移代价函数</strong></h2><p><strong>代价函数：</strong></p>
<p>为了实现神经风格迁移，我们需要为生成的图片定义一个代价函数。</p>
<p>对于神经风格迁移，我们的目标是由内容图片C和风格图片S，生成最终的风格迁移图片G：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-8c42dfc92fc1310b95dbdf8d01cceefb_hd.jpg" alt="img"></p>
<p>所以为了实现神经风格迁移，我们需要定义关于G的代价函数J，以用来评判生成图片的好坏：</p>
<p><img src="http://www.zhihu.com/equation?tex=J%28G%29+%3D+%5Calpha+J_%7Bcontent%7D%28C%2C+G%29+%2B+%5Cbeta+J_%7Bstyle%7D%28S%2CG%29" alt="J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S,G)"></p>
<p>其中</p>
<ul>
<li><img src="http://www.zhihu.com/equation?tex=J_%7Bcontent%7D%28C%2C+G%29+" alt="J_{content}(C, G) "> 代表生成图片G的内容和内容图片C的内容的相似度；</li>
<li><img src="http://www.zhihu.com/equation?tex=+J_%7Bstyle%7D%28S%2CG%29" alt=" J_{style}(S,G)"> 代表生成图片G的内容和风格图片S的内容的相似度；</li>
<li><img src="http://www.zhihu.com/equation?tex=%5Calpha%E3%80%81%5Cbeta" alt="\alpha、\beta"> 两个超参数用来表示以上两者之间的权重。</li>
</ul>
<p><strong>执行过程：</strong></p>
<ul>
<li>随机初始化生成图片G，如大小为 <img src="http://www.zhihu.com/equation?tex=100%5Ctimes100%5Ctimes3" alt="100\times100\times3"> ；</li>
<li>使用梯度下降算法最小化上面定义的代价函数 J(G)， <img src="http://www.zhihu.com/equation?tex=G%EF%BC%9A%3D+G+-+%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial+G%7DJ%28G%29" alt="G：= G - \dfrac{\partial}{\partial G}J(G)"> ；</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ffcbd5e984c3af324601256db6c10531_hd.jpg" alt="img"></p>
<p>对于上图的内容图片C和风格图片S，通过梯度下降算法一次次的徐训练，我们可以由初始的噪声图片得到最终的风格迁移图片G。</p>
<h2 id="8-内容代价函数（Content-cost）"><a href="#8-内容代价函数（Content-cost）" class="headerlink" title="8. 内容代价函数（Content cost）"></a><strong>8. 内容代价函数（Content cost）</strong></h2><ul>
<li>假设我们使用隐藏层 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> 来计算内容代价。（如果选择的$l$ 太小，那么代价函数就会使得我们的生成图片G在像素上非常接近内容图片；然而用很深的网络，那么生成图片G中就会产生与内容图片中所拥有的物体。所以对于 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> 一般选在网络的中间层，既不深也不浅）；</li>
<li>使用一个预训练的卷积网络。（如，VGG或其他）；</li>
<li>令 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%28C%29%7D" alt="a^{[l](C)}"> 和 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%28G%29%7D" alt="a^{[l](G)}"> 分别代表内容图片C和生成图片G的 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> 层的激活值；</li>
<li>如果 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%28C%29%7D" alt="a^{[l](C)}"> 和 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%28G%29%7D" alt="a^{[l](G)}"> 相似，那么两张图片就有相似的内容；</li>
</ul>
<p>定义内容代价函数如下：</p>
<p><img src="http://www.zhihu.com/equation?tex=J_%7Bcontent%7D%28C%2C+G%29+%3D+%5Cdfrac%7B1%7D%7B2%7D%7C%7Ca%5E%7B%5Bl%5D%28C%29%7D+-+a%5E%7B%5Bl%5D%28G%29%7D+%7C%7C%5E%7B2%7D" alt="J_{content}(C, G) = \dfrac{1}{2}||a^{[l](C)} - a^{[l](G)} ||^{2}"></p>
<p>在对代价函数运行梯度下降算法时，会激励这里的内容代价函数，努力使得生成图片G隐含层<img src="http://www.zhihu.com/equation?tex=l" alt="l"> 的激活值和内容图片C隐含层<img src="http://www.zhihu.com/equation?tex=l" alt="l"> 的激活值相似。</p>
<h2 id="9-风格代价函数（Style-cost）"><a href="#9-风格代价函数（Style-cost）" class="headerlink" title="9. 风格代价函数（Style cost）"></a><strong>9. 风格代价函数（Style cost）</strong></h2><p><strong>“Style”的含义：</strong></p>
<p>对于一个卷积网络中，我们选择网络的中间层 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> ， 定义“Style”表示 <img src="http://www.zhihu.com/equation?tex=+l" alt=" l"> 层的各个通道激活项之间的相关性。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2eb6aa70a73a023343d5e9e79db7b628_hd.jpg" alt="img"></p>
<p><strong>相关性大小的度量：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-fdd5fb6f3ebbd35408f69b1938579205_hd.jpg" alt="img"></p>
<p>上面是我们选出的 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> 层的激活项，对于不同的通道值，代表不同的神经元所学习到的特征，这里假如红色的通道可以找到图片中含有垂直纹理特征的区域，黄色通道可以找出橙色的区域。</p>
<p>而相关性大小的含义就是，如假设中，图片出现垂直纹理特征的区域显示橙色可能的大小。</p>
<p>我们将相关系数应用到风格图片S和生成图片G的对应通道上，就可以度量风格图片和生成图片的相似度。</p>
<p><strong>Style 矩阵：</strong></p>
<ul>
<li>令 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D_%7Bi%2Cj%2Ck%7D" alt="a^{[l]}_{i,j,k}"> 表示在（i,j,k）位置的激活值，其中i、j、k分别代表激活值的高、宽、通道；</li>
<li><img src="http://www.zhihu.com/equation?tex=G%5E%7B%5Bl%5D%7D" alt="G^{[l]}"> 是一个 <img src="http://www.zhihu.com/equation?tex=n_%7Bc%7D%5E%7Bl%7D%5Ctimes+n_%7Bc%7D%5E%7Bl%7D" alt="n_{c}^{l}\times n_{c}^{l}"> 大小的矩阵：</li>
</ul>
<p><img src="http://www.zhihu.com/equation?tex=G%5E%7B%5Bl%5D%28S%29%7D_%7Bkk%27%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn_%7Bh%7D%5E%7B%5Bl%5D%7D%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn_%7Bw%7D%5E%7B%5Bl%5D%7D%7Da%5E%7B%5Bl%5D%28S%29%7D_%7Bi%2Cj%2Ck%7Da%5E%7B%5Bl%5D%28S%29%7D_%7Bi%2Cj%2Ck%27%7D" alt="G^{[l](S)}_{kk&#39;} = \sum\limits_{i=1}^{n_{h}^{[l]}}\sum\limits_{j=1}^{n_{w}^{[l]}}a^{[l](S)}_{i,j,k}a^{[l](S)}_{i,j,k&#39;}"></p>
<p><img src="http://www.zhihu.com/equation?tex=G%5E%7B%5Bl%5D%28G%29%7D_%7Bkk%27%7D+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn_%7Bh%7D%5E%7B%5Bl%5D%7D%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn_%7Bw%7D%5E%7B%5Bl%5D%7D%7Da%5E%7B%5Bl%5D%28G%29%7D_%7Bi%2Cj%2Ck%7Da%5E%7B%5Bl%5D%28G%29%7D_%7Bi%2Cj%2Ck%27%7D" alt="G^{[l](G)}_{kk&#39;} = \sum\limits_{i=1}^{n_{h}^{[l]}}\sum\limits_{j=1}^{n_{w}^{[l]}}a^{[l](G)}_{i,j,k}a^{[l](G)}_{i,j,k&#39;}"></p>
<p>上面的矩阵在线性代数中又称为Gram 矩阵，这里称为风格矩阵。</p>
<p><strong>代价函数：</strong></p>
<p><img src="http://www.zhihu.com/equation?tex=J%5E%7B%5Bl%5D%7D_%7Bstyle%7D%28S%2C+G%29+%3D+%5Cdfrac%7B1%7D%7B2n%5E%7B%5Bl%5D%7D_%7Bh%7Dn%5E%7B%5Bl%5D%7D_%7Bw%7Dn%5E%7B%5Bl%5D%7D_%7Bc%7D%7D%7C%7CG%5E%7B%5Bl%5D%28S%29%7D+-+G%5E%7B%5Bl%5D%28G%29%7D+%7C%7C%5E%7B2%7D_%7BF%7D+%3D+%5Cdfrac%7B1%7D%7B2n%5E%7B%5Bl%5D%7D_%7Bh%7Dn%5E%7B%5Bl%5D%7D_%7Bw%7Dn%5E%7B%5Bl%5D%7D_%7Bc%7D%7D%5Csum_%7Bk%7D%5Csum_%7Bk%27%7D%28G%5E%7B%5Bl%5D%28S%29%7D_%7Bkk%27%7D+-+G%5E%7B%5Bl%5D%28G%29%7D_%7Bkk%27%7D%29%5E%7B2%7D" alt="J^{[l]}_{style}(S, G) = \dfrac{1}{2n^{[l]}_{h}n^{[l]}_{w}n^{[l]}_{c}}||G^{[l](S)} - G^{[l](G)} ||^{2}_{F} = \dfrac{1}{2n^{[l]}_{h}n^{[l]}_{w}n^{[l]}_{c}}\sum_{k}\sum_{k&#39;}(G^{[l](S)}_{kk&#39;} - G^{[l](G)}_{kk&#39;})^{2}"></p>
<p>内容代价函数和风格代价函数前面的归一化可以加也可以不加，因为总体的代价函数前面有权重系数。</p>
<p>如果对各层都使用风格代价函数，那么会让结果变得更好：</p>
<p><img src="http://www.zhihu.com/equation?tex=J_%7Bstyle%7D%28S%2CG%29+%3D+%5Csum_%7Bl%7D%5Clambda%5E%7B%5Bl%5D%7DJ_%7Bstyle%7D%5E%7B%5Bl%5D%7D%28S%2CG%29" alt="J_{style}(S,G) = \sum_{l}\lambda^{[l]}J_{style}^{[l]}(S,G)"></p>
<h2 id="10-1D-to-3D-卷积"><a href="#10-1D-to-3D-卷积" class="headerlink" title="10. 1D to 3D 卷积"></a><strong>10. 1D to 3D 卷积</strong></h2><p>在我们上面学过的卷积中，多数是对图形应用2D的卷积运算。同时，我们所应用的卷积运算还可以推广到1D和3D的情况。</p>
<p><strong>2D和1D卷积：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-dcbcc494a2faa24d6ed3dbaf5ef9a6cd_hd.jpg" alt="img"></p>
<ul>
<li>2D卷积： <img src="http://www.zhihu.com/equation?tex=14%5Ctimes14%5Ctimes3%2A+5%5Ctimes5%5Ctimes3+%E2%80%94%E2%80%94%3E10%5Ctimes10%5Ctimes+n_%7Bc%7D" alt="14\times14\times3* 5\times5\times3 ——&gt;10\times10\times n_{c}"> ;</li>
<li>1D卷积： <img src="http://www.zhihu.com/equation?tex=14%5Ctimes1%2A+5%5Ctimes1+%E2%80%94%E2%80%94%3E10%5Ctimes+n_%7Bc%7D" alt="14\times1* 5\times1 ——&gt;10\times n_{c}"> 。</li>
</ul>
<p><strong>3D卷积：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-f3d2c7c5d3ce175a00c985955b0681cb_hd.jpg" alt="img"></p>
<ul>
<li>3D卷积： <img src="http://www.zhihu.com/equation?tex=14%5Ctimes14%5Ctimes14%5Ctimes1+%2A+5%5Ctimes5%5Ctimes5%5Ctimes1%E2%80%94%E2%80%94%3E10%5Ctimes10%5Ctimes10%5Ctimes+n_%7Bc%7D" alt="14\times14\times14\times1 * 5\times5\times5\times1——&gt;10\times10\times10\times n_{c}">；</li>
<li>3D数据：如医疗CT扫描中的即可产生身体的3D模型；电影切片也属于3D数据。</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/卷积神经网络4/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-卷积神经网络3" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/卷积神经网络3/">目标检测</a>
    </h1>
  

        
        <a href="/2018/02/05/卷积神经网络3/" class="archive-article-date">
  	<time datetime="2018-02-05T08:48:42.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-目标定位和特征点检测"><a href="#1-目标定位和特征点检测" class="headerlink" title="1. 目标定位和特征点检测"></a><strong>1. 目标定位和特征点检测</strong></h2><p><strong>图片检测问题：</strong></p>
<ul>
<li>分类问题：判断图中是否为汽车；</li>
<li>目标定位：判断是否为汽车，并确定具体位置；</li>
<li>目标检测：检测不同物体并定位。</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-8bbaccec33b79c571936e6f8540baf8e_hd.jpg" alt="img"></p>
<p><strong>目标分类和定位：</strong></p>
<p>对于目标定位问题，我们卷积神经网络模型结构可能如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-9741b5618f05011318b2e491f0305ce9_hd.jpg" alt="img"></p>
<p>输出：包含图片中存在的对象及定位框</p>
<ul>
<li>行人，0 or 1；</li>
<li>汽车，0 or 1；</li>
<li>摩托车，0 or 1；</li>
<li>图片背景，0 or 1；</li>
<li>定位框： <img src="http://www.zhihu.com/equation?tex=b_%7Bx%7D%E3%80%81b_%7By%7D%E3%80%81b_%7Bh%7D%E3%80%81b_%7Bw%7D" alt="b_{x}、b_{y}、b_{h}、b_{w}"></li>
</ul>
<p>其中， <img src="http://www.zhihu.com/equation?tex=b_%7Bx%7D%E3%80%81b_%7By%7D" alt="b_{x}、b_{y}"> 表示汽车中点， <img src="http://www.zhihu.com/equation?tex=b_%7Bh%7D%E3%80%81b_%7Bw%7D" alt="b_{h}、b_{w}"> 分别表示定位框的高和宽。以图片左上角为(0,0)，以右下角为(1,1)，这些数字均为位置或长度所在图片的比例大小。</p>
<p><strong>目标标签 y：</strong></p>
<p><img src="http://www.zhihu.com/equation?tex=y+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+P_%7Bc%7D%5C%5C%5C+b_%7Bx%7D%5C%5C%5C+b_%7By%7D%5C%5C%5C+b_%7Bh%7D%5C%5C%5C+b_%7Bw%7D%5C%5C%5C+c_%7B1%7D%5C%5C%5C+c_%7B2%7D%5C%5C%5C+c_%7B3%7D+%5Cend%7Barray%7D+%5Cright%5D%5Cleft%7C+%5Cbegin%7Barray%7D%7Bl%7D+%E6%98%AF%E5%90%A6%E5%90%AB%E6%9C%89%E5%AF%B9%E8%B1%A1%EF%BC%8C0+or+1%5C%5C+%5C%5C+%5C%5C+%5C%5C+%5C%5C+%E6%98%AF%E5%90%A6%E6%9C%89%E8%A1%8C%E4%BA%BA%EF%BC%8C0+or+1%5C%5C+%E6%98%AF%E5%90%A6%E6%9C%89%E6%B1%BD%E8%BD%A6%EF%BC%8C0+or+1%5C%5C+%E6%98%AF%E5%90%A6%E6%9C%89%E6%91%A9%E6%89%98%EF%BC%8C0+or+1+%5Cend%7Barray%7D+%5Cright." alt="y = \left[ \begin{array}{l} P_{c}\\\ b_{x}\\\ b_{y}\\\ b_{h}\\\ b_{w}\\\ c_{1}\\\ c_{2}\\\ c_{3} \end{array} \right]\left| \begin{array}{l} 是否含有对象，0 or 1\\ \\ \\ \\ \\ 是否有行人，0 or 1\\ 是否有汽车，0 or 1\\ 是否有摩托，0 or 1 \end{array} \right."></p>
<ul>
<li>当 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D%3D1" alt="P_{c}=1"> 时，表示图片中存在物体；</li>
<li>当 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D%3D0" alt="P_{c}=0"> 时，表示图片中不存在物体，那么此时，输出 <img src="http://www.zhihu.com/equation?tex=y" alt="y"> 的其他值为多少均没有意义，也不会参与损失函数的计算：</li>
</ul>
<p><img src="http://www.zhihu.com/equation?tex=y+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+0%5C%5C+%3F%5C%5C+%3F%5C%5C+...%5C%5C+%3F+%5Cend%7Barray%7D+%5Cright%5D" alt="y = \left[ \begin{array}{l} 0\\ ?\\ ?\\ ...\\ ? \end{array} \right]"></p>
<p><strong>损失函数：</strong></p>
<p>如果采用平方误差形式的损失函数：</p>
<ul>
<li>当 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D%3D1" alt="P_{c}=1"> 时：<img src="http://www.zhihu.com/equation?tex=L%28%5Chat+y%2Cy%29%3D%28%5Chat+y_%7B1%7D-y_%7B1%7D%29%5E%7B2%7D%2B%28%5Chat+y_%7B2%7D-y_%7B2%7D%29%5E%7B2%7D%2B%5Ccdots%2B%28%5Chat+y_%7B8%7D-y_%7B8%7D%29%5E%7B2%7D" alt="L(\hat y,y)=(\hat y_{1}-y_{1})^{2}+(\hat y_{2}-y_{2})^{2}+\cdots+(\hat y_{8}-y_{8})^{2}"></li>
</ul>
<p>此时，我们需要关注神经网络对所有输出值的准确度；</p>
<ul>
<li>当 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D%3D0" alt="P_{c}=0"> 时：<img src="http://www.zhihu.com/equation?tex=L%28%5Chat+y%2Cy%29%3D%28%5Chat+y_%7B1%7D-y_%7B1%7D%29%5E%7B2%7D" alt="L(\hat y,y)=(\hat y_{1}-y_{1})^{2}"></li>
</ul>
<p>此时，我们只关注神经网络对背景值的准确度。</p>
<p>当然在实际的目标定位应用中，我们可以使用更好的方式是：</p>
<ul>
<li>对 <img src="http://www.zhihu.com/equation?tex=c_%7B1%7D%E3%80%81c_%7B2%7D%E3%80%81c_%7B3%7D" alt="c_{1}、c_{2}、c_{3}"> 和softmax使用<strong>对数似然损失函数</strong>；</li>
<li>对边界框的四个值应用<strong>平方误差</strong>或者类似的方法；</li>
<li>对 <img src="http://www.zhihu.com/equation?tex=P_c" alt="P_c"> 应用<strong>logistic regression损失函数</strong>，或者<strong>平方预测误差</strong>。</li>
</ul>
<p><strong>特征点检测：</strong></p>
<p>由前面的目标定位问题，我们可以知道，神经网络可以通过输出图片上特征点的坐标（x,y），来实现对目标特征的识别和定位标记。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-6e55c8d64e63fc7a8eb79bf33bbda57b_hd.jpg" alt="img"></p>
<p>如对于人脸表情识别的问题中，我们通过标定训练数据集中特征点的位置信息，来对人脸进行不同位置不同特征的定位和标记。AR的应用就是基于人脸表情识别来设计的，如脸部扭曲、增加头部配饰等。</p>
<p>在人体姿态检测中，同样可以通过对人体不同的特征位置关键点的标注，来记录人体的姿态。</p>
<h2 id="2-目标检测"><a href="#2-目标检测" class="headerlink" title="2. 目标检测"></a><strong>2. 目标检测</strong></h2><p>目标检测采用的是基于滑动窗口的检测算法。</p>
<p><strong>训练模型：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-1eae5bc8ab22844bf93f5c318354b09f_hd.jpg" alt="img"></p>
<ul>
<li>训练集X：将有汽车的图片进行适当的剪切，剪切成整张几乎都被汽车占据的小图或者没有汽车的小图；</li>
<li>训练集Y：对X中的图片进行标注，有汽车的标注1，没有汽车的标注0。</li>
</ul>
<p><strong>滑动窗口目标检测：</strong></p>
<p>利用滑动窗口在实际图片中实现目标检测。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-53d92d8debf3e6eddd847e3bcac3dbfc_hd.jpg" alt="img"></p>
<ul>
<li>首先选定一个特定大小的窗口，将窗口内的图片输入到模型中进行预测；</li>
<li>以固定步幅滑动该窗口，遍历图像的每个区域，对窗内的各个小图不断输入模型进行预测；</li>
<li>继续选取一个更大的窗口，再次遍历图像的每个区域，对区域内是否有车进行预测；</li>
<li>遍历整个图像，可以保证在每个位置都能检测到是否有车。</li>
</ul>
<p>缺点：计算成本巨大，每个窗口的小图都要进行卷积运算，（但在神经网络兴起之前，使用的是线性分类器，所以滑动窗口算法的计算成本较低）。</p>
<p><strong>卷积层替代全连接层：</strong></p>
<p>对于卷积网络中全连接层，我们可以利用 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 大小卷积核的卷积层来替代。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e7eba2a4221dc203855463b3173afc22_hd.jpg" alt="img"></p>
<p>在上一周课程中，吴恩达老师讲授过 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 的卷积核相当于在一个三维图像的切片上应用了一个全连接的神经网络。同样，全连接层也可以由 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 大小卷积核的卷积层来替代。需注意卷积核的个数与隐层神经元个数相同。</p>
<p><strong>滑动窗口的卷积实现：</strong></p>
<p>在我们实现了以卷积层替代全部的全连接层以后，在该基础上进行滑动窗口在卷积层上的操作。下面以一个小的图片为例：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-6449d6e63fe143c92dfe7c5db9dcaf9a_hd.jpg" alt="img"></p>
<p>我们以上面训练好的模型，输入一个 <img src="http://www.zhihu.com/equation?tex=16%5Ctimes16%5Ctimes3" alt="16\times16\times3"> 大小的整幅图片，图中蓝色部分代表滑动窗口的大小。我们以2为大小的步幅滑动窗口，分别与卷积核进行卷积运算，最后得到4幅 <img src="http://www.zhihu.com/equation?tex=10%5Ctimes10%5Ctimes16" alt="10\times10\times16"> 大小的特征图，然而因为在滑动窗口的操作时，输入部分有大量的重叠，也就是有很多重复的运算，导致在下一层中的特征图值也存在大量的重叠，所以最后得到的第二层激活值（特征图）构成一副 <img src="http://www.zhihu.com/equation?tex=12%5Ctimes12%5Ctimes16" alt="12\times12\times16"> 大小的特征图。对于后面的池化层和全连接层也是同样的过程。</p>
<p>那么由此可知，滑动窗口在整幅图片上进行滑动卷积的操作过程，就等同于在该图片上直接进行卷积运算的过程。所以卷积层实现滑动窗口的这个过程，我们不需要把输入图片分割成四个子集分别执行前向传播，而是把他们作为一张图片输入到卷积神经网络中进行计算，其中的重叠部分（公共区域）可以共享大量的计算。</p>
<p><strong>汽车目标检测：</strong></p>
<p>依据上面的方法，我们将整张图片输入到训练好的卷积神经网络中。无需再利用滑动窗口分割图片，只需一次前向传播，我们就可以同时得到所有图片子集的预测值。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-c90f2f9e1c48bc7caaea1e64e1721403_hd.jpg" alt="img"></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-79dd54e97a7dc03eb192b73161aabd48_hd.jpg" alt="img"></p>
<p>利用卷积的方式实现滑动窗口算法的方法，提高了整体的计算效率。</p>
<h2 id="3-Bounding-Box-预测"><a href="#3-Bounding-Box-预测" class="headerlink" title="3. Bounding Box 预测"></a><strong>3. Bounding Box 预测</strong></h2><p>前面一节的卷积方式实现的滑动窗口算法，使得在预测时计算的效率大大提高。但是其存在的问题是：不能输出最精准的边界框（Bounding Box）。</p>
<p>在滑动窗口算法中，我们取的一些离散的图片子集的位置，在这种情况下，有可能我们没有得到一个能够完美匹配汽车位置的窗口，也有可能真实汽车的边界框为一个长方形。所以我们需要寻找更加精确的边界框。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-054b6ee470945e1237787a09d2a0cb21_hd.jpg" alt="img"></p>
<p><strong>YOLO：</strong></p>
<p>YOLO算法可以使得滑动窗口算法寻找到更加精准的边界框。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ea55629d01beef467d9273eb7b39d29d_hd.jpg" alt="img"></p>
<ul>
<li>在整幅图片上加上较为精细的网格，将图片分割成 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 个小的图片；</li>
<li>采用图像分类和定位算法，分别应用在图像的 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 个格子中。</li>
<li>定义训练标签：（对于每个网格，定义如前面的向量 <img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D" alt="y_{i}"> ）</li>
</ul>
<p><img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+P_%7Bc%7D%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D%5C+c_%7B1%7D%5C+c_%7B2%7D%5C+c_%7B3%7D+%5Cend%7Barray%7D+%5Cright%5D" alt="y_{i} = \left[ \begin{array}{l} P_{c}\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ c_{1}\ c_{2}\ c_{3} \end{array} \right]"></p>
<p>对于不同的网格 <img src="http://www.zhihu.com/equation?tex=i" alt="i"> 有不同的标签向量 <img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D" alt="y_{i}"> 。</p>
<ul>
<li>将 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 个格子标签合并在一起，最终的目标输出Y的大小为： <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n%5Ctimes+8" alt="n\times n\times 8"> （这里8是因为例子中的目标值有8个）。</li>
</ul>
<p>通过这样的训练集训练得到目标探测的卷积网络模型。我们利用训练好的模型，将与模型输入相同大小的图片输入到训练好的网络中，得到大小为 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n%5Ctimes+8" alt="n\times n\times 8"> 的预测输出。通过观察 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 不同位置的输出值，我们就能知道这些位置中是否存在目标物体，然后也能由存在物体的输出向量得到目标物体的更加精准的边界框。</p>
<p><strong>YOLO notation：</strong></p>
<ul>
<li>将对象分配到一个格子的过程是：观察对象的中点，将该对象分配到其中点所在的格子中，（即使对象横跨多个格子，也只分配到中点所在的格子中，其他格子记为无该对象，即标记为“0”）；</li>
<li>YOLO显式地输出边界框，使得其可以具有任意宽高比，并且能输出更精确的坐标，不受滑动窗口算法滑动步幅大小的限制；</li>
<li>YOLO是一次卷积实现，并不是在 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 网格上进行 <img src="http://www.zhihu.com/equation?tex=n%5E%7B2%7D" alt="n^{2}"> 次运算，而是单次卷积实现，算法实现效率高，运行速度快，可以实现实时识别。</li>
</ul>
<p><strong>bounding boxes 细节：</strong></p>
<p>利用YOLO算法实现目标探测的时候，对于存在目标对象的网格中，定义训练标签Y的时候，边界框的指定参数的不同对其预测精度有很大的影响。这里给出一个较为合理的约定：（其他参数指定方式可阅读论文）</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-aa792626fb78010653bfe254f8bf6a47_hd.jpg" alt="img"></p>
<ul>
<li>对于每个网格，以左上角为(0,0)，以右下角为(1,1)；</li>
<li>中点 <img src="http://www.zhihu.com/equation?tex=b_%7Bx%7D%E3%80%81b_%7By%7D" alt="b_{x}、b_{y}"> 表示坐标值，在0~1之间；</li>
<li>宽高 <img src="http://www.zhihu.com/equation?tex=b_%7Bh%7D%E3%80%81b_%7Bw%7D" alt="b_{h}、b_{w}"> 表示比例值，存在&gt;1的情况。</li>
</ul>
<h2 id="4-交并比（Intersection-over-Union）"><a href="#4-交并比（Intersection-over-Union）" class="headerlink" title="4. 交并比（Intersection-over-Union）"></a><strong>4. 交并比（Intersection-over-Union）</strong></h2><p><strong>交并比</strong>函数用来评价目标检测算法是否运作良好。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-5156b1ec84138eb23b879652aba3a865_hd.jpg" alt="img"></p>
<p>对于理想的边界框和目标探测算法预测得到的边界框，交并比函数计算两个边界框交集和并集之比。</p>
<p><img src="http://www.zhihu.com/equation?tex=%5Crm+IoU+%3D+%5Cdfrac%7B%E4%BA%A4%E9%9B%86%E9%9D%A2%E7%A7%AF%7D%7B%E5%B9%B6%E9%9B%86%E9%9D%A2%E7%A7%AF%7D" alt="\rm IoU = \dfrac{交集面积}{并集面积}"></p>
<p>一般在目标检测任务中，约定如果 <img src="http://www.zhihu.com/equation?tex=IoU%5Cgeqslant0.5" alt="IoU\geqslant0.5"> ，那么就说明检测正确。当然标准越大，则对目标检测算法越严格。得到的IoU值越大越好。</p>
<h2 id="5-非最大值抑制（non-max-suppression，NMS）"><a href="#5-非最大值抑制（non-max-suppression，NMS）" class="headerlink" title="5. 非最大值抑制（non-max suppression，NMS）"></a><strong>5. 非最大值抑制（non-max suppression，NMS）</strong></h2><p>对于我们前面提到的目标检测算法，可能会对同一个对象做出多次的检测，<strong>非最大值抑制</strong>可以确保我们的算法对每个对象只检测一次。</p>
<p><strong>多网格检测同一物体：</strong></p>
<p>对于汽车目标检测的例子中，我们将图片分成很多精细的格子。最终预测输出的结果中，可能会有相邻的多个格子里均检测出都具有同一个对象。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d786896a701bc6b22359f8a678969638_hd.jpg" alt="img"></p>
<p><strong>NMS算法思想：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4b2756b3ce3c04b8f8cb99bd113e01eb_hd.jpg" alt="img"></p>
<ul>
<li>在对 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 个网格进行目标检测算法后，每个网格输出的 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 为一个0~1的值，表示有车的概率大小。其中会有多个网格内存在高概率；</li>
<li>得到对同一个对象的多次检测，也就是在一个对象上有多个具有重叠的不同的边界框；</li>
<li>非最大值抑制对多种检测结果进行清理：选取最大 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 的边界框，对所有其他与该边界框具有高交并比或高重叠的边界框进行抑制；</li>
<li>逐一审视剩下的边界框，寻找最高的 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 值边界框，重复上面的步骤。</li>
<li>非最大值抑制，也就是说抑制那些不是最大值，却比较接近最大值的边界框。</li>
</ul>
<p><strong>NMS算法：</strong></p>
<p>以单个对象检测为例：</p>
<ul>
<li>对于图片每个网格预测输出矩阵： <img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+P_%7Bc%7D%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D+%5Cend%7Barray%7D+%5Cright%5D" alt="y_{i} = \left[ \begin{array}{l} P_{c}\ b_{x}\ b_{y}\ b_{h}\ b_{w} \end{array} \right]"> ，其中 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 表示有对象的概率；</li>
<li>抛弃 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D%5Cleqslant0.6" alt="P_{c}\leqslant0.6"> 的边界框，也就是低概率的情况；</li>
<li>对剩余的边界框（while）：</li>
</ul>
<p>- 选取最大 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 值的边界框，作为预测输出边界框；</p>
<p>- 抛弃和选取的边界框 <img src="http://www.zhihu.com/equation?tex=IoU%5Cgeqslant0.5" alt="IoU\geqslant0.5"> 的剩余的边界框。</p>
<p>对于多对象检测，输出标签中就会有多个分量。正确的做法是：对每个输出类别分别独立进行一次非最大值抑制。</p>
<h2 id="6-Anchor-box"><a href="#6-Anchor-box" class="headerlink" title="6. Anchor box"></a><strong>6. Anchor box</strong></h2><p>通过上面的各种方法，目前我们的目标检测算法在每个格子上只能检测出一个对象。使用<strong>Anchor box</strong>可以同时检测出多个对象。</p>
<p><strong>重叠目标：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-12c977ce40c692b2238f4e0f98034d3c_hd.jpg" alt="img"></p>
<p>对于重叠的目标，这些目标的中点有可能会落在同一个网格中，对于我们之前定义的输出： <img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+P_%7Bc%7D%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D%5C+c_%7B1%7D%5C+c_%7B2%7D%5C+c_%7B3%7D+%5Cend%7Barray%7D+%5Cright%5D" alt="y_{i} = \left[ \begin{array}{l} P_{c}\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ c_{1}\ c_{2}\ c_{3} \end{array} \right]"> ，只能得到一个目标的输出。</p>
<p>而Anchor box 则是预先定义多个不同形状的Anchor box，我们需要把预测目标对应地和各个Anchor box 关联起来，所以我们重新定义目标向量：</p>
<p><img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cleft%5B+P_%7Bc%7D%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D%5C+c_%7B1%7D%5C+c_%7B2%7D%5C+c_%7B3%7D%5C+P_%7Bc%7D%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D%5C+c_%7B1%7D%5C+c_%7B2%7D%5C+c_%7B3%7D%5Ccdots%5Cright%5D" alt="y_{i} = \left[ P_{c}\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ c_{1}\ c_{2}\ c_{3}\ P_{c}\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ c_{1}\ c_{2}\ c_{3}\cdots\right]"></p>
<p>用这样的多目标向量分别对应不同的Anchor box，从而检测出多个重叠的目标。</p>
<ul>
<li>不使用Anchor box：训练图片中的每个对象，根据对象的中点，分配到对应的格子中。输出大小（例如8）： <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n%5Ctimes+8" alt="n\times n\times 8"> ；</li>
<li>使用Anchor box：训练图片的每个对象，根据对象的中点，分配到对应的格子中，同时还分配到一个和对象形状的IoU最高的Anchor box 中。输出大小（例如两个Anchor box）： <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n%5Ctimes+16" alt="n\times n\times 16"> 。</li>
</ul>
<p><strong>例子：</strong></p>
<p>如下面的图片，里面有行人和汽车，我们为其分配两个Anchor box。对于行人形状更像Anchor box 1，汽车形状更像Anchor box 2，所以我们将人和汽车分配到不同的输出位置。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d6fe6618e8ea2a5189bd8482c8fa25b1_hd.jpg" alt="img"></p>
<p>如果格子中只有汽车的时候，我们使用了两个Anchor box，那么此时我们的目标向量就成为：</p>
<p><img src="http://www.zhihu.com/equation?tex=y_%7Bi%7D+%3D+%5Cleft%5B+0%5C+%3F%5C+%3F%5C+%3F%5C+%3F%5C+%3F%5C+%3F%5C+%3F%5C+1%5C+b_%7Bx%7D%5C+b_%7By%7D%5C+b_%7Bh%7D%5C+b_%7Bw%7D%5C+0%5C+1%5C+0%5Cright%5D" alt="y_{i} = \left[ 0\ ?\ ?\ ?\ ?\ ?\ ?\ ?\ 1\ b_{x}\ b_{y}\ b_{h}\ b_{w}\ 0\ 1\ 0\right]"></p>
<p>其中，“？”代表的是该位置是什么样的参数我们都不关心。</p>
<p><strong>难点问题：</strong></p>
<ul>
<li>如果我们使用了两个Anchor box，但是同一个格子中却有三个对象的情况，此时只能用一些额外的手段来处理；</li>
<li>同一个格子中存在两个对象，但它们的Anchor box 形状相同，此时也需要引入一些专门处理该情况的手段。</li>
</ul>
<p>但是以上的两种问题出现的可能性不会很大，对目标检测算法不会带来很大的影响。</p>
<p><strong>Anchor box 的选择：</strong></p>
<ul>
<li>一般人工指定Anchor box 的形状，选择5~10个以覆盖到多种不同的形状，可以涵盖我们想要检测的对象的形状；</li>
<li>高级方法：K-means 算法：将不同对象形状进行聚类，用聚类后的结果来选择一组最具代表性的Anchor box，以此来代表我们想要检测对象的形状。</li>
</ul>
<h2 id="7-YOLO算法目标检测"><a href="#7-YOLO算法目标检测" class="headerlink" title="7. YOLO算法目标检测"></a><strong>7. YOLO算法目标检测</strong></h2><p>假设我们要在图片中检测三种目标：行人、汽车和摩托车，同时使用两种不同的Anchor box。</p>
<p><strong>训练集：</strong></p>
<ul>
<li>输入X：同样大小的完整图片；</li>
<li>目标Y：使用 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3" alt="3\times3"> 网格划分，输出大小 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes2%5Ctimes8" alt="3\times3\times2\times8"> ，或者 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes16" alt="3\times3\times16"></li>
<li>对不同格子中的小图，定义目标输出向量Y。</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-04baf38d85d19b9574a31b6a794c7d6a_hd.jpg" alt="img"></p>
<p><strong>模型预测：</strong></p>
<p>输入与训练集中相同大小的图片，同时得到每个格子中不同的输出结果： <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes2%5Ctimes8" alt="3\times3\times2\times8">。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-5fcb40ed001cb3629e0bec69676e3eb7_hd.jpg" alt="img"></p>
<p><strong>运行非最大值抑制（NMS）：</strong></p>
<ul>
<li>假设使用了2个Anchor box，那么对于每一个网格，我们都会得到预测输出的2个bounding boxes，其中一个 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 比较高；</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-22c34225d06de047752ee9752bb741a5_hd.jpg" alt="img"></p>
<ul>
<li>抛弃概率 <img src="http://www.zhihu.com/equation?tex=P_%7Bc%7D" alt="P_{c}"> 值低的预测bounding boxes；</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a0355fddb90cc272161244376621c5e2_hd.jpg" alt="img"></p>
<ul>
<li>对每个对象（如行人、汽车、摩托车）分别使用NMS算法得到最终的预测边界框。</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-638ccfeaf962131a2bccc15b8c4de816_hd.jpg" alt="img"></p>
<h2 id="8-候选区域（region-proposals）"><a href="#8-候选区域（region-proposals）" class="headerlink" title="8. 候选区域（region proposals）"></a><strong>8. 候选区域（region proposals）</strong></h2><p><strong>R-CNN：</strong></p>
<p>R-CNN（Regions with convolutional networks），会在我们的图片中选出一些目标的候选区域，从而避免了传统滑动窗口在大量无对象区域的无用运算。</p>
<p>所以在使用了R-CNN后，我们不会再针对每个滑动窗口运算检测算法，而是只选择一些候选区域的窗口，在少数的窗口上运行卷积网络。</p>
<p>具体实现：运用图像分割算法，将图片分割成许多不同颜色的色块，然后在这些色块上放置窗口，将窗口中的内容输入网络，从而减小需要处理的窗口数量。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-813a1cce626d7c967e3c843a1dd56103_hd.jpg" alt="img"></p>
<p><strong>更快的算法：</strong></p>
<ul>
<li>R-CNN：给出候选区域，<strong>不使用滑动窗口</strong>，对每个候选区域进行分类识别，输出对象 标签 和 bounding box，从而在确实存在对象的区域得到更精确的边界框，但速度慢；</li>
<li>Fast R-CNN：给出候选区域，使用<strong>滑动窗口的卷积实现</strong>去分类所有的候选区域，但得到候选区的聚类步骤仍然非常慢；</li>
<li>Faster R-CNN：使用<strong>卷积网络</strong>给出候选区域。</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/卷积神经网络3/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-卷积神经网络2" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/卷积神经网络2/">深度卷机网络：实例探究</a>
    </h1>
  

        
        <a href="/2018/02/05/卷积神经网络2/" class="archive-article-date">
  	<time datetime="2018-02-05T08:48:35.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-经典的卷积网络"><a href="#1-经典的卷积网络" class="headerlink" title="1. 经典的卷积网络"></a><strong>1. 经典的卷积网络</strong></h2><p>介绍几种经典的卷积神经网络结构，分别是LeNet、AlexNet、VGGNet。</p>
<p><strong>LeNet-5：</strong></p>
<p>LeNet-5主要是针对灰度设计的，所以其输入较小，为 <img src="http://www.zhihu.com/equation?tex=32%5Ctimes32%5Ctimes1" alt="32\times32\times1"> ，其结构如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-47ce686eadb7eca361cb8e30e5ce44a9_hd.jpg" alt="img"></p>
<p>在LetNet中，存在的经典模式：</p>
<ul>
<li>随着网络的深度增加，图像的大小在缩小，与此同时，通道的数量却在增加；</li>
<li>每个卷积层后面接一个池化层。</li>
</ul>
<p><strong>AlexNet：</strong></p>
<p>AlexNet直接对彩色的大图片进行处理，其结构如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-40cf21fdb32b5c45267dc8454b1c305c_hd.jpg" alt="img"></p>
<ul>
<li>与LeNet相似，但网络结构更大，参数更多，表现更加出色；</li>
<li>使用了Relu；</li>
<li>使用了多个GPUs；</li>
<li>LRN（后来发现用处不大，丢弃了）</li>
</ul>
<p>AlexNet使得深度学习在计算机视觉方面受到极大的重视。</p>
<p><strong>VGG-16：</strong></p>
<p>VGG卷积层和池化层均具有相同的卷积核大小，都使用 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3%EF%BC%8Cstride%3D1%2C+SAME" alt="3\times3，stride=1, SAME"> 的卷积和 <img src="http://www.zhihu.com/equation?tex=2%5Ctimes2%EF%BC%8C+stride%3D2" alt="2\times2， stride=2"> 的池化。其结构如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ace8d57233d927540b09dd62f3a02767_hd.jpg" alt="img"></p>
<h2 id="2-ResNet"><a href="#2-ResNet" class="headerlink" title="2. ResNet"></a><strong>2. ResNet</strong></h2><p>ResNet是由残差块所构建。</p>
<p><strong>残差块：</strong></p>
<p>下面是一个普通的神经网络块的传输：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-9af76e289bd4a48aa986560767108833_hd.jpg" alt="img"></p>
<p>其前向传播的计算步骤为：</p>
<ul>
<li>Linear： <img src="http://www.zhihu.com/equation?tex=z%5E%7B%5Bl%2B1%5D%7D+%3D+W%5E%7B%5Bl%2B1%5D%7Da%5E%7B%5Bl%5D%7D+%2B+b%5E%7B%5Bl%2B1%5D%7D" alt="z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}"></li>
<li>Relu： <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B1%5D%7D+%3D+g%28z%5E%7B%5Bl%2B1%5D%7D%29" alt="a^{[l+1]} = g(z^{[l+1]})"></li>
<li>Linear： <img src="http://www.zhihu.com/equation?tex=z%5E%7B%5Bl%2B2%5D%7D+%3D+W%5E%7B%5Bl%2B2%5D%7Da%5E%7B%5Bl%2B1%5D%7D+%2B+b%5E%7B%5Bl%2B2%5D%7D" alt="z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}"></li>
<li>Relu： <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D+%3D+g%28z%5E%7B%5Bl%2B2%5D%7D%29" alt="a^{[l+2]} = g(z^{[l+2]})"></li>
</ul>
<p>而ResNet块则将其传播过程增加了一个从 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="a^{[l]}"> 直接到 <img src="http://www.zhihu.com/equation?tex=z%5E%7B%5Bl%2B2%5D%7D" alt="z^{[l+2]}"> 的连接，将其称之为“<strong>short cut</strong>”或者“<strong>skip connection</strong>”：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a36e3ecbd3d375e85caa06a25ca6c03b_hd.jpg" alt="img"></p>
<p>也就是前向传播公式的最后一个步骤变为： <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D+%3D+g%28z%5E%7B%5Bl%2B2%5D%7D%2Ba%5E%7B%5Bl%5D%7D%29" alt="a^{[l+2]} = g(z^{[l+2]}+a^{[l]})"></p>
<p>增加“short cut”后，成为残差块的网络结构：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-f798208c0ea57bfc0991c280c71f5bf2_hd.jpg" alt="img"></p>
<p>注意这里是连接在Relu激活函数之前。</p>
<p><strong>Residual Network：</strong></p>
<p>多个<strong>残差块</strong>堆积起来构成ResNet网络结构，其结构如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-879e090bfb52fae7112ad9896f316ccd_hd.jpg" alt="img"></p>
<p>没有“short cut”的普通神经网络和ResNet的误差曲线：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-bcaf9f135b7a957a6498020c11f055b7_hd.jpg" alt="img"></p>
<ul>
<li>在没有残差的普通神经网络中，训练的误差实际上是随着网络层数的加深，先减小再增加；</li>
<li>在有残差的ResNet中，即使网络再深，训练误差都会随着网络层数的加深逐渐减小。</li>
</ul>
<p>ResNet对于中间的激活函数来说，有助于能够达到更深的网络，解决梯度消失和梯度爆炸的问题。</p>
<h2 id="3-ResNet表现好的原因"><a href="#3-ResNet表现好的原因" class="headerlink" title="3. ResNet表现好的原因"></a><strong>3. ResNet表现好的原因</strong></h2><p>假设有个比较大的神经网络，输入为 <img src="http://www.zhihu.com/equation?tex=x" alt="x"> ，输出为 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="a^{[l]}"> 。如果我们想增加网络的深度，这里再给网络增加一个残差块：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-92ee772505f210dfe8f6e880b5ff4ea0_hd.jpg" alt="img"></p>
<p>假设网络中均使用Relu激活函数，所以最后的输出 <img src="http://www.zhihu.com/equation?tex=a%5Cgeqslant+0" alt="a\geqslant 0"> 。这里我们给出 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D" alt="a^{[l+2]}"> 的值：</p>
<p><img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D+%3D+g%28z%5E%7B%5Bl%2B2%5D%7D%2Ba%5E%7B%5Bl%5D%7D%29%3Dg%28W%5E%7B%5Bl%2B2%5D%7Da%5E%7B%5Bl%2B1%5D%7D%2Bb%5E%7B%5Bl%2B2%5D%7D%2Ba%5E%7B%5Bl%5D%7D%29" alt="a^{[l+2]} = g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})"></p>
<p>如果使用L2正则化或者权重衰减，会压缩W和b的值。如果 <img src="http://www.zhihu.com/equation?tex=W%5E%7B%5Bl%2B2%5D%7D%3D0" alt="W^{[l+2]}=0"> 同时 <img src="http://www.zhihu.com/equation?tex=b%5E%7B%5Bl%2B2%5D%7D%3D0" alt="b^{[l+2]}=0">，那么上式就变成：</p>
<p><img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D+%3D+g%28z%5E%7B%5Bl%2B2%5D%7D%2Ba%5E%7B%5Bl%5D%7D%29%3Dg%28a%5E%7B%5Bl%5D%7D%29%3Drelu%28a%5E%7B%5Bl%5D%7D%29%3D+a%5E%7B%5Bl%5D%7D" alt="a^{[l+2]} = g(z^{[l+2]}+a^{[l]})=g(a^{[l]})=relu(a^{[l]})= a^{[l]}"></p>
<p>所以从上面的结果我们可以看出，对于残差块来学习上面这个恒等函数是很容易的。所以在增加了残差块后更深的网络的性能也并不逊色于没有增加残差块简单的网络。所以尽管增加了网络的深度，但是并不会影响网络的性能。同时如果增加的网络结构能够学习到一些有用的信息，那么就会提升网络的性能。</p>
<p>同时由于结构 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%2B2%5D%7D+%3Dg%28z%5E%7B%5Bl%2B2%5D%7D%2Ba%5E%7B%5Bl%5D%7D%29" alt="a^{[l+2]} =g(z^{[l+2]}+a^{[l]})"> ，ResNet在设计中使用了很多相同的卷积，以保持 <img src="http://www.zhihu.com/equation?tex=z%5E%7B%5Bl%2B2%5D%7D" alt="z^{[l+2]}"> 和 <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="a^{[l]}"> 的维度相同。</p>
<p>将普通深度神经网络变为ResNet：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-3d285acc6d2b01fc635f021303cd9c2c_hd.jpg" alt="img"></p>
<p>在两个相同的卷积层之间增加“<strong>skip connection</strong>”。</p>
<h2 id="4-1x1卷积"><a href="#4-1x1卷积" class="headerlink" title="4. 1x1卷积"></a><strong>4. 1x1卷积</strong></h2><p><strong>1x1卷积：</strong></p>
<p>在二维上的卷积相当于图片的每个元素和一个卷积核数字相乘。</p>
<p>但是在三维上，与 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1%5Ctimes+n_C" alt="1\times1\times n_C"> 卷积核进行卷积，相当于三维图像上的 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1%5Ctimes+n_C" alt="1\times1\times n_C"> 的切片，也就是 <img src="http://www.zhihu.com/equation?tex=n_C" alt="n_C"> 个点乘以卷积数值权重，通过Relu函数后，输出对应的结果。而不同的卷积核则相当于不同的隐层神经元结点与切片上的点进行一一连接。</p>
<p>所以根本上 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 卷积核相当于对一个切片上的 <img src="http://www.zhihu.com/equation?tex=n_C" alt="n_C"> 个单元都应用了一个全连接的神经网络。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d277d0d16f71b796a5c5f6ee5bc4b034_hd.jpg" alt="img"></p>
<p>最终三维的图形应用 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 的卷积核得到一个相同长宽但第三维度变为卷积核个数的图片。</p>
<p><strong>1x1卷积应用：</strong></p>
<ul>
<li>维度压缩：使用目标维度的 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 的卷积核个数。</li>
<li>增加非线性：保持与原维度相同的 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 的卷积核个数。</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-fe00f8771cc158b27ebf79cc3423e6c8_hd.jpg" alt="img"></p>
<h2 id="5-Inception-Network"><a href="#5-Inception-Network" class="headerlink" title="5. Inception Network"></a><strong>5. Inception Network</strong></h2><p>Inception Network 的作用就是使我们无需去考虑在构建深度卷积神经网络时，使用多大的卷积核以及是否添加池化层等问题。</p>
<p><strong>Inception主要结构：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-9baabd75b8b165539fabbf2d29ef97c8_hd.jpg" alt="img"></p>
<p>在上面的Inception结构中，应用了不同的卷积核，以及带padding的池化层。在保持输入图片大小不变的情况下，通过不同运算结果的叠加，增加了通道的数量。</p>
<p><strong>计算成本的问题：</strong></p>
<p>对于上面的 <img src="http://www.zhihu.com/equation?tex=5%5Ctimes5" alt="5\times5"> 大小卷积核的计算成本：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-75a53d1539434064de1843e757e841fd_hd.jpg" alt="img"></p>
<ul>
<li>1 filters： <img src="http://www.zhihu.com/equation?tex=5%5Ctimes5%5Ctimes192" alt="5\times5\times192"> ；</li>
<li>32 个 filters；</li>
<li>总的计算成本： <img src="http://www.zhihu.com/equation?tex=28%5Ctimes28%5Ctimes32%5Ctimes5%5Ctimes5%5Ctimes192%3D120M" alt="28\times28\times32\times5\times5\times192=120M"></li>
</ul>
<p>对于 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 大小卷积核用作过渡的计算成本，也将下面的中间的层叫做“<strong>bottleneck layer</strong>”：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-8adfd8721d6ea9da0f214c406007f293_hd.jpg" alt="img"></p>
<ul>
<li><img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 卷积层计算成本： <img src="http://www.zhihu.com/equation?tex=28%5Ctimes28%5Ctimes16%5Ctimes1%5Ctimes1%5Ctimes192%3D2.4M" alt="28\times28\times16\times1\times1\times192=2.4M"></li>
<li><img src="http://www.zhihu.com/equation?tex=5%5Ctimes5" alt="5\times5"> 卷积层计算成本： <img src="http://www.zhihu.com/equation?tex=28%5Ctimes28%5Ctimes32%5Ctimes5%5Ctimes5%5Ctimes16%3D10.0M" alt="28\times28\times32\times5\times5\times16=10.0M"></li>
<li>总的计算成本： <img src="http://www.zhihu.com/equation?tex=2.4M%2B10.0M%3D12.4M" alt="2.4M+10.0M=12.4M"></li>
</ul>
<p>所以 <img src="http://www.zhihu.com/equation?tex=1%5Ctimes1" alt="1\times1"> 卷积核作为“bottleneck layer”的过渡层能够有效减小卷积神经网的计算成本。事实证明，只要合理地设置“bottleneck layer”，既可以显著减小上层的规模，同时又能降低计算成本，从而不会影响网络的性能。</p>
<p><strong>Inception 模块：</strong></p>
<p>将上面说介绍的两种主要思想和模式结合到一起构成 Inception 模块，如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-1c0d40f4f581a53d8f9c0c10ebd3e588_hd.jpg" alt="img"></p>
<p><strong>Inception Network：</strong></p>
<p>多个Inception 模块的堆叠构成Inception Network，下面是GoogleNet的结构：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-5922b4756827c373937fe83c2f192607_hd.jpg" alt="img"></p>
<h2 id="6-迁移学习"><a href="#6-迁移学习" class="headerlink" title="6. 迁移学习"></a><strong>6. 迁移学习</strong></h2><p><strong>小数据集：</strong></p>
<p>如今在深度学习领域，许多研究者都会将他们的工作共享到网络上。在我们实施自己的工作的时候，比如说做某种物体的识别分类，但是只有少量的数据集，对于从头开始训练一个深度网络结构是远远不够的。</p>
<p>但是我们可以应用迁移学习，应用其他研究者建立的模型和参数，用少量的数据仅训练最后自定义的softmax网络。从而能够在小数据集上达到很好的效果。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a8f828d1cd4ac169b2d30feeb6327a40_hd.jpg" alt="img"></p>
<p><strong>大数据集：</strong></p>
<p>如果我们在自己的问题上也拥有大量的数据集，我们可以多训练后面的几层。总之随着数据集的增加，我们需要“ <strong>freeze</strong>”的层数越来越少。最后如果我们有十分庞大的数据集，那么我们可以训练网络模型的所有参数，将其他研究者训练的模型参数作为<strong>参数的初始化</strong>来替代随机初始化，来加速我们模型的训练。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-80a990183643a60667f96f2bd1a6eb78_hd.jpg" alt="img"></p>
<h2 id="7-数据扩充"><a href="#7-数据扩充" class="headerlink" title="7. 数据扩充"></a><strong>7. 数据扩充</strong></h2><p>与其他机器学习问题相比，在计算机视觉领域当下最主要的问题是没有办法得到充足的数据。所以在我们训练计算机数据模型的时候，数据的扩充就是会非常有用。</p>
<p><strong>数据扩充的方法：</strong></p>
<ul>
<li>镜像翻转（Mirroring）；</li>
<li>随机剪裁（Random Cropping）；</li>
<li>色彩转换（Color shifting）：</li>
</ul>
<p>为图片的RGB三个色彩通道进行增减值，如（R：+20，G：-20，B：+20）；PCA颜色增强：对图片的主色的变化较大，图片的次色变化较小，使总体的颜色保持一致。</p>
<p><strong>训练过程中的数据扩充：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a83cec7b4b533453b94041f27affb7e1_hd.jpg" alt="img"></p>
<p>为了节省时间，数据扩充的过程和训练过程可以多CPU多线程来并行的实现。</p>
<h2 id="8-计算机视觉现状"><a href="#8-计算机视觉现状" class="headerlink" title="8. 计算机视觉现状"></a><strong>8. 计算机视觉现状</strong></h2><p><strong>数据和手工工程：</strong></p>
<p>不同问题当前的数据集大小：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-66425ba48cf3db7af7b38effeca69aa2_hd.jpg" alt="img"></p>
<p>在有大量数据的时候，我们更倾向于使用简单的算法和更少的手工工程。因为此时有大量的数据，我们不需要为这个问题来精心设计特征，我们使用一个大的网络结果或者更简单的模型就能够解决。</p>
<p>相反，在有少量数据的时候，我们从事更多的是手工工程。因为数据量太少，较大的网络结构或者模型很难从这些少量的数据中获取足够的特征，而手工工程实际上是获得良好表现的最佳方式。</p>
<p>对于机器学习应用：</p>
<ul>
<li>标记数据，（x,y）；</li>
<li>手工特征工程/网络结构/其他构建。</li>
</ul>
<p><strong>Tips for doing well：</strong></p>
<p>在基准研究和比赛中，下面的tips可能会有较好的表现：</p>
<ul>
<li>Ensembling：独立地训练多个网络模型，输出平均结果或加权平均结果；</li>
<li>测试时的 Multi-crop：在测试图片的多种版本上运行分类器，输出平均结果。</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/卷积神经网络2/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-卷积神经网络1" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/卷积神经网络1/">计算机视觉</a>
    </h1>
  

        
        <a href="/2018/02/05/卷积神经网络1/" class="archive-article-date">
  	<time datetime="2018-02-05T08:48:26.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-计算机视觉"><a href="#1-计算机视觉" class="headerlink" title="1. 计算机视觉"></a><strong>1. 计算机视觉</strong></h2><p>计算机视觉（Computer Vision）包含很多不同类别的问题，如图片分类、目标检测、图片风格迁移等等。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-447bc5a6b5836f98051c9cf41ea81ca5_hd.jpg" alt="img"></p>
<p>对于小尺寸的图片问题，也许我们用深度神经网络的结构可以较为简单的解决一定的问题。但是当应用在大尺寸的图片上，输入规模将变得十分庞大，使用神经网络将会有非常多的参数需要去学习，这个时候神经网络就不再适用。</p>
<p>卷积神经网络在计算机视觉问题上是一个非常好的网络结构。</p>
<h2 id="2-边缘检测示例"><a href="#2-边缘检测示例" class="headerlink" title="2. 边缘检测示例"></a><strong>2. 边缘检测示例</strong></h2><p>卷积运算是卷积神经网络的基本组成部分。下面以边缘检测的例子来介绍卷积运算。</p>
<p>所谓边缘检测，在下面的图中，分别通过垂直边缘检测和水平边缘检测得到不同的结果：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2ee2dc2433c755b8997fa73d52b19b27_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p><strong>垂直边缘检测：</strong></p>
<p>假设对于一个 <img src="http://www.zhihu.com/equation?tex=6%5Ctimes6" alt="6\times6"> 大小的图片（以数字表示），以及一个 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3" alt="3\times3"> 大小的 <strong>filter</strong>（卷积核） 进行卷积运算，以“ <img src="http://www.zhihu.com/equation?tex=%2A" alt="*"> ” 符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-f6f3a2d714ffc11be19acbcf15945f17_hd.jpg" alt="img"></p>
<p><strong>filter </strong>不断地和其大小相同的部分做对应元素的乘法运算并求和，最终得到的数字相当于新图片的一个像素值，如右矩阵所示，最终得到一个 <img src="http://www.zhihu.com/equation?tex=4%5Ctimes4" alt="4\times4"> 大小的图片。</p>
<p><strong>边缘检测的原理：</strong></p>
<p>以一个有一条垂直边缘线的简单图片来说明。通过垂直边缘 <strong>filter</strong> 我们得到的最终结果图片可以明显地将边缘和非边缘区分出来：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ef33cdc3a41a9f684b7ff94145a78112_hd.jpg" alt="img"></p>
<p>卷积运算提供了一个方便的方法来检测图像中的边缘，成为卷积神经网络中重要的一部分。</p>
<p><strong>多种边缘检测：</strong></p>
<p>垂直和水平边缘检测</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-13d7c463c6122598c80c48e804fdf9fb_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>更复杂的 <strong>filter</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-85fc99b475104f2906a2d4a4694ece0f_hd.jpg" alt="img"></p>
<p>对于复杂的图片，我们可以直接将 <strong>filter</strong> 中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面filter更好的更复杂的 <strong>filter</strong> ，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。</p>
<p>通过卷积运算，在卷积神经网络中通过反向传播算法，可以学习到相应于目标结果的 <strong>filter</strong>，将其应用于整个图片，输出其提取到的所有有用的特征。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d8cef747fcb56e5dea88a143ab79a83c_hd.jpg" alt="img"></p>
<p><strong>卷积和互相关：</strong></p>
<p>在数学定义上，矩阵的<strong>卷积</strong>（convolution）操作为首先将卷积核同时在水平和垂直方向上进行翻转，构成一个卷积核的镜像，然后使用该镜像再和前面的矩阵进行移动相乘求和操作。如下面例子所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-0c53577a6080af59aebe67ac1a73ec80_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为<strong>互相关</strong>（cross-correlation）。</p>
<h2 id="3-Padding"><a href="#3-Padding" class="headerlink" title="3. Padding"></a><strong>3. Padding</strong></h2><p><strong>没有Padding的缺点：</strong></p>
<ul>
<li>每次卷积操作，图片会缩小；</li>
</ul>
<p>就前面的例子来说， <img src="http://www.zhihu.com/equation?tex=6%5Ctimes6" alt="6\times6"> 大小的图片，经过 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3" alt="3\times3"> 大小的 filter，缩小成了 <img src="http://www.zhihu.com/equation?tex=4%5Ctimes4" alt="4\times4"> 大小</p>
<p>图片： <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n--%3E+%28n-f%2B1%29%5Ctimes+%28n-f%2B1%29" alt="n\times n--&gt; (n-f+1)\times (n-f+1)"></p>
<ul>
<li>角落和边缘位置的像素进行卷积运算的次数少，可能会丢失有用信息。</li>
</ul>
<p>其中，<strong>n </strong>表示图片的长或宽的大小，<strong>f </strong>表示filter的长或宽的大小。</p>
<p><strong>加Padding：</strong></p>
<p>为了解决上面的两个缺点，我们在进行卷积运算前为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落和边沿的信息。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-676e173811a76674d447be028a4dbf43_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>以<strong>p</strong>表示 Padding 的值，则输入 <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> 大小的图片，最终得到的图片大小为 <img src="http://www.zhihu.com/equation?tex=+%28n%2B2p-f%2B1%29%5Ctimes+%28n%2B2p-f%2B1%29" alt=" (n+2p-f+1)\times (n+2p-f+1)"> ，为使图片大小保持不变，需根据filter的大小调整p的值。</p>
<p><strong>Valid / Same 卷积：</strong></p>
<ul>
<li><strong>Valid</strong>：no padding；（ <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n+--%3E+%28n-f%2B1%29%5Ctimes+%28n-f%2B1%29" alt="n\times n --&gt; (n-f+1)\times (n-f+1)"> ）</li>
<li><strong>Same</strong>：padding，输出与输入图片大小相同，（ <img src="http://www.zhihu.com/equation?tex=p%3D%28f-1%29%2F2" alt="p=(f-1)/2"> ）。在计算机视觉中，一般来说padding的值为奇数（因为filter一般为奇数）</li>
</ul>
<h2 id="4-卷积步长（stride）"><a href="#4-卷积步长（stride）" class="headerlink" title="4. 卷积步长（stride）"></a><strong>4. 卷积步长（stride）</strong></h2><p>卷积的步长是构建卷积神经网络的一个基本的操作。</p>
<p>如前面的例子中，我们使用的 stride=1，每次的卷积运算以1个步长进行移动。下面是 stride=2 时对图片进行卷积的结果：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-6ae842e349ed01f48b5343ce9b5b386a_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>以<strong>s</strong>表示stride的大小，那么在进行卷积运算后，图片的变化为：</p>
<p><img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n+--%3E+%5Cleft%5Clfloor+%5Cdfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1+%5Cright%5Crfloor%5Ctimes+%5Cleft%5Clfloor+%5Cdfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1+%5Cright%5Crfloor" alt="n\times n --&gt; \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor\times \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor"></p>
<p>注意，在当 <img src="http://www.zhihu.com/equation?tex=padding%5Cne+1" alt="padding\ne 1"> 时，若移动的窗口落在图片外面，则不要再进行相乘的操作，丢弃边缘的数值信息，所以输出图片的最终维度为<strong>向下取整</strong>。</p>
<h2 id="5-立体卷积"><a href="#5-立体卷积" class="headerlink" title="5. 立体卷积"></a><strong>5. 立体卷积</strong></h2><p><strong>卷积核的通道数：</strong></p>
<p>对于灰色图像中，卷积核和图像均是二维的。而应用于彩色图像中，因为图片有R、G、B三个颜色通道，所以此时的卷积核应为三维卷积核。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-94b43f2cf7f532acc039eedc2e36b811_hd.jpg" alt="img"></p>
<p>卷积核的第三个维度需要与进行卷积运算的图片的通道数相同。</p>
<p><strong>多卷积核：</strong></p>
<p>单个卷积核应用于图片时，提取图片特定的特征，不同的卷积核提取不同的特征。如两个大小均为 <img src="http://www.zhihu.com/equation?tex=+3%5Ctimes3%5Ctimes3" alt=" 3\times3\times3"> 的卷积核分别提取图片的垂直边缘和水平边缘。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-14d2d62dd293798e516be6387bbbbadc_hd.jpg" alt="img"></p>
<p>由图可知，最终提取到彩色图片的垂直特征图和水平特征图，得到有2个通道的 <img src="http://www.zhihu.com/equation?tex=4%5Ctimes4" alt="4\times4"> 大小的特征图片。</p>
<p><strong>Summary：</strong></p>
<p>图片： <img src="http://www.zhihu.com/equation?tex=%28n%5Ctimes+n%5Ctimes+n_%7Bc%7D+%29%2A+%28f%5Ctimes+f%5Ctimes+n_%7Bc%7D%29+%E2%80%94%E2%80%94%3E%28n-f%2B1%29%5Ctimes+%28n-f%2B1%29%5Ctimes+n%27_%7Bc%7D" alt="(n\times n\times n_{c} )* (f\times f\times n_{c}) ——&gt;(n-f+1)\times (n-f+1)\times n&#39;_{c}"></p>
<p>其中， <img src="http://www.zhihu.com/equation?tex=n_%7Bc%7D" alt="n_{c}"> 表示通道的数量， <img src="http://www.zhihu.com/equation?tex=n%27_%7Bc%7D" alt="n&#39;_{c}"> 表示下一层的通道数，同时也等于本层卷积核的个数。</p>
<h2 id="6-简单卷积网络"><a href="#6-简单卷积网络" class="headerlink" title="6. 简单卷积网络"></a><strong>6. 简单卷积网络</strong></h2><p><strong>单层卷积网络的例子：</strong></p>
<p>和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出：</p>
<p><img src="http://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D%3Dw%5E%7B%5B1%5D%7Da%5E%7B%5B0%5D%7D%2Bb%5E%7B%5B1%5D%7D" alt="z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}"></p>
<p><img src="http://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D%3Dg%28z%5E%7B%5B1%5D%7D%29" alt="a^{[1]}=g(z^{[1]})"></p>
<p>不同点是：在卷积神经网络中，权重和输入进行的是卷积运算。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-590163150a6d7e02edaec6632f345ff4_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p><strong>单层卷积的参数个数：</strong></p>
<p>在一个卷积层中，如果我们有10个 <img src="http://www.zhihu.com/equation?tex=3%5Ctimes3%5Ctimes3" alt="3\times3\times3"> 大小的卷积核，那么加上每个卷积核对应的偏置，则对于一个卷积层，我们共有的参数个数为：</p>
<p><img src="http://www.zhihu.com/equation?tex=%283%5Ctimes3%5Ctimes3%2B1%29%5Ctimes10+%3D+280" alt="(3\times3\times3+1)\times10 = 280"></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-0b5f7d68c804d74c795d1664fa4c5107_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>无论图片大小是多少，该例子中的卷积层参数个数一直都是280个，相对于普通的神经网络，卷积神经网络的参数个数要少很多。</p>
<p><strong>标记的总结：</strong></p>
<p>如果 <img src="http://www.zhihu.com/equation?tex=l" alt="l"> 表示一个卷积层：</p>
<ul>
<li><img src="http://www.zhihu.com/equation?tex=f%5E%7B%5Bl%5D%7D" alt="f^{[l]}"> ：filter 的大小；</li>
<li><img src="http://www.zhihu.com/equation?tex=p%5E%7B%5Bl%5D%7D" alt="p^{[l]}"> ：padding；</li>
<li><img src="http://www.zhihu.com/equation?tex=s%5E%7B%5Bl%5D%7D" alt="s^{[l]}"> ：步长（stride）；</li>
<li>卷积核的个数： <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D_%7BC%7D" alt="n^{[l]}_{C}"> ；</li>
<li>filter大小： <img src="http://www.zhihu.com/equation?tex=f%5E%7B%5Bl%5D%7D%5Ctimes+f%5E%7B%5Bl%5D%7D%5Ctimes+n%5E%7B%5Bl-1%5D%7D_%7BC%7D" alt="f^{[l]}\times f^{[l]}\times n^{[l-1]}_{C}"> ;</li>
<li>激活值（Activations）： <img src="http://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%E2%80%94%3En%5E%7B%5Bl%5D%7D_%7BH%7D%5Ctimes+n%5E%7B%5Bl%5D%7D_%7BW%7D%5Ctimes+n%5E%7B%5Bl%5D%7D_%7BC%7D" alt="a^{[l]}—&gt;n^{[l]}_{H}\times n^{[l]}_{W}\times n^{[l]}_{C}">；</li>
<li>权重（Weights）： <img src="http://www.zhihu.com/equation?tex=f%5E%7B%5Bl%5D%7D%5Ctimes+f%5E%7B%5Bl%5D%7D%5Ctimes+n%5E%7B%5Bl-1%5D%7D_%7BC%7D%5Ctimes+n%5E%7B%5Bl%5D%7D_%7BC%7D" alt="f^{[l]}\times f^{[l]}\times n^{[l-1]}_{C}\times n^{[l]}_{C}"> ；</li>
<li>偏置（bias）： <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D_%7BC%7D%E2%80%94%E2%80%94%281%2C1%2C1%2Cn%5E%7B%5Bl%5D%7D_%7BC%7D%29" alt="n^{[l]}_{C}——(1,1,1,n^{[l]}_{C})"></li>
</ul>
<ul>
<li>Input： <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D_%7BH%7D%5Ctimes+n%5E%7B%5Bl-1%5D%7D_%7BW%7D%5Ctimes+n%5E%7B%5Bl-1%5D%7D_%7BC%7D" alt="n^{[l-1]}_{H}\times n^{[l-1]}_{W}\times n^{[l-1]}_{C}"> ；</li>
<li>Output： <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D_%7BH%7D%5Ctimes+n%5E%7B%5Bl%5D%7D_%7BW%7D%5Ctimes+n%5E%7B%5Bl%5D%7D_%7BC%7D" alt="n^{[l]}_{H}\times n^{[l]}_{W}\times n^{[l]}_{C}"> ；</li>
<li>其中， <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D_%7BH%7D+%3D+%5Cleft%5Clfloor+%5Cdfrac%7Bn%5E%7B%5Bl-1%5D%7D_%7BH%7D%2B2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2B1+%5Cright%5Crfloor" alt="n^{[l]}_{H} = \left\lfloor \dfrac{n^{[l-1]}_{H}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \right\rfloor"> ， <img src="http://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D_%7BW%7D+%3D+%5Cleft%5Clfloor+%5Cdfrac%7Bn%5E%7B%5Bl-1%5D%7D_%7BW%7D%2B2p%5E%7B%5Bl%5D%7D-f%5E%7B%5Bl%5D%7D%7D%7Bs%5E%7B%5Bl%5D%7D%7D%2B1+%5Cright%5Crfloor" alt="n^{[l]}_{W} = \left\lfloor \dfrac{n^{[l-1]}_{W}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \right\rfloor"> 。</li>
</ul>
<p><strong>简单卷积网络示例：</strong></p>
<p>多层卷积构成卷积神经网络，下面是一个卷积神经网络的例子：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-6532a7ea215b4eee12a7170d61c8462d_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p><strong>卷积网络层的类型：</strong></p>
<ul>
<li>卷积层（Convolution），Conv；</li>
<li>池化层（Pooling），Pool；</li>
<li>全连接层（Fully connected）：Fc；</li>
</ul>
<h2 id="7-池化层"><a href="#7-池化层" class="headerlink" title="7. 池化层"></a><strong>7. 池化层</strong></h2><p><strong>最大池化（Max pooling）：</strong></p>
<p>最大池化是对前一层得到的特征图进行池化减小，仅由当前小区域内的最大值来代表最终池化后的值。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-516cf3ce448be6a529256afd344842b6_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p>在最大池化中，有一组超参数需要进行调整，其中， <img src="http://www.zhihu.com/equation?tex=f" alt="f"> 表示池化的大小， <img src="http://www.zhihu.com/equation?tex=s" alt="s"> 表示步长。</p>
<ul>
<li>池化前： <img src="http://www.zhihu.com/equation?tex=n%5Ctimes+n" alt="n\times n"> ；</li>
<li>池化后： <img src="http://www.zhihu.com/equation?tex=%5Cleft%5Clfloor+%5Cdfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1+%5Cright%5Crfloor%5Ctimes+%5Cleft%5Clfloor+%5Cdfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1+%5Cright%5Crfloor" alt="\left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor\times \left\lfloor \dfrac{n+2p-f}{s}+1 \right\rfloor"> 。</li>
</ul>
<p><strong>平均池化（Average pooling）：</strong></p>
<p>平均池化与最大池化唯一不同的是其选取的是小区域内的均值来代表该区域内的值。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-15c3b97ce2a9cc354b2affdf0ed0d391_hd%20-%20%E5%89%AF%E6%9C%AC.jpg" alt="img"></p>
<p><strong>池化 Summary：</strong></p>
<p>池化层的超参数：</p>
<ul>
<li><img src="http://www.zhihu.com/equation?tex=f" alt="f"> ：filter的大小；</li>
<li><img src="http://www.zhihu.com/equation?tex=s" alt="s"> ：stride大小；</li>
<li>最大池化或者平均池化；</li>
<li><img src="http://www.zhihu.com/equation?tex=p" alt="p"> ：padding，这里要注意，几乎很少使用。</li>
</ul>
<p>注意，池化层没有需要学习的参数。</p>
<h2 id="8-卷积神经网络示例"><a href="#8-卷积神经网络示例" class="headerlink" title="8. 卷积神经网络示例"></a><strong>8. 卷积神经网络示例</strong></h2><p>这里以 <strong>LeNet-5</strong> 为例，给出一个完整的卷积神经网络。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-875949201832bc5062806de09a3bb258_hd.jpg" alt="img"></p>
<p><strong>构建深度卷积的模式：</strong></p>
<ul>
<li>随着网络的深入，提取的特征图片大小将会逐渐减小，但同时通道数量应随之增加；</li>
<li>Conv——Pool——Conv——Pool——Fc——Fc——Fc——softmax。</li>
</ul>
<p><strong>卷积神经网络的参数：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-fb2fcdb920a95c8317cb7cefe49f89bb_hd.jpg" alt="img"></p>
<p>根据上表我们可以看出，对于卷积卷积神经网络的参数：</p>
<ul>
<li>在卷积层，仅有少量的参数；</li>
<li>在池化层，没有参数；</li>
<li>在全连接层，存在大量的参数。</li>
</ul>
<h2 id="9-使用卷积神经网络"><a href="#9-使用卷积神经网络" class="headerlink" title="9. 使用卷积神经网络"></a><strong>9. 使用卷积神经网络</strong></h2><p><strong>参数少的优势：</strong></p>
<p>与普通的全连接神经网络相比，卷积神经网络的参数更少。如图中的例子，卷积神经网络仅有 <img src="http://www.zhihu.com/equation?tex=6%5Ctimes%285%5Ctimes5%2B1%29%3D156" alt="6\times(5\times5+1)=156"> 个参数，而普通的全连接网络有 <img src="http://www.zhihu.com/equation?tex=3072%5Ctimes4704%5Capprox+14M" alt="3072\times4704\approx 14M"> 个参数。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-bfda021d68a44eb5215c4c3154ac3595_hd.jpg" alt="img"></p>
<ul>
<li><strong>参数共享</strong>：一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。</li>
<li><strong>连接的稀疏性</strong>：在每一层中，每个输出值只取决于少量的输入。</li>
</ul>
<p><strong>训练卷积神经网络：</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-be6cb6ec955958f676c150d3ceedae64_hd.jpg" alt="img"></p>
<p>我们将训练集输入到卷积神经网络中，对网络进行训练。利用梯度下降（Adam、momentum等优化算法）最小化代价函数来寻找网络的最优参数。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/卷积神经网络1/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-结构化机器学习项目2" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/结构化机器学习项目2/">机器学习策略2</a>
    </h1>
  

        
        <a href="/2018/02/05/结构化机器学习项目2/" class="archive-article-date">
  	<time datetime="2018-02-05T08:46:58.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-误差分析"><a href="#1-误差分析" class="headerlink" title="1. 误差分析"></a><strong>1. 误差分析</strong></h2><p>当我们在训练一个模型的时候，如一个猫和狗分类模型，最终得到了 <img src="http://www.zhihu.com/equation?tex=90%5C%25" alt="90\%"> 的精确度，即有 <img src="http://www.zhihu.com/equation?tex=10%5C%25" alt="10\%"> 的错误率。所以我们需要对模型的一些部分做相应调整，才能更好地提升分类的精度。</p>
<p>如果不加分析去做，可能几个月的努力对于提升精度并没有作用。所以一个好的误差分析的流程就相当重要。</p>
<p><strong>收集错误样例：</strong></p>
<p>在开发集（测试集）中，获取大约100个错误标记的例子，并统计其中有多少个是狗。</p>
<ul>
<li>假设一种情况是100个数据中，有5个样例是狗，那么如果我们对数据集的错误标记做努力去改进模型的精度，那么可以提升的上限就是 <img src="http://www.zhihu.com/equation?tex=5%5C%25" alt="5\%"> ，即仅仅可以达到 <img src="http://www.zhihu.com/equation?tex=9.5%5C%25" alt="9.5\%"> 的错误率，这有时称为<strong>性能上限</strong>。那么这种情况下，可能这样耗时的努力方向就不是很值得的一件事情。</li>
<li>另外一种假设是100个数据中，有50多个样例是狗，那么这种情况下，我们去改进数据集的错误标记，就是一个比较值得的改进方向，可以将模型的精确度提升至 <img src="http://www.zhihu.com/equation?tex=95%5C%25" alt="95\%"> 。</li>
</ul>
<p><strong>并行分析：</strong></p>
<ul>
<li>修改那些被分类成猫的狗狗图片标签；</li>
<li>修改那些被错误分类的大型猫科动物，如：狮子，豹子等；</li>
<li>提升模糊图片的质量。</li>
</ul>
<p>为了并行的分析，建立表格来进行。以单个错误分类样本为对象，分析每个样本错误分类的原因。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-86d2d7c8fec128c33255cdd659f8f8fe_hd.jpg" alt="img"></p>
<p>最后，统计错误类型的百分比，这个分析步骤可以给我们一个粗略的估计，让我们大致确定是否值得去处理每个不同的错误类型。</p>
<h2 id="2-清除错误标记的样本"><a href="#2-清除错误标记的样本" class="headerlink" title="2. 清除错误标记的样本"></a><strong>2. 清除错误标记的样本</strong></h2><p>下面还是以猫和狗分类问题为例子，来进行分析。如下面的分类中的几个样本：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-7cd438bc8cc081d478982651eece606c_hd.jpg" alt="img"></p>
<p><strong>情况一：</strong></p>
<p>深度学习算法对训练集中的随机误差具有相当的鲁棒性。</p>
<p>只要我们标记出错的例子符合随机误差，如：做标记的人不小心错误，或按错分类键。那么像这种随机误差导致的标记错误，一般来说不管这些误差可能也没有问题。</p>
<p>所以对于这类误差，我们可以不去用大量的时间和精力去做修正，只要数据集足够大，实际误差不会因为这些随机误差有很大的变化。</p>
<p><strong>情况二：</strong></p>
<p>虽然深度学习算法对随机误差具有很好的鲁棒性，但是对于系统误差就不是这样了。</p>
<p>如果做标记的人一直把如例子中的白色的狗标记成猫，那么最终导致我们的分类器就会出现错误。</p>
<p><strong>dev、test集中错误标记的情况：</strong></p>
<p>如果在开发集和测试集中出现了错误标记的问题，我们可以在误差分析的过程中，增加错误标记这一原因，再对错误的数据进行分析，得出修正这些标记错误的价值。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4eee1b87e2d407c8cb71d716fe647e0b_hd.jpg" alt="img"></p>
<p><strong>修正开发、测试集上错误样例：</strong></p>
<ul>
<li>对开发集和测试集上的数据进行检查，确保他们来自于相同的分布。使得我们以开发集为目标方向，更正确地将算法应用到测试集上。</li>
<li>考虑算法分类错误的样本的同时也去考虑算法分类正确的样本。（通常难度比较大，很少这么做）</li>
<li>训练集和开发/测试集来自不同的分布。</li>
</ul>
<h2 id="3-搭建系统"><a href="#3-搭建系统" class="headerlink" title="3. 搭建系统"></a><strong>3. 搭建系统</strong></h2><ul>
<li>设置开发、测试集和优化指标（确定方向）；</li>
<li>快速地建立基本的系统；</li>
<li>使用偏差方差分析、误差分析去确定后面步骤的优先步骤。</li>
</ul>
<p>总的来说，如果我们想建立自己的深度学习系统，我们就需要做到：快速的建立自己的基本系统，并进行迭代。而不是想的太多，在一开始就建立一个非常复杂，难以入手的系统。</p>
<h2 id="4-不同分布上的训练和测试"><a href="#4-不同分布上的训练和测试" class="headerlink" title="4. 不同分布上的训练和测试"></a><strong>4. 不同分布上的训练和测试</strong></h2><p>在深度学习的时代，因为需求的数据量非常大，现在很多的团队，使用的训练数据都是和开发集和测试集来自不同的分布。</p>
<p>下面是一些处理训练集和测试集存在差异的最佳的做法。以前一周中的猫的分类问题为例：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d2a6084c955b897161eb34ebd4ad1420_hd.jpg" alt="img"></p>
<p>我们可以从网上获取大量的高清晰的猫的图片去做分类，如200000张，但是只能获取少量利用手机拍摄的不清晰的图片，如10000张。但是我们系统的目的是应用到手机上做分类。</p>
<p>也就是说，我们的训练集和开发集、测试集来自于不同的分布。</p>
<p><strong>方法一：</strong></p>
<p>将两组数据合并到一起，总共得到21万张图片样本。将这些样本随机分配到训练、开发、测试集中。</p>
<ul>
<li>好处：三个集合中的数据均来自于同一分布；</li>
<li>坏处：我们设立开发集的目的是瞄准目标，而现在我们的目标绝大部分是为了去优化网上获取的高清晰度的照片，而不是我们真正的目标。</li>
</ul>
<p>该方法不是一个好的方法，不推荐。</p>
<p><strong>方法二：</strong></p>
<p>训练集均是来自网上下载的20万张高清图片，当然也可以加上5000张手机非高清图片；对于开发和测试集都是手机非高清图片。</p>
<ul>
<li>好处：开发集全部来自手机图片，瞄准目标；</li>
<li>坏处：训练集和开发、测试集来自不同的分布。</li>
</ul>
<p>从长期来看，这样的分布能够给我们带来更好的系统性能。</p>
<h2 id="5-不同分布上的偏差和方差"><a href="#5-不同分布上的偏差和方差" class="headerlink" title="5. 不同分布上的偏差和方差"></a><strong>5. 不同分布上的偏差和方差</strong></h2><p>通过估计学习算法的偏差和方差，可以帮助我们确定接下来应该优先努力的方向。但是当我们的训练集和开发、测试集来自不同的分布时，分析偏差和方差的方式就有一定的不同。</p>
<p><strong>方差和分布原由分析</strong></p>
<p>以猫分类为例，假设以人的分类误差 <img src="http://www.zhihu.com/equation?tex=0%5C%25" alt="0\%"> 作为贝叶斯误差。若我们模型的误差为：</p>
<ul>
<li>Training error： <img src="http://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"></li>
<li>Dev error： <img src="http://www.zhihu.com/equation?tex=10%5C%25" alt="10\%"></li>
</ul>
<p>如果我们的训练集和开发、测试集来自相同的分布，那么我们可以说模型存在很大的方差问题。但如果数据来自不同的分布，那么我们就不能下这样的定论了。</p>
<p>那么我们如何去确定是由于分布不匹配的问题导致开发集的误差，还是由于算法中存在的方差问题所致？</p>
<p><strong>设立“训练开发集”</strong></p>
<p>训练开发集，其中的数据和训练数据来自同一分布，但是却不用于训练过程。</p>
<p>如果最终，我们的模型得到的误差分别为：</p>
<ul>
<li>Training error： <img src="http://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"></li>
<li>Training-dev error： <img src="http://www.zhihu.com/equation?tex=9%5C%25" alt="9\%"></li>
<li>Dev error： <img src="http://www.zhihu.com/equation?tex=10%5C%25" alt="10\%"></li>
</ul>
<p>那么，由于<strong>训练开发集</strong>尽管和训练集来自同一分布，但是却有很大的误差， 模型无法泛化到同分布的数据，那么说明我们的模型存在<strong>方差问题</strong>。</p>
<p>但如果我们的模型得到的误差分别为：</p>
<ul>
<li>Training error： <img src="http://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"></li>
<li>Training-dev error： <img src="http://www.zhihu.com/equation?tex=1.5%5C%25" alt="1.5\%"></li>
<li>Dev error： <img src="http://www.zhihu.com/equation?tex=10%5C%25" alt="10\%"></li>
</ul>
<p>那么在这样的情况下，我们可以看到，来自同分布的数据，模型的泛化能力强，而开发集的误差主要是来自于<strong>分布不匹配</strong>导致的。</p>
<p><strong>分布不同的偏差方差分析</strong></p>
<p>通过：Human level、Training set error、Training-dev set error、Dev error、Test error 之间误差的大小，可以分别得知我们的模型，需要依次在：可避免的偏差、方差、数据分布不匹配、开发集的或拟合程度，这些方面做改进。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-cb81af7e2b2228130c7916b4acf2583f_hd.jpg" alt="img">误差分析</p>
<p>通常情况下来说，通过不同的集合上的误差分析，我们得出的结果会是中间一列误差由小变大，即误差上升的情况。但是也有一定的可能会出现右边一列误差在开发测试集上又表现的好的情况。</p>
<p>下面通过一个后视镜语音检测的例子来说明。我们以该例子建立更加一般的表格。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e73aa735dc3de823156b48d609f46364_hd.jpg" alt="img"></p>
<p>其中，横向分别是：普通语音识别数据、后视镜语音识别数据；纵向分别是：Human level、训练数据误差、未训练数据误差。表格中不同的位置分别代表不同的数据集。</p>
<p>通常情况下，我们分析误差会是一个递增的情况，但是对于我们的模型，在后视镜语音识别的数据数据上，可能已经可以达到人类水平误差的 <img src="http://www.zhihu.com/equation?tex=6%5C%25" alt="6\%"> 了，而最终的开发测试集也会 <img src="http://www.zhihu.com/equation?tex=6%5C%25" alt="6\%"> 的误差，要比训练误差和训练开发误差都要小。所以如果遇到这种情况，就要利用上表进行分析。</p>
<h2 id="6-解决数据分布不匹配问题"><a href="#6-解决数据分布不匹配问题" class="headerlink" title="6. 解决数据分布不匹配问题"></a><strong>6. 解决数据分布不匹配问题</strong></h2><p>如果通过上一节的误差分析，我们可以得知，模型最终在开发和测试集上的误差最终是由于数据分布不匹配而导致。那么这样的情况下如何解决？</p>
<ul>
<li>进行人工误差分析，尝试去了解训练集和开发测试集的具体差异在哪里。如：噪音等；</li>
<li>尝试把训练数据变得更像开发集，或者收集更多的类似开发集和测试集的数据，如增加噪音；</li>
</ul>
<h2 id="7-迁移学习"><a href="#7-迁移学习" class="headerlink" title="7. 迁移学习"></a><strong>7. 迁移学习</strong></h2><p>将从一个任务中学到的知识，应用到另一个独立的任务中。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e479451be79e560d14fdf50426de2aad_hd.jpg" alt="img"></p>
<p><strong>迁移学习的意义：</strong></p>
<p>迁移学习适合以下场合：迁移来源问题有很多数据，但是迁移目标问题却没有那么多的数据。</p>
<p>假设图像识别任务中有1百万个样本，里面的数据相当多；但对与一些特定的图像识别问题，如放射科图像，也许只有一百个样本，所以对于放射学诊断问题的数据很少。所以从图像识别训练中学到的很多知识可以迁移，来帮助我们提升放射科识别任务的性能。</p>
<p>同样一个例子是语音识别，可能在普通的语音识别中，我们有庞大的数据量来训练模型，所以模型从中学到了很多人类声音的特征。但是对于触发字检测任务，可能我们拥有的数据量很少，所以对于这种情况下，学习人类声音特征等知识就显得相当重要。所以迁移学习可以帮助我们建立一个很好的唤醒字检测系统。</p>
<p><strong>迁移学习有意义的情况：</strong></p>
<ul>
<li>任务A和任务B有着相同的输入；</li>
<li>任务A所拥有的数据要远远大于任务B（对于更有价值的任务B，任务A所拥有的数据要比B大很多）；</li>
<li>任务A的低层特征学习对任务B有一定的帮助。</li>
</ul>
<h2 id="8-多任务学习"><a href="#8-多任务学习" class="headerlink" title="8. 多任务学习"></a><strong>8. 多任务学习</strong></h2><p>与迁移学习的串行学习方式不同，在多任务学习中，多个任务是并行进行学习的，同时希望各个任务对其他的任务均有一定的帮助。</p>
<p><strong>自动驾驶的例子：</strong></p>
<p>假设在自动驾驶的例子中，我们需要检测的物体很多，如行人、汽车、交通灯等等。</p>
<p>对于现在的任务，我们的目标值变成了一个向量的形式向量中的每一个值代表检测到是否有如行人、汽车、交通灯等，一张图片有多个标签。</p>
<p><img src="http://www.zhihu.com/equation?tex=%5Chat+y%5E%7B%28i%29%7D%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+1%5C%5C+0%5C%5C+1%5C%5C+0+%5Cend%7Barray%7D+%5Cright%5D+%5Cleft%7C+%5Cbegin%7Barray%7D%7Bl%7D+Pedestrians%5C%5C+Cars%5C%5C+Road%5C+signs-Stop%5C%5C+Traffic%5C+lights+%5Cend%7Barray%7D+%5Cright." alt="\hat y^{(i)}=\left[ \begin{array}{l} 1\\ 0\\ 1\\ 0 \end{array} \right] \left| \begin{array}{l} Pedestrians\\ Cars\\ Road\ signs-Stop\\ Traffic\ lights \end{array} \right."></p>
<p>模型的神经网络结构如下图所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-69284cfc23139eba9f718d7242ce3009_hd.jpg" alt="img"></p>
<p>该问题的 <strong>Loss function</strong>：</p>
<p><img src="http://www.zhihu.com/equation?tex=loss+%3D+%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7B4%7DL%28%5Chat+y%5E%7B%28i%29%7D_%7Bj%7D%2Cy%5E%7B%28i%29%7D_%7Bj%7D%29%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7B4%7D%28y%5E%7B%28i%29%7D_%7Bj%7D%5Clog+%28%5Chat+y%5E%7B%28i%29%7D_%7Bj%7D%29%2B%281-y%5E%7B%28i%29%7D_%7Bj%7D%29%5Clog%281-%5Chat+y%5E%7B%28i%29%7D_%7Bj%7D%29%29" alt="loss = \dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{4}L(\hat y^{(i)}_{j},y^{(i)}_{j})=\dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{4}(y^{(i)}_{j}\log (\hat y^{(i)}_{j})+(1-y^{(i)}_{j})\log(1-\hat y^{(i)}_{j}))"></p>
<p>对于这样的问题，我们就是在做多任务学习，因为我们建立单个神经网络，来解决多个问题。</p>
<p>特定的对于一些问题，例如在我们的例子中，数据集中可能只标注了部分信息，如其中一张只标注了人，汽车和交通灯的标识没有标注。那么对于这样的数据集，我们依旧可以用多任务学习来训练模型。当然要注意这里Loss function求和的时候，只对带0、1标签的<strong> j</strong> 进行求和。</p>
<p><img src="https://pic4.zhimg.com/80/v2-026ec0400199a7598cda399156a9fb92_hd.jpg" alt="img"></p>
<p><strong>多任务学习有意义的情况：</strong></p>
<ul>
<li>如果训练的一组任务可以共用低层特征；</li>
<li>通常，对于每个任务大量的数据具有很大的相似性；（如，在迁移学习中由任务A“100万数据”迁移到任务B“1000数据”；多任务学习中，任务 <img src="http://www.zhihu.com/equation?tex=A_%7B1%7D%EF%BC%8C...%EF%BC%8CA_%7Bn%7D" alt="A_{1}，...，A_{n}"> ，每个任务均有1000个数据，合起来就有1000n个数据，共同帮助任务的训练）</li>
<li>可以训练一个足够大的神经网络并同时做好所有的任务。</li>
</ul>
<h2 id="9-端到端深度学习"><a href="#9-端到端深度学习" class="headerlink" title="9. 端到端深度学习"></a><strong>9. 端到端深度学习</strong></h2><p><strong>端到端学习的定义：</strong></p>
<p>相对于传统的一些数据处理系统或者学习系统，它们包含了多个阶段的处理过程，而端到端的深度学习则忽略了这些阶段，用单个神经网络来替代。</p>
<p><strong>语音识别例子：</strong></p>
<p>在少数据集的情况下传统的特征提取方式可能会取得好的效果；如果在有足够的大量数据集情况下，端到端的深度学习会发挥巨大的价值。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-bdda015b423811a7c6bb7e55210c334c_hd.jpg" alt="img"></p>
<p><strong>优缺点：</strong></p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>端到端学习可以直接让数据“说话”；</li>
<li>所需手工设计的组件更少。</li>
</ol>
<ul>
<li>缺点：</li>
</ul>
<ol>
<li>需要大量的数据；</li>
<li>排除了可能有用的手工设计组件。</li>
</ol>
<p>应用端到端学习的<strong> Key question</strong>：是否有足够的数据能够直接学习到从 <strong>x</strong> 映射到<strong> y </strong>的足够复杂的函数。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/结构化机器学习项目2/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-结构化机器学习项目1" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/结构化机器学习项目1/">机器学习策略</a>
    </h1>
  

        
        <a href="/2018/02/05/结构化机器学习项目1/" class="archive-article-date">
  	<time datetime="2018-02-05T08:45:32.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-正交化"><a href="#1-正交化" class="headerlink" title="1. 正交化"></a><strong>1. 正交化</strong></h2><p>表示在机器学习模型建立的整个流程中，我们需要根据不同部分反映的问题，去做相应的调整，从而更加容易地判断出是在哪一个部分出现了问题，并做相应的解决措施。</p>
<p>正交化或正交性是一种系统设计属性，其确保修改算法的指令或部分不会对系统的其他部分产生或传播副作用。 相互独立地验证使得算法变得更简单，减少了测试和开发的时间。</p>
<p>当在监督学习模型中，以下的4个假设需要真实且是相互正交的：</p>
<ul>
<li>系统在训练集上表现的好</li>
</ul>
<p>否则，使用<strong>更大的神经网络</strong>、<strong>更好的优化算法</strong></p>
<ul>
<li>系统在开发集上表现的好</li>
</ul>
<p>否则，使用<strong>正则化</strong>、<strong>更大的训练集</strong></p>
<ul>
<li>系统在测试集上表现的好</li>
</ul>
<p>否则，使用<strong>更大的开发集</strong></p>
<ul>
<li>在真实的系统环境中表现的好</li>
</ul>
<p>否则，修改<strong>开发测试集</strong>、修改<strong>代价函数</strong></p>
<h2 id="2-单一数字评估指标"><a href="#2-单一数字评估指标" class="headerlink" title="2. 单一数字评估指标"></a><strong>2. 单一数字评估指标</strong></h2><p>在训练机器学习模型的时候，无论是调整超参数，还是尝试更好的优化算法，为问题设置一个单一数字评估指标，可以更好更快的评估模型。</p>
<p><strong>example1</strong></p>
<p>下面是分别训练的两个分类器的Precision、Recall以及F1 score。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-67bd35839c8d21ea2cbbd5b05f163c22_hd.jpg" alt="img"></p>
<p>由上表可以看出，以<strong>Precision</strong>为指标，则分类器A的分类效果好；以<strong>Recall</strong>为指标，则分类器B的分类效果好。所以在有两个及以上判定指标的时候，我们很难决定出A好还是B好。</p>
<p>这里以Precision和Recall为基础，构成一个综合指标<strong>F1 Score</strong>，那么我们利用F1 Score便可以更容易的评判出分类器A的效果更好。</p>
<p>指标介绍：</p>
<p>在二分类问题中，通过预测我们得到下面的真实值 <img src="https://www.zhihu.com/equation?tex=y" alt="y"> 和预测值 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="\hat y"> 的表：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-58205d1e739986419e6246ca43bb65c3_hd.jpg" alt="img"></p>
<ul>
<li>Precision（查准率）：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=Precision+%3D+%5Cdfrac%7BTrue%5C+positive%7D%7BNumber%5C+of%5C+predicted%5C+positive%7D+%5Ctimes+100%5C%25%3D+%5Cdfrac%7BTrue%5C+positive%7D%7BTrue%5C+positive+%2B+False%5C+positive%7D" alt="Precision = \dfrac{True\ positive}{Number\ of\ predicted\ positive} \times 100\%= \dfrac{True\ positive}{True\ positive + False\ positive}"></p>
<p>假设在是否为猫的分类问题中，查准率代表：所有模型预测为猫的图片中，确实为猫的概率。</p>
<ul>
<li>Recall（查全率）：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=Recall+%3D+%5Cdfrac%7BTrue%5C+positive%7D%7BNumber%5C+of%5C+actually%5C+positive%7D+%5Ctimes+100%5C%25%3D+%5Cdfrac%7BTrue%5C+positive%7D%7BTrue%5C+positive+%2B+False%5C+negative%7D" alt="Recall = \dfrac{True\ positive}{Number\ of\ actually\ positive} \times 100\%= \dfrac{True\ positive}{True\ positive + False\ negative}"></p>
<p>假设在是否为猫的分类问题中，查全率代表：真实为猫的图片中，预测正确的概率。</p>
<ul>
<li>F1 Score：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=F1-Socre+%3D+%5Cdfrac+%7B2%7D+%7B%5Cdfrac%7B1%7D%7Bp%7D%2B%5Cdfrac%7B1%7D%7Br%7D%7D" alt="F1-Socre = \dfrac {2} {\dfrac{1}{p}+\dfrac{1}{r}}"></p>
<p>相当与查准率和查全率的一个特别形式的平均指标。</p>
<p><strong>example2</strong></p>
<p>下面是另外一个问题多种分类器在不同的国家中的分类错误率结果：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d271693edd11c1e41ae10cfe83cfe9af_hd.jpg" alt="img"></p>
<p>模型在各个地区有不同的表现，这里用地区的平均值来对模型效果进行评估，转换为单一数字评估指标，就可以很容易的得出表现最好的模型。</p>
<h2 id="3-满足和优化指标"><a href="#3-满足和优化指标" class="headerlink" title="3. 满足和优化指标"></a><strong>3. 满足和优化指标</strong></h2><p>假设有三个不同的分类器性能表现如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e72c89d963bd6f7224d61073507dd16e_hd.jpg" alt="img"></p>
<p>又时对于某一问题，对模型的效果有一定的要求，如要求模型准确率尽可能的高，运行时间在 <img src="https://www.zhihu.com/equation?tex=100%5Crm+%5C+ms" alt="100\rm \ ms"> 以内。这里以 Accuracy 为优化指标，以 Running time 为满足指标，我们可以从中选出B是满足条件的最好的分类器。</p>
<p>一般的，如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标：</p>
<p><img src="https://www.zhihu.com/equation?tex=N_%7Bmetric%7D%3A%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bl%7D+1%5Cqquad+%5Cqquad+%5Cqquad+Optimizing%5C+metric%5C%5C+N_%7Bmetric%7D-1%5Cqquad+Satisificing%5C+metric+%5Cend%7Barray%7D+%5Cright." alt="N_{metric}:\left\{ \begin{array}{l} 1\qquad \qquad \qquad Optimizing\ metric\\ N_{metric}-1\qquad Satisificing\ metric \end{array} \right."></p>
<h2 id="4-训练、开发、测试集"><a href="#4-训练、开发、测试集" class="headerlink" title="4. 训练、开发、测试集"></a><strong>4. 训练、开发、测试集</strong></h2><p>训练、开发、测试集选择设置的一些规则和意见：</p>
<ul>
<li>训练、开发、测试集的设置会对产品带来非常大的影响；</li>
<li>在选择<strong>开发集</strong>和<strong>测试集</strong>时要使二者来自同一分布，且从所有数据中随机选取；</li>
<li>所选择的开发集和测试集中的数据，要与未来想要或者能够得到的数据类似，即模型数据和未来数据要具有相似性；</li>
<li>设置的测试集只要足够大，使其能够在过拟合的系统中给出高方差的结果就可以，也许10000左右的数目足够；</li>
<li>设置开发集只要足够使其能够检测不同算法、不同模型之间的优劣差异就可以，百万大数据中 <img src="https://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"> 的大小就足够；</li>
</ul>
<h2 id="5-改变开发、测试集和评估指标"><a href="#5-改变开发、测试集和评估指标" class="headerlink" title="5. 改变开发、测试集和评估指标"></a><strong>5. 改变开发、测试集和评估指标</strong></h2><p>在针对某一问题我们设置开发集和评估指标后，这就像把目标定在某个位置，后面的过程就聚焦在该位置上。但有时候在这个项目的过程中，可能会发现目标的位置设置错了，所以要移动改变我们的目标。</p>
<p><strong>example1</strong></p>
<p>假设有两个猫的图片的分类器：</p>
<ul>
<li>评估指标：分类错误率</li>
<li>算法A： <img src="https://www.zhihu.com/equation?tex=3%5C%25" alt="3\%"> 错误率</li>
<li>算法B： <img src="https://www.zhihu.com/equation?tex=5%5C%25" alt="5\%"> 错误率</li>
</ul>
<p>这样来看，算法A的表现更好。但是在实际的测试中，算法A可能因为某些原因，将很多色情图片分类成了猫。所以当我们在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片（因为其误差率只有 <img src="https://www.zhihu.com/equation?tex=3%5C%25" alt="3\%"> ），但同时也会给用户推送一些色情图片，这是不能忍受的。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。</p>
<p>这个时候我们就需要改变开发集、测试集或者评估指标。</p>
<p>假设开始我们的评估指标如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=Error+%3D+%5Cdfrac%7B1%7D%7Bm_%7Bdev%7D%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm_%7Bdev%7D%7DI%5C%7By%5E%7B%28i%29%7D_%7Bpred%7D%5Cneq+y%5E%7B%28i%29%7D%5C%7D" alt="Error = \dfrac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}I\{y^{(i)}_{pred}\neq y^{(i)}\}"></p>
<p>该评估指标对色情图片和非色情图片一视同仁，但是我们希望，分类器不会错误将色情图片标记为猫。</p>
<p>修改的方法，在其中加入权重 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%28i%29%7D" alt="w^{(i)}"> ：</p>
<p><img src="https://www.zhihu.com/equation?tex=Error+%3D+%5Cdfrac%7B1%7D%7B%5Csum+w%5E%7B%28i%29%7D%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm_%7Bdev%7D%7D+w%5E%7B%28i%29%7DI%5C%7By%5E%7B%28i%29%7D_%7Bpred%7D%5Cneq+y%5E%7B%28i%29%7D%5C%7D" alt="Error = \dfrac{1}{\sum w^{(i)}}\sum\limits_{i=1}^{m_{dev}} w^{(i)}I\{y^{(i)}_{pred}\neq y^{(i)}\}"></p>
<p>其中：</p>
<p><img src="https://www.zhihu.com/equation?tex=w%5E%7B%28i%29%7D%3D%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bl%7D+1%5Cqquad+%5Cqquad+%5Cqquad+%E5%A6%82%E6%9E%9Cx%5E%7B%28i%29%7D%E4%B8%8D%E6%98%AF%E8%89%B2%E6%83%85%E5%9B%BE%E7%89%87%5C%5C+10%E6%88%96100%5Cqquad+%5Cqquad%E5%A6%82%E6%9E%9Cx%5E%7B%28i%29%7D%E6%98%AF%E8%89%B2%E6%83%85%E5%9B%BE%E7%89%87+%5Cend%7Barray%7D+%5Cright." alt="w^{(i)}=\left\{ \begin{array}{l} 1\qquad \qquad \qquad 如果x^{(i)}不是色情图片\\ 10或100\qquad \qquad如果x^{(i)}是色情图片 \end{array} \right."></p>
<p>这样通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大。</p>
<p>总结来说就是：如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标。</p>
<p><strong>example2</strong></p>
<p>同样针对example1中的两个不同的猫图片的分类器A和B。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4ce3894de11d20f5b1c41afce4a01dae_hd.jpg" alt="img"></p>
<p>由训练误差可以看出分类器A的分类效果比较好。但实际情况是对分类器A，我们一直使用的是网上下载的高质量的图片进行训练；而当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好。</p>
<p>如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中自己所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标。</p>
<p><strong>Guideline：</strong></p>
<ol>
<li>定义正确的评估指标来更好的给分类器的好坏进行排序；</li>
<li>优化评估指标。</li>
</ol>
<h2 id="6-与人类表现做比较"><a href="#6-与人类表现做比较" class="headerlink" title="6. 与人类表现做比较"></a><strong>6. 与人类表现做比较</strong></h2><p><strong>可避免偏差</strong></p>
<p>假设针对两个问题分别具有相同的训练误差和交叉验证误差，如下所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4b876324925dc62b78fb5f3db012d04d_hd.jpg" alt="img"></p>
<p>对于左边的问题，人类的误差为 <img src="https://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"> ，对于右边的问题，人类的误差为 <img src="https://www.zhihu.com/equation?tex=7.5%5C%25" alt="7.5\%"> 。</p>
<p>对于某些任务如计算机视觉上，人类能够做到的水平和<strong>贝叶斯误差</strong>相差不远。（这里贝叶斯误差指最好的分类器的分类误差，也就是说没有分类器可以做到 <img src="https://www.zhihu.com/equation?tex=100%5C%25" alt="100\%"> 正确）。这里将人类水平误差近似为贝叶斯误差。</p>
<ul>
<li>左边的例子： <img src="https://www.zhihu.com/equation?tex=8%5C%25" alt="8\%"> 与 <img src="https://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"> 差距较大</li>
</ul>
<p>主要着手<strong>减少偏差</strong>，即减少训练集误差和人类水平误差之间的差距，来提高模型性能。</p>
<ul>
<li>右边的例子： <img src="https://www.zhihu.com/equation?tex=8%5C%25" alt="8\%"> 与 <img src="https://www.zhihu.com/equation?tex=7.5%5C%25" alt="7.5\%"> 接近</li>
</ul>
<p>主要着手<strong>减少方差</strong>，即减少开发集误差和测试集误差之间的差距，来提高模型性能。</p>
<p><strong>理解人类表现</strong></p>
<p>如医学图像分类问题上，假设有下面几种分类的水平：</p>
<ul>
<li>普通人：<img src="https://www.zhihu.com/equation?tex=3%5C%25" alt="3\%"> error</li>
<li>普通医生： <img src="https://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"> error</li>
<li>专家： <img src="https://www.zhihu.com/equation?tex=0.7%5C%25" alt="0.7\%"> error</li>
<li>专家团队： <img src="https://www.zhihu.com/equation?tex=0.5%5C%25" alt="0.5\%"> error</li>
</ul>
<p>在减小误诊率的背景下，人类水平误差在这种情形下应定义为： <img src="https://www.zhihu.com/equation?tex=0.5%5C%25" alt="0.5\%"> error；</p>
<p>如果在为了部署系统或者做研究分析的背景下，也许超过一名普通医生即可，即人类水平误差在这种情形下定义为： <img src="https://www.zhihu.com/equation?tex=1%5C%25" alt="1\%"> error 即可。</p>
<p><strong>总结：</strong></p>
<p>对人类水平误差有一个大概的估计，可以让我们去估计贝叶斯误差，这样可以让我们更快的做出决定：<strong>减少偏差</strong>还是<strong>减少方差</strong>。</p>
<p>而这个决策技巧通常都很有效果，直到系统的性能开始超越人类，那么我们对贝叶斯误差的估计就不再准确了，再从减少偏差和减少方差方面提升系统性能就会比较困难了。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4864c0e398d6c3e2e9b48b63a2cbbf10_hd.jpg" alt="img"></p>
<h2 id="7-改善模型的表现"><a href="#7-改善模型的表现" class="headerlink" title="7. 改善模型的表现"></a><strong>7. 改善模型的表现</strong></h2><p><strong>基本假设：</strong></p>
<ul>
<li>模型在训练集上有很好的表现；</li>
<li>模型推广到开发和测试集啥会给你也有很好的表现。</li>
</ul>
<p><strong>减少可避免偏差</strong></p>
<ul>
<li>训练更大的模型</li>
<li>训练更长时间、训练更好的优化算法（Momentum、RMSprop、Adam）</li>
<li>寻找更好的网络架构（RNN、CNN）、寻找更好的超参数</li>
</ul>
<p><strong>减少方差</strong></p>
<ul>
<li>收集更多的数据</li>
<li>正则化（L2、dropout、数据增强）</li>
<li>寻找更好的网络架构（RNN、CNN）、寻找更好的超参数</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-33288b47102b78b3bda187170f931dbb_hd.jpg" alt="img"></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/结构化机器学习项目1/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-改善深层神经网络3" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/改善深层神经网络3/">超参数调试，Batch正则化和程序框架</a>
    </h1>
  

        
        <a href="/2018/02/05/改善深层神经网络3/" class="archive-article-date">
  	<time datetime="2018-02-05T08:37:35.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-超参数调试处理"><a href="#1-超参数调试处理" class="headerlink" title="1. 超参数调试处理"></a><strong>1. 超参数调试处理</strong></h2><ul>
<li>在机器学习领域，超参数比较少的情况下，我们之前利用设置网格点的方式来调试超参数；</li>
<li>但在深度学习领域，超参数较多的情况下，不是设置规则的网格点，而是随机选择点进行调试。这样做是因为在我们处理问题的时候，是无法知道哪个超参数是更重要的，所以随机的方式去测试超参数点的性能，更为合理，这样可以探究更超参数的潜在价值。</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e7efb36483524c45c51c451c8b136493_hd.jpg" alt="img"></p>
<h2 id="2-为超参数选择合适的范围"><a href="#2-为超参数选择合适的范围" class="headerlink" title="2. 为超参数选择合适的范围"></a><strong>2. 为超参数选择合适的范围</strong></h2><p><strong>Scale均匀随机</strong></p>
<p>在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有一些超参数的选择做均匀随机取值是不合适的，这里需要按照一定的比例在不同的小范围内进行均匀随机取值，以学习率 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> 的选择为例，在 <img src="https://www.zhihu.com/equation?tex=0.001%2C%5Cldots%2C1" alt="0.001,\ldots,1"> 范围内进行选择：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-5f1dced5e43bc1c3e63676a066441374_hd.jpg" alt="img"></p>
<p>如上图所示，如果在<img src="https://www.zhihu.com/equation?tex=0.001%2C%5Cldots%2C1" alt="0.001,\ldots,1">的范围内进行进行均匀随机取值，则有90%的概率 选择范围在<img src="https://www.zhihu.com/equation?tex=0.1%5Csim1" alt="0.1\sim1">之间，而只有10%的概率才能选择到<img src="https://www.zhihu.com/equation?tex=0.001%5Csim0.1" alt="0.001\sim0.1">之间，显然是不合理的。</p>
<p>所以在选择的时候，在不同比例范围内进行均匀随机取值，如 <img src="https://www.zhihu.com/equation?tex=0.001%5Csim0.001" alt="0.001\sim0.001"> 、 <img src="https://www.zhihu.com/equation?tex=0.001%5Csim0.01" alt="0.001\sim0.01"> 、 <img src="https://www.zhihu.com/equation?tex=0.01%5Csim0.1" alt="0.01\sim0.1"> 、 <img src="https://www.zhihu.com/equation?tex=0.1%5Csim1" alt="0.1\sim1"> 范围内选择。</p>
<p><strong>代码实现</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = -4 * np.random.rand()     # r in [-4,0]</span><br><span class="line">learning_rate = 10 ** r      # 10^&#123;r&#125;</span><br></pre></td></tr></table></figure>
<p>一般地，如果在 <img src="https://www.zhihu.com/equation?tex=10%5E%7Ba%7D%5Csim10%5E%7Bb%7D" alt="10^{a}\sim10^{b}"> 之间的范围内进行按比例的选择，则 <img src="https://www.zhihu.com/equation?tex=r+%5Cin+%5Ba%2C+b%5D" alt="r \in [a, b]"> ， <img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+10%5E%7Br%7D" alt="\alpha = 10^{r}"> 。</p>
<p>同样，在使用指数加权平均的时候，超参数 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 也需要用上面这种方向进行选择。</p>
<h2 id="3-超参数调试实践–Pandas-vs-Caviar"><a href="#3-超参数调试实践–Pandas-vs-Caviar" class="headerlink" title="3. 超参数调试实践–Pandas vs. Caviar"></a><strong>3. 超参数调试实践–Pandas vs. Caviar</strong></h2><p>在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-9db64788ca9dd9065937217bffba1cd1_hd.jpg" alt="img"></p>
<ul>
<li>在计算资源有限的情况下，使用第一种，仅调试一个模型，每天不断优化；</li>
<li>在计算资源充足的情况下，使用第二种，同时并行调试多个模型，选取其中最好的模型。</li>
</ul>
<h2 id="4-网络中激活值的归一化"><a href="#4-网络中激活值的归一化" class="headerlink" title="4. 网络中激活值的归一化"></a><strong>4. 网络中激活值的归一化</strong></h2><p>在Logistic Regression 中，将输入特征进行归一化，可以加速模型的训练。那么对于更深层次的神经网络，我们是否可以归一化隐藏层的输出 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D" alt="a^{[l]}"> 或者经过激活函数前的 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="z^{[l]}"> ，以便加速神经网络的训练过程？答案是肯定的。</p>
<p>常用的方式是将隐藏层的经过激活函数前的 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="z^{[l]}"> 进行归一化。</p>
<p><strong>Batch Norm 的实现</strong></p>
<p>以神经网络中某一隐藏层的中间值为例： <img src="https://www.zhihu.com/equation?tex=z%5E%7B%281%29%7D%2Cz%5E%7B%282%29%7D%2C%5Cldots%2Cz%5E%7B%28m%29%7D" alt="z^{(1)},z^{(2)},\ldots,z^{(m)}"> ：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%7Dz%5E%7B%28i%29%7D+" alt="\mu = \dfrac{1}{m}\sum\limits_{i}z^{(i)} "></p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Csigma%5E%7B2%7D%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%7D%28z%5E%7B%28i%29%7D-%5Cmu%29%5E%7B2%7D" alt=" \sigma^{2}=\dfrac{1}{m}\sum\limits_{i}(z^{(i)}-\mu)^{2}"></p>
<p><img src="https://www.zhihu.com/equation?tex=z%5E%7B%28i%29%7D_%7B%5Crm+norm%7D+%3D+%5Cdfrac%7Bz%5E%7B%28i%29%7D-%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E%7B2%7D%2B%5Cvarepsilon%7D%7D" alt="z^{(i)}_{\rm norm} = \dfrac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}"></p>
<p>这里加上 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" alt="\varepsilon"> 是为了保证数值的稳定。</p>
<p>到这里所有 <img src="https://www.zhihu.com/equation?tex=z" alt="z"> 的分量都是平均值为0和方差为1的分布，但是我们不希望隐藏层的单元总是如此，也许不同的分布会更有意义，所以我们再进行计算：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cwidetilde+z%5E%7B%28i%29%7D+%3D+%5Cgamma+z%5E%7B%28i%29%7D_%7B%5Crm+norm%7D%2B%5Cbeta" alt="\widetilde z^{(i)} = \gamma z^{(i)}_{\rm norm}+\beta"></p>
<p>这里 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma"> 和 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 是可以更新学习的参数，如神经网络的权重 <img src="https://www.zhihu.com/equation?tex=w" alt="w"> 一样，两个参数的值来确定 <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde+z%5E%7B%28i%29%7D" alt="\widetilde z^{(i)}"> 所属的分布。</p>
<h2 id="5-在神经网络中融入Batch-Norm"><a href="#5-在神经网络中融入Batch-Norm" class="headerlink" title="5. 在神经网络中融入Batch Norm"></a><strong>5. 在神经网络中融入Batch Norm</strong></h2><p>在深度神经网络中应用Batch Norm，这里以一个简单的神经网络为例，前向传播的计算流程如下图所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d8039b96e3fab8d90614b7713e1b7282_hd.jpg" alt="img"></p>
<p><strong>实现梯度下降</strong></p>
<ul>
<li><p>for t = 1 … num （这里num 为Mini Batch 的数量）：</p>
</li>
<li><ul>
<li><p>在每一个<img src="https://www.zhihu.com/equation?tex=X%5E%7Bt%7D" alt="X^{t}">上进行前向传播（forward prop）的计算：</p>
</li>
<li><ul>
<li>在每个隐藏层都用 Batch Norm 将 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="z^{[l]}"> 替换为 <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde+z%5E%7B%5Bl%5D%7D" alt="\widetilde z^{[l]}"></li>
</ul>
</li>
<li><p>使用反向传播（Back prop）计算各个参数的梯度： <img src="https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D%E3%80%81d%5Cgamma%5E%7B%5Bl%5D%7D%E3%80%81d%5Cbeta%5E%7B%5Bl%5D%7D" alt="dw^{[l]}、d\gamma^{[l]}、d\beta^{[l]}"></p>
</li>
<li><p>更新参数：</p>
</li>
<li><ul>
<li><img src="https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D%3A%3Dw%5E%7B%5Bl%5D%7D-%5Calpha+dw%5E%7B%5Bl%5D%7D" alt="w^{[l]}:=w^{[l]}-\alpha dw^{[l]}"></li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B%5Bl%5D%7D%3A%3D%5Cgamma%5E%7B%5Bl%5D%7D-%5Calpha+d%5Cgamma%5E%7B%5Bl%5D%7D" alt="\gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}"></li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cbeta%5E%7B%5Bl%5D%7D%3A%3D%5Cbeta%5E%7B%5Bl%5D%7D-%5Calpha+d%5Cbeta%5E%7B%5Bl%5D%7D" alt="\beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]}"></li>
</ul>
</li>
</ul>
</li>
<li><p>同样与Mini-batch 梯度下降法相同，Batch Norm同样适用于momentum、RMSprop、Adam的梯度下降法来进行参数更新。</p>
</li>
</ul>
<p><strong>Notation</strong></p>
<p>这里没有写出偏置参数 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="b^{[l]}"> 是因为 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3Dw%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D" alt="z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}"> ，而Batch Norm 要做的就是将 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D" alt="z^{[l]}"> 归一化，结果成为均值为0，标准差为1的分布，再由 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 和 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma"> 进行重新的分布缩放，那就是意味着，无论 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="b^{[l]}"> 值为多少，在这个过程中都会被减去，不会再起作用。所以如果在神经网络中应用Batch Norm 的话，就直接将偏置参数 <img src="https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D" alt="b^{[l]}"> 去掉，或者将其置零。</p>
<h2 id="6-Batch-Norm-起作用的原因"><a href="#6-Batch-Norm-起作用的原因" class="headerlink" title="6. Batch Norm 起作用的原因"></a><strong>6. Batch Norm 起作用的原因</strong></h2><p><strong>First Reason</strong></p>
<p>首先Batch Norm 可以加速神经网络训练的原因和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理是有相同的道理。</p>
<p>只是Batch Norm 不是单纯的将输入的特征进行归一化，而是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布。</p>
<p><strong>Second Reason</strong></p>
<p>Batch Norm 可以加速神经网络训练的另外一个原因是它可以使权重比网络更滞后或者更深层。</p>
<p>下面是一个判别是否是猫的分类问题，假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果我们将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，那么我们在很大程度上是无法保证能够得到很好的判别结果。</p>
<p>这是因为第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，所以我们无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界。第二个样本集合相当于第一个样本的分布的改变，称为：Covariate shift。如下图所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-72b393411080e18a0bcc2b02c74bf6d8_hd.jpg" alt="img"></p>
<p>那么存在Covariate shift的问题如何应用在神经网络中？就是利用<strong>Batch Norm</strong>来实现。如下面的网络结构：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2233827f3f5f56c486c763b4203ebf58_hd.jpg" alt="img"></p>
<p>网络的目的是通过不断的训练，最后输出一个更加接近于真实值的 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="\hat y"> 。现在以第2个隐藏层为输入来看：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-81ead15d0cbf78c9d037190d7ab118fd_hd.jpg" alt="img"></p>
<p>对于后面的神经网络，是以第二层隐层的输出值 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D" alt="a^{[2]}"> 作为输入特征的，通过前向传播得到最终的 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="\hat y"> ，但是因为我们的网络还有前面两层，由于训练过程，参数 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%5B1%5D%7D%EF%BC%8Cw%5E%7B%5B2%5D%7D" alt="w^{[1]}，w^{[2]}"> 是不断变化的，那么也就是说对于后面的网络， <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D" alt="a^{[2]}"> 的值也是处于不断变化之中，所以就有了Covariate shift的问题。</p>
<p>那么如果对 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D" alt="z^{[2]}"> 使用了Batch Norm，那么即使其值不断的变化，但是其均值和方差却会保持。那么Batch Norm的作用便是其限制了前层的参数更新导致对后面网络数值分布程度的影响，使得输入后层的数值变得更加稳定。另一个角度就是可以看作，Batch Norm 削弱了前层参数与后层参数之间的联系，使得网络的每层都可以自己进行学习，相对其他层有一定的独立性，这会有助于加速整个网络的学习。</p>
<p><strong>Batch Norm 正则化效果</strong></p>
<p>Batch Norm还有轻微的正则化效果。</p>
<p>这是因为在使用Mini-batch梯度下降的时候，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样就在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的 <img src="https://www.zhihu.com/equation?tex=%5Cwidetilde+z%5E%7B%5Bl%5D%7D" alt="\widetilde z^{[l]}"> 也将会加入一定的噪声。</p>
<p>所以和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（这里因为Dropout以一定的概率给神经元乘上0或者1）。所以和Dropout相似，Batch Norm 也有轻微的正则化效果。</p>
<p>这里引入一个小的细节就是，如果使用Batch Norm ，那么使用大的Mini-batch如256，相比使用小的Mini-batch如64，会引入跟少的噪声，那么就会减少正则化的效果。</p>
<h2 id="7-在测试数据上使用-Batch-Norm"><a href="#7-在测试数据上使用-Batch-Norm" class="headerlink" title="7. 在测试数据上使用 Batch Norm"></a><strong>7. 在测试数据上使用 Batch Norm</strong></h2><p>训练过程中，我们是在每个Mini-batch使用Batch Norm，来计算所需要的均值 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="\mu "> 和方差 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="\sigma^{2}"> 。但是在测试的时候，我们需要对每一个测试样本进行预测，无法计算均值和方差。</p>
<p>此时，我们需要单独进行估算均值 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="\mu "> 和方差 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="\sigma^{2}"> 。通常的方法就是在我们训练的过程中，对于训练集的Mini-batch，使用指数加权平均，当训练结束的时候，得到指数加权平均后的均值 <img src="https://www.zhihu.com/equation?tex=%5Cmu+" alt="\mu "> 和方差 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="\sigma^{2}"> ，而这些值直接用于Batch Norm公式的计算，用以对测试样本进行预测。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-1f93b92f43c110aad45eb75c087a3da6_hd.jpg" alt="img"></p>
<h2 id="8-Softmax-回归"><a href="#8-Softmax-回归" class="headerlink" title="8. Softmax 回归"></a><strong>8. Softmax 回归</strong></h2><p>在多分类问题中，有一种 logistic regression的一般形式，叫做Softmax regression。Softmax回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别。</p>
<p><strong>计算公式</strong></p>
<p>下图是Softmax的公式以及一个简单的例子：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-066432f24abbc42211cd330feb2b3317_hd.jpg" alt="img"></p>
<p>可以看出Softmax通过向量 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5BL%5D%7D" alt="z^{[L]}"> 计算出总和为1的四个概率。</p>
<p>在没有隐藏隐藏层的时候，直接对Softmax层输入样本的特点，则在不同数量的类别下，Sotfmax层的作用：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-fa33a0eeac239892d525dfe0b12931ad_hd.jpg" alt="img"></p>
<h2 id="9-训练-Sotfmax-分类器"><a href="#9-训练-Sotfmax-分类器" class="headerlink" title="9. 训练 Sotfmax 分类器"></a><strong>9. 训练 Sotfmax 分类器</strong></h2><p><strong>理解 Sotfmax</strong></p>
<p>为什么叫做Softmax？我们以前面的例子为例，由 <img src="https://www.zhihu.com/equation?tex=z%5E%7B%5BL%5D%7D" alt="z^{[L]}"> 到 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5BL%5D%7D" alt="a^{[L]}"> 的计算过程如下：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-03fc9e10fa225b4f92c711b9a91b0117_hd.jpg" alt="img"></p>
<p>通常我们判定模型的输出类别，是将输出的最大值对应的类别判定为该模型的类别，也就是说最大值为的位置1，其余位置为0，这也就是所谓的“hardmax”。而Sotfmax将模型判定的类别由原来的最大数字5，变为了一个最大的概率0.842，这相对于“hardmax”而言，输出更加“soft”而没有那么“hard”。</p>
<p>Sotfmax回归 将 logistic回归 从二分类问题推广到了多分类问题上。</p>
<p><strong>Softmax 的 Loss function</strong></p>
<p>在使用Sotfmax层时，对应的目标值 <img src="https://www.zhihu.com/equation?tex=y" alt="y"> 以及训练结束前某次的输出的概率值 <img src="https://www.zhihu.com/equation?tex=%5Chat+y" alt="\hat y"> 分别为：</p>
<p><img src="https://www.zhihu.com/equation?tex=y%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+0%5C%5C1%5C%5C0%5C%5C0+%5Cend%7Barray%7D+%5Cright%5D+%2C+%5C+%5Chat+y%3D%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+0.3%5C%5C0.2%5C%5C0.1%5C%5C0.4+%5Cend%7Barray%7D+%5Cright%5D" alt="y=\left[ \begin{array}{l} 0\\1\\0\\0 \end{array} \right] , \ \hat y=\left[ \begin{array}{l} 0.3\\0.2\\0.1\\0.4 \end{array} \right]"></p>
<p>Sotfmax使用的 Loss function 为：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Chat+y%2Cy%29%3D-%5Csum%5Climits_%7Bj%3D1%7D%5E%7B4%7Dy_%7Bj%7D%5Clog+%5Chat+y_%7Bj%7D" alt="L(\hat y,y)=-\sum\limits_{j=1}^{4}y_{j}\log \hat y_{j}"></p>
<p>在训练过程中，我们的目标是最小化Loss function，由目标值我们可以知道， <img src="https://www.zhihu.com/equation?tex=y_%7B1%7D%3Dy_%7B3%7D%3Dy_%7B4%7D%3D0%EF%BC%8Cy_%7B2%7D%3D1" alt="y_{1}=y_{3}=y_{4}=0，y_{2}=1"> ，所以代入 <img src="https://www.zhihu.com/equation?tex=L%28%5Chat+y%2Cy%29" alt="L(\hat y,y)"> 中，有：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Chat+y%2Cy%29%3D-%5Csum%5Climits_%7Bj%3D1%7D%5E%7B4%7Dy_%7Bj%7D%5Clog+%5Chat+y_%7Bj%7D%3D-y_%7B2%7D%5Clog+%5Chat+y_%7B2%7D%3D-%5Clog+%5Chat+y_%7B2%7D" alt="L(\hat y,y)=-\sum\limits_{j=1}^{4}y_{j}\log \hat y_{j}=-y_{2}\log \hat y_{2}=-\log \hat y_{2}"></p>
<p>所以为了最小化Loss function，我们的目标就变成了使得 <img src="https://www.zhihu.com/equation?tex=%5Chat+y_%7B2%7D" alt="\hat y_{2}"> 的概率尽可能的大。</p>
<p>也就是说，这里的损失函数的作用就是找到你训练集中的真实的类别，然后使得该类别相应的概率尽可能地高，这其实是最大似然估计的一种形式。</p>
<p>对应的Cost function如下：</p>
<p><img src="https://www.zhihu.com/equation?tex=J%28w%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C%5Cldots%29%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7DL%28%5Chat+y%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29" alt="J(w^{[1]},b^{[1]},\ldots)=\dfrac{1}{m}\sum\limits_{i=1}^{m}L(\hat y^{(i)},y^{(i)})"></p>
<p><strong>Softmax 的梯度下降</strong></p>
<p>在Softmax层的梯度计算公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+J%7D%7B%5Cpartial+z%5E%7B%5BL%5D%7D%7D%3Ddz%5E%7B%5BL%5D%7D+%3D+%5Chat+y+-y" alt="\dfrac{\partial J}{\partial z^{[L]}}=dz^{[L]} = \hat y -y"></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/改善深层神经网络3/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-改善深层神经网络2" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/02/05/改善深层神经网络2/">优化算法</a>
    </h1>
  

        
        <a href="/2018/02/05/改善深层神经网络2/" class="archive-article-date">
  	<time datetime="2018-02-05T08:36:15.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2018-02-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-Mini-batch-梯度下降法"><a href="#1-Mini-batch-梯度下降法" class="headerlink" title="1. Mini-batch 梯度下降法"></a><strong>1. Mini-batch 梯度下降法</strong></h2><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。</p>
<p>但是如果每次处理训练数据的一部分，即用其子集进行梯度下降，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。</p>
<p><strong>算法核心</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-a871740e81fd724832bac62243dee01e_hd.jpg" alt="img"></p>
<p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。</p>
<p><strong>不同size大小的比较</strong></p>
<p>普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d7374a43267473cd13bfec8764a32471_hd.jpg" alt="img"></p>
<ul>
<li>batch梯度下降：</li>
</ul>
<ol>
<li>对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；</li>
<li>Cost function 总是向减小的方向下降。</li>
</ol>
<ul>
<li>随机梯度下降：</li>
</ul>
<ol>
<li>对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；</li>
<li>Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</li>
</ol>
<ul>
<li>Mini-batch梯度下降：</li>
</ul>
<ol>
<li>选择一个 <img src="https://www.zhihu.com/equation?tex=1%3Csize%3Cm" alt="1&lt;size&lt;m"> 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处；</li>
<li>Cost function的下降处于前两者之间。</li>
</ol>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2cdd92c6706eaf29b80694c1213804bc_hd.jpg" alt="img"></p>
<p><strong>Mini-batch 大小的选择</strong></p>
<ul>
<li>如果训练样本的大小比较小时，如 <img src="https://www.zhihu.com/equation?tex=m%5Cleqslant+2000" alt="m\leqslant 2000"> 时 —— 选择batch梯度下降法；</li>
<li>如果训练样本的大小比较大时，典型的大小为： <img src="https://www.zhihu.com/equation?tex=2%5E%7B6%7D%E3%80%812%5E%7B7%7D%E3%80%81%5Ccdots%E3%80%812%5E%7B10%7D" alt="2^{6}、2^{7}、\cdots、2^{10}"> ；</li>
<li>Mini-batch的大小要符合CPU/GPU内存。</li>
</ul>
<h2 id="2-指数加权平均"><a href="#2-指数加权平均" class="headerlink" title="2. 指数加权平均"></a><strong>2. 指数加权平均</strong></h2><p>指数加权平均的关键函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=v_%7Bt%7D+%3D+%5Cbeta+v_%7Bt-1%7D%2B%281-%5Cbeta%29%5Ctheta_%7Bt%7D" alt="v_{t} = \beta v_{t-1}+(1-\beta)\theta_{t}"></p>
<p>下图是一个关于天数和温度的散点图：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-2969f0fa8cfb79021856d26e2c269306_hd.jpg" alt="img"></p>
<ul>
<li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.9" alt="\beta =0.9"> 时，指数加权平均最后的结果如图中红色线所示；</li>
<li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.98" alt="\beta =0.98"> 时，指数加权平均最后的结果如图中绿色线所示；</li>
<li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.5" alt="\beta =0.5"> 时，指数加权平均最后的结果如下图中黄色线所示；</li>
</ul>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-44d786b0b58dba00c780e398259a58a7_hd.jpg" alt="img"></p>
<p><strong>理解指数加权平均</strong></p>
<p>例子，当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.9" alt="\beta =0.9"> 时：</p>
<p><img src="https://www.zhihu.com/equation?tex=v_%7B100%7D+%3D+0.9v_%7B99%7D%2B0.1%5Ctheta_%7B100%7D%5C%5Cv_%7B99%7D+%3D+0.9v_%7B98%7D%2B0.1%5Ctheta_%7B99%7D%5C%5Cv_%7B98%7D+%3D+0.9v_%7B97%7D%2B0.1%5Ctheta_%7B98%7D%5C%5C+%5Cldots" alt="v_{100} = 0.9v_{99}+0.1\theta_{100}\\v_{99} = 0.9v_{98}+0.1\theta_{99}\\v_{98} = 0.9v_{97}+0.1\theta_{98}\\ \ldots"></p>
<p>展开，有：</p>
<p><img src="https://www.zhihu.com/equation?tex=v_%7B100%7D%3D0.1%5Ctheta_%7B100%7D%2B0.9%280.1%5Ctheta_%7B99%7D%2B0.9%280.1%5Ctheta_%7B98%7D%2B0.9v_%7B97%7D%29%29%5C%5C%3D0.1%5Ctheta_%7B100%7D%2B0.1%5Ctimes0.9%5Ctheta_%7B99%7D%2B0.1%5Ctimes%280.9%29%5E%7B2%7D%5Ctheta_%7B98%7D%2B0.1%5Ctimes%280.9%29%5E%7B3%7D%5Ctheta_%7B97%7D%2B%5Ccdots" alt="v_{100}=0.1\theta_{100}+0.9(0.1\theta_{99}+0.9(0.1\theta_{98}+0.9v_{97}))\\=0.1\theta_{100}+0.1\times0.9\theta_{99}+0.1\times(0.9)^{2}\theta_{98}+0.1\times(0.9)^{3}\theta_{97}+\cdots"></p>
<p>上式中所有 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta"> 前面的系数相加起来为1或者接近于1，称之为偏差修正。</p>
<p>总体来说存在， <img src="https://www.zhihu.com/equation?tex=%281-%5Cvarepsilon%29%5E%7B1%2F%5Cvarepsilon%7D%3D%5Cdfrac%7B1%7D%7Be%7D" alt="(1-\varepsilon)^{1/\varepsilon}=\dfrac{1}{e}"> ，在我们的例子中， <img src="https://www.zhihu.com/equation?tex=1-%5Cvarepsilon%3D%5Cbeta%3D0.9" alt="1-\varepsilon=\beta=0.9"> ，即 <img src="https://www.zhihu.com/equation?tex=0.9%5E%7B10%7D%5Capprox+0.35%5Capprox%5Cdfrac%7B1%7D%7Be%7D" alt="0.9^{10}\approx 0.35\approx\dfrac{1}{e}"> 。相当于大约10天后，系数的峰值（这里是0.1）下降到原来的 <img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B1%7D%7Be%7D" alt="\dfrac{1}{e}"> ，只关注了过去10天的天气。</p>
<p><strong>指数加权平均实现</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=v_%7B0%7D+%3D0%5C%5C+v_%7B1%7D%3D+%5Cbeta+v_%7B0%7D%2B%281-%5Cbeta%29%5Ctheta_%7B1%7D%5C%5C+v_%7B2%7D%3D+%5Cbeta+v_%7B1%7D%2B%281-%5Cbeta%29%5Ctheta_%7B2%7D%5C%5C+v_%7B3%7D%3D+%5Cbeta+v_%7B2%7D%2B%281-%5Cbeta%29%5Ctheta_%7B3%7D%5C%5C+%5Cldots" alt="v_{0} =0\\ v_{1}= \beta v_{0}+(1-\beta)\theta_{1}\\ v_{2}= \beta v_{1}+(1-\beta)\theta_{2}\\ v_{3}= \beta v_{2}+(1-\beta)\theta_{3}\\ \ldots"></p>
<p>因为，在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。</p>
<p><strong>指数加权平均的偏差修正</strong></p>
<p>在我们执行指数加权平均的公式时，当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D0.98" alt="\beta=0.98"> 时，我们得到的并不是图中的绿色曲线，而是下图中的紫色曲线，其起点比较低。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-e52c3c3cebde2ec15f3f055fcbca7faa_hd.jpg" alt="img"></p>
<ul>
<li>原因：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=v_%7B0%7D%3D0%5C%5Cv_%7B1%7D%3D0.98v_%7B0%7D%2B0.02%5Ctheta_%7B1%7D%3D0.02%5Ctheta_%7B1%7D%5C%5Cv_%7B2%7D%3D0.98v_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%3D0.98%5Ctimes0.02%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%3D0.0196%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D" alt="v_{0}=0\\v_{1}=0.98v_{0}+0.02\theta_{1}=0.02\theta_{1}\\v_{2}=0.98v_{1}+0.02\theta_{2}=0.98\times0.02\theta_{1}+0.02\theta_{2}=0.0196\theta_{1}+0.02\theta_{2}"></p>
<p>如果第一天的值为如 <img src="https://www.zhihu.com/equation?tex=40" alt="40"> ，则 <img src="https://www.zhihu.com/equation?tex=v_%7B1%7D%3D0.02%5Ctimes40%3D8" alt="v_{1}=0.02\times40=8"> ，得到的值要远小于实际值，后面几天的情况也会由于初值引起的影响，均低于实际均值。</p>
<ul>
<li>偏差修正：</li>
</ul>
<p>使用 <img src="https://www.zhihu.com/equation?tex=v_%7B1%7D%3D0.02%5Ctimes40%3D8" alt="v_{1}=0.02\times40=8"></p>
<p>当 <img src="https://www.zhihu.com/equation?tex=t%3D2" alt="t=2"> 时：</p>
<p><img src="https://www.zhihu.com/equation?tex=+1-%5Cbeta%5E%7Bt%7D%3D1-%280.98%29%5E%7B2%7D%3D0.0396" alt=" 1-\beta^{t}=1-(0.98)^{2}=0.0396"></p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cdfrac%7Bv_%7B2%7D%7D%7B0.0396%7D%3D%5Cdfrac%7B0.0196%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%7D%7B0.0396%7D" alt=" \dfrac{v_{2}}{0.0396}=\dfrac{0.0196\theta_{1}+0.02\theta_{2}}{0.0396}"></p>
<p>偏差修正得到了绿色的曲线，在开始的时候，能够得到比紫色曲线更好的计算平均的效果。随着 <img src="https://www.zhihu.com/equation?tex=t" alt="t"> 逐渐增大， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%5E%7Bt%7D" alt="\beta^{t}"> 接近于0，所以后面绿色的曲线和紫色的曲线逐渐重合了。</p>
<p>虽然存在这种问题，但是在实际过程中，一般会忽略前期均值偏差的影响。</p>
<h2 id="3-动量（Momentum）梯度下降法"><a href="#3-动量（Momentum）梯度下降法" class="headerlink" title="3. 动量（Momentum）梯度下降法"></a><strong>3. 动量（Momentum）梯度下降法</strong></h2><p>动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。</p>
<p>在我们优化 Cost function 的时候，以下图所示的函数图为例：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-4e37ac10a00135412175a0ba85103035_hd.jpg" alt="img"></p>
<p>在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个较小的学习率来进行迭代。</p>
<p>如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率。</p>
<p>但是我们又希望在如图的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。</p>
<p><strong>算法实现</strong></p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-070324bcdbf16b9104872c2a63b21e3b_hd.jpg" alt="img"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 常用的值是0.9。</p>
<p>在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在纵轴方向上的上下波动，经过平均以后，接近于0，纵轴上的波动变得非常的小；但在横轴方向上，所有的微分都指向横轴方向，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。</p>
<p><strong>算法本质解释</strong></p>
<p>在对应上面的计算公式中，将Cost function想象为一个碗状，想象从顶部往下滚球，其中：</p>
<ul>
<li>微分项 <img src="https://www.zhihu.com/equation?tex=dw%2Cdb" alt="dw,db"> 想象为球提供的加速度；</li>
<li>动量项 <img src="https://www.zhihu.com/equation?tex=v_%7Bdw%7D%2Cv_%7Bdb%7D" alt="v_{dw},v_{db}"> 相当于速度；</li>
</ul>
<p>小球在向下滚动的过程中，因为加速度的存在使得速度会变快，但是由于 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta"> 的存在，其值小于1，可以认为是摩擦力，所以球不会无限加速下去。</p>
<h2 id="4-RMSprop"><a href="#4-RMSprop" class="headerlink" title="4. RMSprop"></a><strong>4. RMSprop</strong></h2><p>除了上面所说的<strong>Momentum</strong>梯度下降法，<strong>RMSprop</strong>（root mean square prop）也是一种可以加快梯度下降的算法。</p>
<p>同样算法的样例实现如下图所示：</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-d9b0e716021f96c12ec478c8d4629b4f_hd.jpg" alt="img"></p>
<p>这里假设参数b的梯度处于纵轴方向，参数w的梯度处于横轴方向（当然实际中是处于高维度的情况），利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，如图中蓝色线所示，使其梯度下降的速度变得更快，如图绿色线所示。</p>
<p>在如图所示的实现中，RMSprop将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以0，平方根分母中在实际使用会加入一个很小的值如 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon%3D10%5E%7B-8%7D" alt="\varepsilon=10^{-8}"> 。</p>
<h2 id="5-Adam-优化算法"><a href="#5-Adam-优化算法" class="headerlink" title="5. Adam 优化算法"></a><strong>5. Adam 优化算法</strong></h2><p>Adam （Adaptive Moment Estimation）优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。</p>
<p><strong>算法实现</strong></p>
<ul>
<li><p>初始化： <img src="https://www.zhihu.com/equation?tex=V_%7Bdw%7D+%3D+0%EF%BC%8CS_%7Bdw%7D%3D0%EF%BC%8CV_%7Bdb%7D%3D0%EF%BC%8CS_%7Bdb%7D+%3D+0" alt="V_{dw} = 0，S_{dw}=0，V_{db}=0，S_{db} = 0"></p>
</li>
<li><p>第 <img src="https://www.zhihu.com/equation?tex=t" alt="t"> 次迭代：</p>
</li>
<li><ul>
<li>Compute <img src="https://www.zhihu.com/equation?tex=dw%EF%BC%8Cdb" alt="dw，db"> on the current mini-batch</li>
<li><img src="https://www.zhihu.com/equation?tex=V_%7Bdw%7D%3D%5Cbeta_%7B1%7DV_%7Bdw%7D%2B%281-%5Cbeta_%7B1%7D%29dw%EF%BC%8CV_%7Bdb%7D%3D%5Cbeta_%7B1%7DV_%7Bdb%7D%2B%281-%5Cbeta_%7B1%7D%29db" alt="V_{dw}=\beta_{1}V_{dw}+(1-\beta_{1})dw，V_{db}=\beta_{1}V_{db}+(1-\beta_{1})db"> —– “Momentum”</li>
<li><img src="https://www.zhihu.com/equation?tex=S_%7Bdw%7D%3D%5Cbeta_%7B2%7DS_%7Bdw%7D%2B%281-%5Cbeta_%7B2%7D%29%28dw%29%5E%7B2%7D%EF%BC%8CS_%7Bdb%7D%3D%5Cbeta_%7B2%7DS_%7Bdb%7D%2B%281-%5Cbeta_%7B2%7D%29%28db%29%5E%7B2%7D" alt="S_{dw}=\beta_{2}S_{dw}+(1-\beta_{2})(dw)^{2}，S_{db}=\beta_{2}S_{db}+(1-\beta_{2})(db)^{2}"> —– “RMSprop”</li>
<li><img src="https://www.zhihu.com/equation?tex=V_%7Bdw%7D%5E%7Bcorrected%7D+%3D+V_%7Bdw%7D%2F%281-%5Cbeta_%7B1%7D%5E%7Bt%7D%29%EF%BC%8CV_%7Bdb%7D%5E%7Bcorrected%7D+%3D+V_%7Bdb%7D%2F%281-%5Cbeta_%7B1%7D%5E%7Bt%7D%29" alt="V_{dw}^{corrected} = V_{dw}/(1-\beta_{1}^{t})，V_{db}^{corrected} = V_{db}/(1-\beta_{1}^{t})"> —– 偏差修正</li>
<li><img src="https://www.zhihu.com/equation?tex=S_%7Bdw%7D%5E%7Bcorrected%7D+%3D+S_%7Bdw%7D%2F%281-%5Cbeta_%7B2%7D%5E%7Bt%7D%29%EF%BC%8CS_%7Bdb%7D%5E%7Bcorrected%7D+%3D+S_%7Bdb%7D%2F%281-%5Cbeta_%7B2%7D%5E%7Bt%7D%29" alt="S_{dw}^{corrected} = S_{dw}/(1-\beta_{2}^{t})，S_{db}^{corrected} = S_{db}/(1-\beta_{2}^{t})"> —– 偏差修正</li>
<li><img src="https://www.zhihu.com/equation?tex=w%3A%3Dw-%5Calpha%5Cdfrac%7BV_%7Bdw%7D%5E%7Bcorrected%7D%7D%7B%5Csqrt%7BS_%7Bdw%7D%5E%7Bcorrected%7D%7D%2B%5Cvarepsilon%7D%EF%BC%8Cb%3A%3Db-%5Calpha%5Cdfrac%7BV_%7Bdb%7D%5E%7Bcorrected%7D%7D%7B%5Csqrt%7BS_%7Bdb%7D%5E%7Bcorrected%7D%7D%2B%5Cvarepsilon%7D" alt="w:=w-\alpha\dfrac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}}+\varepsilon}，b:=b-\alpha\dfrac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}"></li>
</ul>
</li>
</ul>
<p><strong>超参数的选择</strong></p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> ：需要进行调试；</li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cbeta_%7B1%7D" alt="\beta_{1}"> ：常用缺省值为0.9， <img src="https://www.zhihu.com/equation?tex=dw" alt="dw"> 的加权平均；</li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cbeta_%7B2%7D" alt="\beta_{2}"> ：推荐使用0.999， <img src="https://www.zhihu.com/equation?tex=dw%5E%7B2%7D" alt="dw^{2}"> 的加权平均值；</li>
<li><img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" alt="\varepsilon"> ：推荐使用 <img src="https://www.zhihu.com/equation?tex=10%5E%7B-8%7D" alt="10^{-8}"> 。</li>
</ul>
<h2 id="6-学习率衰减"><a href="#6-学习率衰减" class="headerlink" title="6. 学习率衰减"></a><strong>6. 学习率衰减</strong></h2><p>在我们利用 mini-batch 梯度下降法来寻找Cost function的最小值的时候，如果我们设置一个固定的学习速率 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> ，则算法在到达最小值点附近后，由于不同batch中存在一定的噪声，使得不会精确收敛，而一直会在一个最小值点较大的范围内波动，如下图中蓝色线所示。</p>
<p>但是如果我们使用学习率衰减，逐渐减小学习速率 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> ，在算法开始的时候，学习速率还是相对较快，能够相对快速的向最小值点的方向下降。但随着 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha"> 的减小，下降的步伐也会逐渐变小，最终会在最小值附近的一块更小的区域里波动，如图中绿色线所示。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-0b54853b610c36c030c124606cea1d5c_hd.jpg" alt="img"></p>
<p><strong>学习率衰减的实现</strong></p>
<ul>
<li>常用： <img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+%5Cdfrac%7B1%7D%7B1%2Bdecay%5C_rate%2Aepoch%5C_num%7D%5Calpha_%7B0%7D" alt="\alpha = \dfrac{1}{1+decay\_rate*epoch\_num}\alpha_{0}"></li>
<li>指数衰减： <img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+0.95%5E%7Bepoch%5C_num%7D%5Calpha_%7B0%7D" alt="\alpha = 0.95^{epoch\_num}\alpha_{0}"></li>
<li>其他： <img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+%5Cdfrac%7Bk%7D%7Bepoch%5C_num%7D%5Ccdot%5Calpha_%7B0%7D" alt="\alpha = \dfrac{k}{epoch\_num}\cdot\alpha_{0}"></li>
<li>离散下降（不同阶段使用不同的学习速率）</li>
</ul>
<h2 id="7-局部最优问题"><a href="#7-局部最优问题" class="headerlink" title="7. 局部最优问题"></a><strong>7. 局部最优问题</strong></h2><p>在低维度的情形下，我们可能会想象到一个Cost function 如左图所示，存在一些局部最小值点，在初始化参数的时候，如果初始值选取的不得当，会存在陷入局部最优点的可能性。</p>
<p>但是，如果我们建立一个高维度的神经网络。通常梯度为零的点，并不是如左图中的局部最优点，而是右图中的<strong>鞍点</strong>（叫鞍点是因为其形状像马鞍的形状）。</p>
<p><img src="http://p3ev46x16.bkt.clouddn.com/v2-ec0c95cf84b7d2ca878bcd51b4485707_hd.jpg" alt="img"></p>
<p>在一个具有高维度空间的函数中，如果梯度为0，那么在每个方向，Cost function可能是凸函数，也有可能是凹函数。但如果参数维度为2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为 <img src="https://www.zhihu.com/equation?tex=2%5E%7B-20000%7D" alt="2^{-20000}"> ，可能性非常的小。也就是说，在低维度中的局部最优点的情况，并不适用于高维度，在梯度为0的点更有可能是鞍点，而不是局部最小值点。</p>
<p>在高纬度的情况下：</p>
<ul>
<li>几乎不可能陷入局部最小值点；</li>
<li>处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。</li>
</ul>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/深度学习//" class="article-tag-list-link color5">深度学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/2018/02/05/改善深层神经网络2/">more >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/2/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/">Next &raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2018 Remaerd
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 50%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 50%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">集成学习</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NLP</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">PCA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">范数</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">决策树</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">SVM</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">RNN</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">优化</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">线性代数</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">朴素贝叶斯</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">概率统计</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">特征工程</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">线性回归</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">逻辑回归</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">一个大数据和机器学习爱好者。</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>